"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[4893],{3905:function(e,t,a){a.d(t,{Zo:function(){return d},kt:function(){return m}});var n=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var s=n.createContext({}),c=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},d=function(e){var t=c(e.components);return n.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,o=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=c(a),m=i,b=u["".concat(s,".").concat(m)]||u[m]||p[m]||o;return a?n.createElement(b,r(r({ref:t},d),{},{components:a})):n.createElement(b,r({ref:t},d))}));function m(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=a.length,r=new Array(o);r[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,r[1]=l;for(var c=2;c<o;c++)r[c]=a[c];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},9787:function(e,t,a){a.r(t),a.d(t,{assets:function(){return d},contentTitle:function(){return s},default:function(){return m},frontMatter:function(){return l},metadata:function(){return c},toc:function(){return p}});var n=a(87462),i=a(63366),o=(a(67294),a(3905)),r=["components"],l={sidebar_position:1,title:"Databricks Jobs",description:"Databricks jobs",id:"databricks-jobs",tags:["jobs","deployment","scheduling"]},s=void 0,c={unversionedId:"low-code-jobs/databricks-jobs",id:"low-code-jobs/databricks-jobs",title:"Databricks Jobs",description:"Databricks jobs",source:"@site/docs/low-code-jobs/databricks-jobs.md",sourceDirName:"low-code-jobs",slug:"/low-code-jobs/databricks-jobs",permalink:"/low-code-jobs/databricks-jobs",draft:!1,tags:[{label:"jobs",permalink:"/tags/jobs"},{label:"deployment",permalink:"/tags/deployment"},{label:"scheduling",permalink:"/tags/scheduling"}],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Databricks Jobs",description:"Databricks jobs",id:"databricks-jobs",tags:["jobs","deployment","scheduling"]},sidebar:"defaultSidebar",previous:{title:"Low-code Jobs",permalink:"/low-code-jobs/"},next:{title:"Airflow",permalink:"/low-code-jobs/airflow"}},d={},p=[{value:"Development",id:"development",level:2},{value:"Your first Job",id:"your-first-job",level:3},{value:"Building the Job",id:"building-the-job",level:3},{value:"Pipeline Gem",id:"pipeline-gem",level:4},{value:"Script Gem",id:"script-gem",level:4},{value:"Visual == Code",id:"visual--code",level:2},{value:"Job Configuration",id:"job-configuration",level:2},{value:"Deployment Modes",id:"deployment-modes",level:2},{value:"Multi Job Cluster Mode",id:"multi-job-cluster-mode",level:3},{value:"Single Job Cluster Mode",id:"single-job-cluster-mode",level:3},{value:"Fully Configurable Cluster Mode",id:"fully-configurable-cluster-mode",level:3},{value:"Job Monitoring",id:"job-monitoring",level:2},{value:"Guides",id:"guides",level:2}],u={toc:p};function m(e){var t=e.components,l=(0,i.Z)(e,r);return(0,o.kt)("wrapper",(0,n.Z)({},u,l,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"Once you have developed a ",(0,o.kt)("a",{parentName:"p",href:"/concepts/Pipeline"},"Spark Pipeline")," using Prophecy, you will want to schedule it to run at\nsome frequency. To support this, Prophecy provides a low-code layer on top of Databricks Jobs\nor ",(0,o.kt)("a",{parentName:"p",href:"/low-code-jobs/airflow"},"Airflow")," for an easy orchestration."),(0,o.kt)("h2",{id:"development"},"Development"),(0,o.kt)("h3",{id:"your-first-job"},"Your first Job"),(0,o.kt)("p",null,"You can create a Job from two places. If you're going to schedule only a single Pipeline, the easiest way to\nbuild a Job for it is to do it directly from the Pipeline editor screen. This way your Job is automatically initialized\nwith the Pipeline you create it from."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Databricks Job Creation From Pipeline",src:a(48391).Z,width:"4053",height:"1335"})),(0,o.kt)("p",null,"To do that, simply navigate to your Pipeline, and click on the Schedule button (1). That opens a modal that shows all\nthe Jobs that refer to this Job or allow you to create a completely new Job from scratch. Upon clicking\nCreate New (2) you are redirected to the ",(0,o.kt)("a",{parentName:"p",href:"/low-code-jobs/databricks-jobs#building-the-job"},"Job building page"),"."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Databricks Job Creation",src:a(98235).Z,width:"4053",height:"1335"})),(0,o.kt)("p",null,"Alternatively, if you'd like to create a new Job completely from scratch, you can do that directly from the entity\ncreation page (1). There you can choose the Job tile (2) and that opens a similar modal where you can define your\nJob details (3)."),(0,o.kt)("p",null,"Whenever, creating a new Job you're asked for the following details:"),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:null},"Field Name"),(0,o.kt)("th",{parentName:"tr",align:null},"Description"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Project"),(0,o.kt)("td",{parentName:"tr",align:null},"Which ",(0,o.kt)("a",{parentName:"td",href:"/concepts/project"},"Project")," to create the Job in. This controls who has access to the Job, groups Jobs together for lineage, and allows you to use Pipelines already published within that Project.")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Branch"),(0,o.kt)("td",{parentName:"tr",align:null},"Which Git branch to use when developing this Job.")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Name"),(0,o.kt)("td",{parentName:"tr",align:null},"Unique Job name.")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Scheduler"),(0,o.kt)("td",{parentName:"tr",align:null},"The underlying engine that's going to execute your Job. Databricks is recommended.")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Fabric"),(0,o.kt)("td",{parentName:"tr",align:null},"The ",(0,o.kt)("a",{parentName:"td",href:"/concepts/fabrics"},"execution Fabric")," to which the Job is going to be deployed.")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Job Size"),(0,o.kt)("td",{parentName:"tr",align:null},"The ",(0,o.kt)("a",{parentName:"td",href:"/concepts/fabrics#whats-in-a-fabric"},"default size")," of the cluster that's going to be created for the Job to run.")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Schedule Interval"),(0,o.kt)("td",{parentName:"tr",align:null},"Defines how often your Job is going to run. The interval is defined using the ",(0,o.kt)("a",{parentName:"td",href:"http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html"},"Quartz format"),". You can click on the clock icon to select the interval.")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Alerts email"),(0,o.kt)("td",{parentName:"tr",align:null},"Comma separated list of emails that are going to receive notifications on specific Job status events (start, failure, or success).")))),(0,o.kt)("h3",{id:"building-the-job"},"Building the Job"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Example Databricks Job",src:a(20282).Z,width:"2762",height:"1222"})),(0,o.kt)("p",null,"Now that you've created your first Job, you can start adding Gems to the canvas to define which Pipelines will\nbe run during the Job. To define dependencies between the Pipelines within the Job you can simply connect them\nby dragging-and-dropping the edges between Gems."),(0,o.kt)("p",null,"Two Gem types are available when defining Databricks Jobs:"),(0,o.kt)("h4",{id:"pipeline-gem"},"Pipeline Gem"),(0,o.kt)("p",null,"The Pipeline Gem triggers a Spark Pipeline developed in Prophecy."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Pipeline Component",src:a(41290).Z,width:"3000",height:"1588"})),(0,o.kt)("p",null,"Settings for Pipeline component can be inherited from overall Job configuration or can be set inside the component itself."),(0,o.kt)("h4",{id:"script-gem"},"Script Gem"),(0,o.kt)("p",null,"Script Gem can be used to write any ad-hoc code."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Script Component",src:a(62226).Z,width:"3010",height:"1794"})),(0,o.kt)("p",null,"Settings for script component can be inherited from overall Job configuration or can be set inside the component itself."),(0,o.kt)("h2",{id:"visual--code"},"Visual == Code"),(0,o.kt)("p",null,"The visual graph created on the Jobs page is automatically converted to code (JSON) in the backend which gets committed to Git."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Code View",src:a(62446).Z,width:"3324",height:"1850"})),(0,o.kt)("h2",{id:"job-configuration"},"Job Configuration"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Example Configuration",src:a(51902).Z,width:"3006",height:"1446"})),(0,o.kt)("hr",null),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:null},"Field Name"),(0,o.kt)("th",{parentName:"tr",align:null},"Description"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Scheduler"),(0,o.kt)("td",{parentName:"tr",align:null},"The underlying engine that's going to execute your Job. Databricks is recommended.")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Fabric"),(0,o.kt)("td",{parentName:"tr",align:null},"The ",(0,o.kt)("a",{parentName:"td",href:"/concepts/fabrics"},"execution Fabric")," to which the Job is going to be deployed.")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Cluster Size"),(0,o.kt)("td",{parentName:"tr",align:null},"The ",(0,o.kt)("a",{parentName:"td",href:"/concepts/fabrics#whats-in-a-fabric"},"default size")," of the cluster that's going to be created for the Job to run.")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Cluster Mode"),(0,o.kt)("td",{parentName:"tr",align:null},"Can be selected as ",(0,o.kt)("inlineCode",{parentName:"td"},"Single")," (all Gems within the Job re-use the same Cluster) or ",(0,o.kt)("inlineCode",{parentName:"td"},"Multi")," (all Gems within the Job run on a separate new cluster)")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Schedule Interval"),(0,o.kt)("td",{parentName:"tr",align:null},"Defines how often your Job is going to run. The interval is defined using the ",(0,o.kt)("a",{parentName:"td",href:"http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html"},"Quartz format"),". You can click on the clock icon to select the interval.")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Pipeline level Timeout"),(0,o.kt)("td",{parentName:"tr",align:null},"Timeout at Pipeline level")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Alerts Email for Pipeline"),(0,o.kt)("td",{parentName:"tr",align:null},"Comma separated list of emails, that are going to receive notifications on specific Job status events (Job start, failure, or success) for entire Pipeline.")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Per Gem Timeout"),(0,o.kt)("td",{parentName:"tr",align:null},"Timeout for each Gem in Job Pipeline")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Number of retries per Gem"),(0,o.kt)("td",{parentName:"tr",align:null},"Number of retries for each Gem in Job Pipeline")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"Alerts Email per Gem"),(0,o.kt)("td",{parentName:"tr",align:null},"Comma separated list of emails that are going to receive notifications on specific Job status events (start, failure, or success) for each Gem in Job Pipeline.")))),(0,o.kt)("h2",{id:"deployment-modes"},"Deployment Modes"),(0,o.kt)("p",null,"To deploy a Job on Databricks, we need to release the project from Prophecy UI as shown in example below. As soon as the project is\nreleased, the Job would start appearing on Databricks Jobs page as well."),(0,o.kt)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,o.kt)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,o.kt)("iframe",{src:"https://user-images.githubusercontent.com/103921419/184726064-67e3df01-ba4c-431e-92e9-8bda92a19530.mp4",title:"Job Deployment",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,o.kt)("hr",null),(0,o.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"Make sure to enable the Job before creating a Release. If it is not enabled the Job will not run on the specified schedule."),(0,o.kt)("p",{parentName:"div"},"If a Job's selected Fabric is changed it will create a separate Databricks job definition. The previous Job (with the previous Fabric) will be paused automatically and the new version will be scheduled."))),(0,o.kt)("p",null,"Prophecy supports two different Job deployment models. Each has different impacts on Job cost and parallelism."),(0,o.kt)("h3",{id:"multi-job-cluster-mode"},"Multi Job Cluster Mode"),(0,o.kt)("p",null,"In this mode, each component of Job will spawn a separate cluster of its own."),(0,o.kt)("p",null,"Here's how the Databricks UI looks for Prophecy's Multi Cluster Mode."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Multi Job Cluster",src:a(93532).Z,width:"3322",height:"1756"})),(0,o.kt)("h3",{id:"single-job-cluster-mode"},"Single Job Cluster Mode"),(0,o.kt)("p",null,"In this mode, each component of Job will run on the same cluster."),(0,o.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"To use single cluster mode the package name across each Pipeline in Job should be unique.\nThis is done to ensure that the folder structure for one Pipeline does not overwrite another.\nPlease refer to the steps below in continuation to our earlier ",(0,o.kt)("a",{parentName:"p",href:"databricks-jobs#deployment-modes"},"Example")," on how to configure package name in Pipeline."))),(0,o.kt)("hr",null),(0,o.kt)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,o.kt)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,o.kt)("iframe",{src:"https://user-images.githubusercontent.com/103921419/184726133-51bf76ec-31d7-4976-8d7d-68230c28e233.mp4",title:"Single Cluster Mode",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,o.kt)("p",null,"Here's how the Databricks UI looks for Prophecy's Single Cluster Mode."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Single Job Cluster",src:a(15973).Z,width:"3320",height:"1776"})),(0,o.kt)("h3",{id:"fully-configurable-cluster-mode"},"Fully Configurable Cluster Mode"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Coming Soon!!!")),(0,o.kt)("h2",{id:"job-monitoring"},"Job Monitoring"),(0,o.kt)("p",null,"Prophecy provides monitoring page which shows the status (enable/disable) of all the Jobs deployed via Prophecy and\nstatus of historic/current runs (success/failure/in-progress) for quick reference."),(0,o.kt)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,o.kt)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,o.kt)("iframe",{src:"https://user-images.githubusercontent.com/103921419/184726121-d2b7c5c7-ec01-48b1-9764-781292940f53.mp4",title:"Monitoring",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,o.kt)("h2",{id:"guides"},"Guides"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"/tutorials/low-code-jobs/multi-jobs-trigger"},"How to trigger a job from another job?")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"/tutorials/low-code-jobs/reliable-ci-cd"},"How to design a reliable CI/CD process?"))))}m.isMDXComponent=!0},51902:function(e,t,a){t.Z=a.p+"assets/images/databricks-job-config-example-e0806c65b871d08950145d379a564d04.png"},48391:function(e,t,a){t.Z=a.p+"assets/images/databricks-job-creation-from-pipeline-a5f148c2b2ce1ebfb3037517c2d53ed8.png"},98235:function(e,t,a){t.Z=a.p+"assets/images/databricks-job-creation-8b9fa7361efb938c441c3d5038733934.png"},20282:function(e,t,a){t.Z=a.p+"assets/images/databricks-job-example-44729011c5907c78c30c0b8f86276e40.png"},62446:function(e,t,a){t.Z=a.p+"assets/images/databricks-jobs-code-view-b0a6f49e0f77cb400aa84d00e72ed68f.png"},93532:function(e,t,a){t.Z=a.p+"assets/images/databricks-jobs-multi-cluster-eg-66a6d04dd6284d3a717d06d48ffca37a.png"},41290:function(e,t,a){t.Z=a.p+"assets/images/databricks-jobs-pipeline-config-e7e41f2a83032abd13757a6ef3d17c12.png"},62226:function(e,t,a){t.Z=a.p+"assets/images/databricks-jobs-script-config-1ecac2eda041dac20f670f3a9380faf1.png"},15973:function(e,t,a){t.Z=a.p+"assets/images/databricks-jobs-single-cluster-eg-32e4fb49f206dea7c50b81079cc7f3fe.png"}}]);