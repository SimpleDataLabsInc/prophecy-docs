"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[463],{15680:(e,t,r)=>{r.d(t,{xA:()=>p,yg:()=>f});var n=r(96540);function o(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function a(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function i(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?a(Object(r),!0).forEach((function(t){o(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):a(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function s(e,t){if(null==e)return{};var r,n,o=function(e,t){if(null==e)return{};var r,n,o={},a=Object.keys(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||(o[r]=e[r]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(o[r]=e[r])}return o}var c=n.createContext({}),l=function(e){var t=n.useContext(c),r=t;return e&&(r="function"==typeof e?e(t):i(i({},t),e)),r},p=function(e){var t=l(e.components);return n.createElement(c.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},y=n.forwardRef((function(e,t){var r=e.components,o=e.mdxType,a=e.originalType,c=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=l(r),y=o,f=u["".concat(c,".").concat(y)]||u[y]||d[y]||a;return r?n.createElement(f,i(i({ref:t},p),{},{components:r})):n.createElement(f,i({ref:t},p))}));function f(e,t){var r=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=r.length,i=new Array(a);i[0]=y;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[u]="string"==typeof e?e:o,i[1]=s;for(var l=2;l<a;l++)i[l]=r[l];return n.createElement.apply(null,i)}return n.createElement.apply(null,r)}y.displayName="MDXCreateElement"},25109:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>l});var n=r(58168),o=(r(96540),r(15680));const a={title:"Orchestration",id:"Orchestration",description:"Airflow and Databricks Jobs",tags:["jobs","deployment","scheduling"]},i=void 0,s={unversionedId:"Orchestration/Orchestration",id:"Orchestration/Orchestration",title:"Orchestration",description:"Airflow and Databricks Jobs",source:"@site/docs/Orchestration/Orchestration.md",sourceDirName:"Orchestration",slug:"/Orchestration/",permalink:"/Orchestration/",draft:!1,tags:[{label:"jobs",permalink:"/tags/jobs"},{label:"deployment",permalink:"/tags/deployment"},{label:"scheduling",permalink:"/tags/scheduling"}],version:"current",frontMatter:{title:"Orchestration",id:"Orchestration",description:"Airflow and Databricks Jobs",tags:["jobs","deployment","scheduling"]},sidebar:"defaultSidebar",previous:{title:"Use project tests",permalink:"/SQL/data-tests/use-project-tests"},next:{title:"Databricks Jobs",permalink:"/Orchestration/databricks-jobs"}},c={},l=[],p={toc:l},u="wrapper";function d(e){let{components:t,...r}=e;return(0,o.yg)(u,(0,n.A)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Once you have developed a Spark data Pipeline or an SQL Model using Prophecy, you will want to schedule it to run at some frequency. To\nsupport this, Prophecy provides you with an easy-to-use interface to develop Jobs, using two different\nschedulers:"),(0,o.yg)("ol",null,(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("p",{parentName:"li"},(0,o.yg)("strong",{parentName:"p"},(0,o.yg)("a",{parentName:"strong",href:"/Orchestration/databricks-jobs"},"Databricks Jobs"))," - for simpler data-Pipeline use-cases, where you just\norchestrate multiple data-Pipelines to run together. Databricks Jobs is a ",(0,o.yg)("strong",{parentName:"p"},"recommended")," scheduler, if you're\nDatabricks Native.")),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("p",{parentName:"li"},(0,o.yg)("strong",{parentName:"p"},(0,o.yg)("a",{parentName:"strong",href:"/Orchestration/airflow/"},"Airflow"))," - for more complex use-cases, where you have to use various operators, or need\nany additional data pre-and-post-processing, you can interface from Prophecy with your production-ready Airflow deployment. To get started with your first Airflow jobs, try Prophecy Managed Airflow using this ",(0,o.yg)("a",{parentName:"p",href:"/getting-started/airflow"},"guide"),"."))),(0,o.yg)("p",null,"Alternatively, since Prophecy provides you native Spark code on Git, you can easily integrate with any other scheduler."))}d.isMDXComponent=!0}}]);