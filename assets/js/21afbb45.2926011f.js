"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[36656],{15680:(e,t,a)=>{a.d(t,{xA:()=>p,yg:()=>y});var n=a(96540);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var c=n.createContext({}),l=function(e){var t=n.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},p=function(e){var t=l(e.components);return n.createElement(c.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},g=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,c=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),d=l(a),g=r,y=d["".concat(c,".").concat(g)]||d[g]||u[g]||i;return a?n.createElement(y,s(s({ref:t},p),{},{components:a})):n.createElement(y,s({ref:t},p))}));function y(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,s=new Array(i);s[0]=g;var o={};for(var c in t)hasOwnProperty.call(t,c)&&(o[c]=t[c]);o.originalType=e,o[d]="string"==typeof e?e:r,s[1]=o;for(var l=2;l<i;l++)s[l]=a[l];return n.createElement.apply(null,s)}return n.createElement.apply(null,a)}g.displayName="MDXCreateElement"},23069:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>u,frontMatter:()=>i,metadata:()=>o,toc:()=>l});var n=a(58168),r=(a(96540),a(15680));const i={title:"Fabrics",id:"Fabric",description:"Logical execution environments",sidebar_position:4,tags:["concepts","fabric","source","target","datasets"]},s=void 0,o={unversionedId:"concepts/fabrics/Fabric",id:"concepts/fabrics/Fabric",title:"Fabrics",description:"Logical execution environments",source:"@site/docs/concepts/fabrics/fabrics.md",sourceDirName:"concepts/fabrics",slug:"/concepts/fabrics/",permalink:"/concepts/fabrics/",draft:!1,tags:[{label:"concepts",permalink:"/tags/concepts"},{label:"fabric",permalink:"/tags/fabric"},{label:"source",permalink:"/tags/source"},{label:"target",permalink:"/tags/target"},{label:"datasets",permalink:"/tags/datasets"}],version:"current",sidebarPosition:4,frontMatter:{title:"Fabrics",id:"Fabric",description:"Logical execution environments",sidebar_position:4,tags:["concepts","fabric","source","target","datasets"]},sidebar:"mySidebar",previous:{title:"Datasets",permalink:"/concepts/project/dataset"},next:{title:"Concepts",permalink:"/concepts/"}},c={},l=[{value:"Fabric creation",id:"fabric-creation",level:2},{value:"Use case",id:"use-case",level:2},{value:"Components",id:"components",level:2},{value:"Fabric metadata",id:"fabric-metadata",level:2},{value:"Hands-on",id:"hands-on",level:2}],p={toc:l},d="wrapper";function u(e){let{components:t,...i}=e;return(0,r.yg)(d,(0,n.A)({},p,i,{components:t,mdxType:"MDXLayout"}),(0,r.yg)("p",null,"Prophecy helps you develop data pipelines in high-quality Spark or SQL code\u2014but what does Prophecy use to compute these pipelines? The first thing to understand before building any pipeline is that your pipeline must be connected to an ",(0,r.yg)("strong",{parentName:"p"},"execution environment"),"."),(0,r.yg)("p",null,"This is why ",(0,r.yg)("strong",{parentName:"p"},"fabrics")," exist in Prophecy. Fabrics let Prophecy connect to specific execution environments."),(0,r.yg)("p",null,"Prophecy provides a Prophecy-managed fabric that can get you started with building your pipelines. However, you can also create your own fabrics to connect to other execution environments, such as a Databricks workspace. When you attach to an external execution environment, you can access the data sources available to you in that environment."),(0,r.yg)("h2",{id:"fabric-creation"},"Fabric creation"),(0,r.yg)("p",null,"A team admin typically sets up fabrics. Detailed steps for fabric creation can be found in the ",(0,r.yg)("a",{parentName:"p",href:"/administration/Spark-fabrics/Fabrics"},"Set up Spark fabrics")," and ",(0,r.yg)("a",{parentName:"p",href:"/administration/sql-fabrics/Fabrics"},"Set up SQL fabrics")," sections of the documentation."),(0,r.yg)("p",null,"Even though teams share fabrics, ",(0,r.yg)("strong",{parentName:"p"},"each user must add their individual credentials")," to be able to use the fabric in their projects."),(0,r.yg)("h2",{id:"use-case"},"Use case"),(0,r.yg)("p",null,"Here is one way you might set up your fabrics. First, the team admin creates:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"A team named Marketing_DSS for the Marketing Decision Support System users."),(0,r.yg)("li",{parentName:"ul"},"A ",(0,r.yg)("inlineCode",{parentName:"li"},"dev")," fabric for development activities that specifies the Marketing_DSS team."),(0,r.yg)("li",{parentName:"ul"},"A ",(0,r.yg)("inlineCode",{parentName:"li"},"prod")," fabric for production pipelines that specifies the Marketing_DSS team.")),(0,r.yg)("p",null,"In this example, all users in the Marketing_DSS Team will have access to the ",(0,r.yg)("inlineCode",{parentName:"p"},"dev")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"prod")," fabrics."),(0,r.yg)("h2",{id:"components"},"Components"),(0,r.yg)("p",null,"Fabrics include everything required to run a data pipeline. As an example, the following table describes the components of a Spark Databricks fabric."),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Component"),(0,r.yg)("th",{parentName:"tr",align:null},"Description"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"Connection Credentials"),(0,r.yg)("td",{parentName:"tr",align:null},"Includes details like ",(0,r.yg)("em",{parentName:"td"},"Workspace URL")," and ",(0,r.yg)("em",{parentName:"td"},"Access Token")," for Databricks.")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"Cluster Configuration"),(0,r.yg)("td",{parentName:"tr",align:null},"Defines settings such as ",(0,r.yg)("em",{parentName:"td"},"Databricks Runtime Version"),", ",(0,r.yg)("em",{parentName:"td"},"Machine Type"),", and ",(0,r.yg)("em",{parentName:"td"},"Idle Timeout"),".")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"Job Sizes"),(0,r.yg)("td",{parentName:"tr",align:null},"Lets you define reusable cluster sizes (e.g., an XL cluster with 10 i3.xlarge servers, 40 CPUs, and 70GB memory).")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"Scheduler"),(0,r.yg)("td",{parentName:"tr",align:null},"Executes Spark data pipelines on a defined schedule, such as weekdays at 9:00 AM. Databricks provides a default scheduler, and an Airflow Scheduler is available for enterprise users.")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"Database Connections"),(0,r.yg)("td",{parentName:"tr",align:null},"Supports connections to databases (MySQL, Postgres) and data warehouses (Snowflake) via JDBC or other protocols. Credentials are securely stored on the fabric for reuse.")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"Metadata Connection"),(0,r.yg)("td",{parentName:"tr",align:null},"Enhances fabric management for large datasets, useful for users handling hundreds or thousands of tables. ",(0,r.yg)("a",{parentName:"td",href:"/administration/metadata-connections"},"Learn more"),".")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"Credentials & Secrets"),(0,r.yg)("td",{parentName:"tr",align:null},"Securely stores credentials in Databricks using Personal Access Tokens (PAT) or ",(0,r.yg)("a",{parentName:"td",href:"/administration/authentication/databricks_oauth"},"Databricks OAuth"),". Secrets are stored as key-value pairs, accessible only to running workflows.")))),(0,r.yg)("h2",{id:"fabric-metadata"},"Fabric metadata"),(0,r.yg)("p",null,"A list of all fabrics available to you can be found in the ",(0,r.yg)("strong",{parentName:"p"},"Fabrics")," tab of the ",(0,r.yg)("strong",{parentName:"p"},"Metadata")," page."),(0,r.yg)("p",null,(0,r.yg)("img",{alt:"Fabric Metadata",src:a(41724).A,width:"2880",height:"686"})),(0,r.yg)("p",null,"You can click into each fabric to access the fabric settings. These will resemble the settings that appear during fabric creation."),(0,r.yg)("h2",{id:"hands-on"},"Hands-on"),(0,r.yg)("p",null,"Get started with hands-on guides. Learn step by step how to connect to your execution engine by creating a fabric:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"Create a SQL fabric with a JDBC or Unity Catalog connection following ",(0,r.yg)("a",{parentName:"li",href:"/getting-started/tutorials/sql-with-databricks"},"this guide"),"."),(0,r.yg)("li",{parentName:"ol"},"Create a Databricks fabric following ",(0,r.yg)("a",{parentName:"li",href:"/administration/Spark-fabrics/databricks/"},"these steps"),"."),(0,r.yg)("li",{parentName:"ol"},"Create an EMR fabric with Livy step by step ",(0,r.yg)("a",{parentName:"li",href:"/administration/Spark-fabrics/emr"},"here"),".")))}u.isMDXComponent=!0},41724:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/fabric_metadata_1-3ee2c76c404d725d4206ea6a41a62026.png"}}]);