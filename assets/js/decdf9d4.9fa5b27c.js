"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[1679],{3905:(e,o,t)=>{t.d(o,{Zo:()=>u,kt:()=>w});var r=t(67294);function n(e,o,t){return o in e?Object.defineProperty(e,o,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[o]=t,e}function a(e,o){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);o&&(r=r.filter((function(o){return Object.getOwnPropertyDescriptor(e,o).enumerable}))),t.push.apply(t,r)}return t}function i(e){for(var o=1;o<arguments.length;o++){var t=null!=arguments[o]?arguments[o]:{};o%2?a(Object(t),!0).forEach((function(o){n(e,o,t[o])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(o){Object.defineProperty(e,o,Object.getOwnPropertyDescriptor(t,o))}))}return e}function l(e,o){if(null==e)return{};var t,r,n=function(e,o){if(null==e)return{};var t,r,n={},a=Object.keys(e);for(r=0;r<a.length;r++)t=a[r],o.indexOf(t)>=0||(n[t]=e[t]);return n}(e,o);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)t=a[r],o.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var s=r.createContext({}),c=function(e){var o=r.useContext(s),t=o;return e&&(t="function"==typeof e?e(o):i(i({},o),e)),t},u=function(e){var o=c(e.components);return r.createElement(s.Provider,{value:o},e.children)},d="mdxType",p={inlineCode:"code",wrapper:function(e){var o=e.children;return r.createElement(r.Fragment,{},o)}},f=r.forwardRef((function(e,o){var t=e.components,n=e.mdxType,a=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),d=c(t),f=n,w=d["".concat(s,".").concat(f)]||d[f]||p[f]||a;return t?r.createElement(w,i(i({ref:o},u),{},{components:t})):r.createElement(w,i({ref:o},u))}));function w(e,o){var t=arguments,n=o&&o.mdxType;if("string"==typeof e||n){var a=t.length,i=new Array(a);i[0]=f;var l={};for(var s in o)hasOwnProperty.call(o,s)&&(l[s]=o[s]);l.originalType=e,l[d]="string"==typeof e?e:n,i[1]=l;for(var c=2;c<a;c++)i[c]=t[c];return r.createElement.apply(null,i)}return r.createElement.apply(null,t)}f.displayName="MDXCreateElement"},13847:(e,o,t)=>{t.r(o),t.d(o,{assets:()=>s,contentTitle:()=>i,default:()=>p,frontMatter:()=>a,metadata:()=>l,toc:()=>c});var r=t(87462),n=(t(67294),t(3905));const a={sidebar_position:2,title:"Airflow",id:"airflow",description:"How Prophecy integrates with Airflow",tags:["scheduling","airflow","jobs"]},i=void 0,l={unversionedId:"low-code-jobs/airflow",id:"low-code-jobs/airflow",title:"Airflow",description:"How Prophecy integrates with Airflow",source:"@site/docs/low-code-jobs/airflow.md",sourceDirName:"low-code-jobs",slug:"/low-code-jobs/airflow",permalink:"/low-code-jobs/airflow",draft:!1,tags:[{label:"scheduling",permalink:"/tags/scheduling"},{label:"airflow",permalink:"/tags/airflow"},{label:"jobs",permalink:"/tags/jobs"}],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"Airflow",id:"airflow",description:"How Prophecy integrates with Airflow",tags:["scheduling","airflow","jobs"]},sidebar:"defaultSidebar",previous:{title:"Databricks Jobs",permalink:"/low-code-jobs/databricks-jobs"},next:{title:"Package Hub",permalink:"/package-hub/"}},s={},c=[{value:"Airflow Jobs in Prophecy",id:"airflow-jobs-in-prophecy",level:2},{value:"How to create an Airflow Fabric",id:"how-to-create-an-airflow-fabric",level:3},{value:"How to create an Airflow Job",id:"how-to-create-an-airflow-job",level:3},{value:"Running and scheduling Airflow Jobs",id:"running-and-scheduling-airflow-jobs",level:3}],u={toc:c},d="wrapper";function p(e){let{components:o,...t}=e;return(0,n.kt)(d,(0,r.Z)({},u,t,{components:o,mdxType:"MDXLayout"}),(0,n.kt)("admonition",{title:"Coming Soon",type:"info"},(0,n.kt)("p",{parentName:"admonition"},"Detailed documentation on this feature, including setup instructions and best practices")),(0,n.kt)("p",null,"Apache Airflow is an open-source platform used to programmatically author, schedule, and monitor workflows."),(0,n.kt)("p",null,"Airflow allows users to define workflows as DAGs (Directed Acyclic Graphs), where each node in the graph represents a task that needs to be executed.\nTasks can be Python functions, scripts, or executable files. Dependencies between tasks are defined using operators, which are essentially plugins that define how tasks interact with each other."),(0,n.kt)("p",null,"Apache Airflow is particularly useful for creating and managing data pipelines. With its DAG-based architecture, it makes it easy to schedule and run complex data workflows. It also provides a powerful interface for monitoring and troubleshooting these workflows."),(0,n.kt)("h2",{id:"airflow-jobs-in-prophecy"},"Airflow Jobs in Prophecy"),(0,n.kt)("p",null,"With Prophecy, you can create and manage Airflow jobs using a visual Drag and drop interface. This allows you to easily design and schedule complex workflows, without having to write any code."),(0,n.kt)("h3",{id:"how-to-create-an-airflow-fabric"},"How to create an Airflow Fabric"),(0,n.kt)("p",null,'To create an Airflow Fabric in Prophecy, you can use the "Airflow" Fabric type. This allows you to specify the parameters needed to connect to your Airflow instance based on the Provider. Currently, for Cloud Composer you would provider Airflow URL, Project Id, Dag location, and authentication key.'),(0,n.kt)("p",null,"Please see below video for example."),(0,n.kt)("h3",{id:"how-to-create-an-airflow-job"},"How to create an Airflow Job"),(0,n.kt)("p",null,"Once you've created the Fabric, you can use it to create and manage Airflow jobs within Prophecy. This makes it easy to build and schedule complex workflows without having to leave the Prophecy platform."),(0,n.kt)("p",null,"To create an Airflow Job in Prophecy, you can use the visual interface to define a DAG and its associated tasks. You can specify the already created Pipelines, and add your own scripts to be executed as tasks, and use the operators provided by Airflow to define dependencies between them.\nPlease see below video for example."),(0,n.kt)("h3",{id:"running-and-scheduling-airflow-jobs"},"Running and scheduling Airflow Jobs"),(0,n.kt)("p",null,"Once you've defined your DAG, you can schedule it to run at specific intervals, or trigger it manually when needed.\nProphecy provides a powerful monitoring interface that allows you to track the progress of your DAGs and quickly identify any issues that may arise.\nPlease see below video for example"),(0,n.kt)("p",null,"Once done, Simply enable them and release your Project to schedule these Jobs on Airflow."))}p.isMDXComponent=!0}}]);