"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[22559],{15680:(e,a,t)=>{t.d(a,{xA:()=>p,yg:()=>y});var r=t(96540);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}function o(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,r,n=function(e,a){if(null==e)return{};var t,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var c=r.createContext({}),l=function(e){var a=r.useContext(c),t=a;return e&&(t="function"==typeof e?e(a):o(o({},a),e)),t},p=function(e){var a=l(e.components);return r.createElement(c.Provider,{value:a},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var a=e.children;return r.createElement(r.Fragment,{},a)}},b=r.forwardRef((function(e,a){var t=e.components,n=e.mdxType,i=e.originalType,c=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=l(t),b=n,y=u["".concat(c,".").concat(b)]||u[b]||d[b]||i;return t?r.createElement(y,o(o({ref:a},p),{},{components:t})):r.createElement(y,o({ref:a},p))}));function y(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var i=t.length,o=new Array(i);o[0]=b;var s={};for(var c in a)hasOwnProperty.call(a,c)&&(s[c]=a[c]);s.originalType=e,s[u]="string"==typeof e?e:n,o[1]=s;for(var l=2;l<i;l++)o[l]=t[l];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}b.displayName="MDXCreateElement"},37807:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>l});var r=t(58168),n=(t(96540),t(15680));const i={title:"Databricks",id:"databricks-fabric",description:"Configuring Databricks Fabric",sidebar_position:2,tags:["concepts","Fabric","databricks","livy","prophecyManaged"]},o=void 0,s={unversionedId:"Spark/fabrics/databricks-fabric",id:"Spark/fabrics/databricks-fabric",title:"Databricks",description:"Configuring Databricks Fabric",source:"@site/docs/Spark/fabrics/databricks.md",sourceDirName:"Spark/fabrics",slug:"/Spark/fabrics/databricks-fabric",permalink:"/Spark/fabrics/databricks-fabric",draft:!1,tags:[{label:"concepts",permalink:"/tags/concepts"},{label:"Fabric",permalink:"/tags/fabric"},{label:"databricks",permalink:"/tags/databricks"},{label:"livy",permalink:"/tags/livy"},{label:"prophecyManaged",permalink:"/tags/prophecy-managed"}],version:"current",sidebarPosition:2,frontMatter:{title:"Databricks",id:"databricks-fabric",description:"Configuring Databricks Fabric",sidebar_position:2,tags:["concepts","Fabric","databricks","livy","prophecyManaged"]},sidebar:"defaultSidebar",previous:{title:"Prophecy Managed",permalink:"/Spark/fabrics/prophecy-managed-databricks"},next:{title:"Livy",permalink:"/Spark/fabrics/livy"}},c={},l=[{value:"Databricks Execution",id:"databricks-execution",level:2}],p={toc:l},u="wrapper";function d(e){let{components:a,...i}=e;return(0,n.yg)(u,(0,r.A)({},p,i,{components:a,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"Create a Databricks Fabric to connect Prophecy to your existing Databricks environment. Think of a Fabric as connection to your ",(0,n.yg)("a",{parentName:"p",href:"https://docs.databricks.com/workspace/index.html#navigate-the-workspace"},"Databricks workspace"),".\nThis Fabric enables Prophecy to connect to existing Spark clusters (or create new ones), execute Spark pipelines, read and write data, etc - all according to each user's permissions defined by their personal access token."),(0,n.yg)("p",null,"Please refer to the video below for a step-by-step example."),(0,n.yg)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,n.yg)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,n.yg)("iframe",{src:"https://user-images.githubusercontent.com/121796483/217735090-41853091-ef2e-4d60-bdf6-62fe31a7ee3b.mp4",title:"Databricks Fabric",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Databricks Credentials")," - Here you will provide your Databricks Workspace URL and ",(0,n.yg)("a",{parentName:"li",href:"https://docs.databricks.com/dev-tools/api/latest/authentication.html#generate-a-personal-access-token"},"Personal Access Token")," (PAT). The PAT must have permission to attach clusters. If you'd like to create clusters or read/write data from Prophecy, then these permissions should be enabled for the PAT as well. Keep in mind each user will need to use their own PAT in the Fabric. Prophecy respects the permissions scoped to each user."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Cluster Details")," - Here you would need to provide the ",(0,n.yg)("a",{parentName:"li",href:"https://docs.databricks.com/runtime/dbr.html#databricks-runtime"},"Databricks Runtime version"),", Executor and Drive Machine Types and Termination Timeout if any. These cluster details will be used when creating a cluster via Prophecy during Interactive development and for job clusters during Scheduled Databricks Job runs."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Job sizes")," - User can create Job sizes here using which clusters can be spawned while testing through prophecy IDE. Here you can provide Cluster mode, Databricks Runtime version, total number of the Executors, Core and Memory for them, etc. This provides all the options which are available on Databricks while spawning clusters through Databricks. We recommend using the smallest machines and smallest number of nodes appropriate for your use case.")),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Editing a Job",src:t(3507).A,width:"1283",height:"1068"})),(0,n.yg)("p",null,"In Json you can just copy-paste your compute Json from Databricks."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Prophecy Library")," - These are some Scala and Python libraries written by Prophecy to provide additional functionalities on top of Spark. These would get automatically installed in your Spark execution environment when you attach to a cluster/create new cluster. These libraries are also publicly available on Maven central and Pypi respectively."),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Metadata Connection")," - Optionally, enhance your Fabric by creating a ",(0,n.yg)("a",{parentName:"li",href:"/metadata/metadata-connections"},"Metadata Connection"),", recommended for users with hundreds or thousands of tables housed in their data provider(s).")),(0,n.yg)("h2",{id:"databricks-execution"},"Databricks Execution"),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"/Spark/execution/executions_on_databricks_clusters"},"Execution on Databricks")),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"/Spark/execution/interactive-execution"},"Interactive Execution")),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"/Spark/execution/execution-metrics"},"Execution Metrics")))}d.isMDXComponent=!0},3507:(e,a,t)=>{t.d(a,{A:()=>r});const r=t.p+"assets/images/job_size_new-f8b58a9a3298e4d5e3b3efbff4b0d61c.png"}}]);