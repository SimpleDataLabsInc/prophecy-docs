"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[5707],{3905:function(e,t,a){a.d(t,{Zo:function(){return p},kt:function(){return b}});var r=a(67294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var c=r.createContext({}),l=function(e){var t=r.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=l(e.components);return r.createElement(c.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,c=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=l(a),b=n,m=u["".concat(c,".").concat(b)]||u[b]||d[b]||i;return a?r.createElement(m,o(o({ref:t},p),{},{components:a})):r.createElement(m,o({ref:t},p))}));function b(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,o=new Array(i);o[0]=u;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:n,o[1]=s;for(var l=2;l<i;l++)o[l]=a[l];return r.createElement.apply(null,o)}return r.createElement.apply(null,a)}u.displayName="MDXCreateElement"},78508:function(e,t,a){a.r(t),a.d(t,{assets:function(){return p},contentTitle:function(){return c},default:function(){return b},frontMatter:function(){return s},metadata:function(){return l},toc:function(){return d}});var r=a(87462),n=a(63366),i=(a(67294),a(3905)),o=["components"],s={title:"How to create a Fabric",id:"create-a-fabric",description:"Guide on how to create Fabrics",sidebar_position:1,tags:["concepts","fabric","databricks","livy","prophecyManaged"]},c=void 0,l={unversionedId:"concepts/fabrics/create-a-fabric",id:"concepts/fabrics/create-a-fabric",title:"How to create a Fabric",description:"Guide on how to create Fabrics",source:"@site/docs/concepts/fabrics/create_a_fabric.md",sourceDirName:"concepts/fabrics",slug:"/concepts/fabrics/create-a-fabric",permalink:"/concepts/fabrics/create-a-fabric",draft:!1,tags:[{label:"concepts",permalink:"/tags/concepts"},{label:"fabric",permalink:"/tags/fabric"},{label:"databricks",permalink:"/tags/databricks"},{label:"livy",permalink:"/tags/livy"},{label:"prophecyManaged",permalink:"/tags/prophecy-managed"}],version:"current",sidebarPosition:1,frontMatter:{title:"How to create a Fabric",id:"create-a-fabric",description:"Guide on how to create Fabrics",sidebar_position:1,tags:["concepts","fabric","databricks","livy","prophecyManaged"]},sidebar:"defaultSidebar",previous:{title:"Fabrics",permalink:"/concepts/fabrics/"},next:{title:"Pipelines",permalink:"/concepts/pipeline"}},p={},d=[{value:"<strong>Prophecy Managed</strong>",id:"prophecy-managed",level:3},{value:"<strong>Databricks</strong>",id:"databricks",level:3},{value:"<strong>Livy</strong>",id:"livy",level:3}],u={toc:d};function b(e){var t=e.components,s=(0,n.Z)(e,o);return(0,i.kt)("wrapper",(0,r.Z)({},u,s,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Prophecy provides you with three different types of fabrics."),(0,i.kt)("h3",{id:"prophecy-managed"},(0,i.kt)("strong",{parentName:"h3"},"Prophecy Managed")),(0,i.kt)("p",null,"Using this option, you can create a 14-Day Free Trial Fabric, On Prophecy Managed Databricks. You can use this when trying out Prophecy and when you don't want to connect your own Spark Execution Environment to Prophecy. We already have some sample data and tables created to try out the different functionalities.\nPlease refer below video for step-by-step example"),(0,i.kt)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,i.kt)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,i.kt)("iframe",{src:"https://user-images.githubusercontent.com/121796483/217787623-1cf01df2-54d6-4338-bd59-bd921e101ce9.mp4",title:"Databricks Fabric",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,i.kt)("p",null,"In this Fabric you can only change the ",(0,i.kt)("a",{parentName:"p",href:"https://docs.databricks.com/runtime/dbr.html#databricks-runtime"},"Databricks Runtime version"),". The auto-termination timeout, Executor and Driver Machine Type and Job sizes are uneditable."),(0,i.kt)("h3",{id:"databricks"},(0,i.kt)("strong",{parentName:"h3"},"Databricks")),(0,i.kt)("p",null,"To connect your own Databricks Workspace to Prophecy, you can use this option to create a Fabric. Think of a Fabric as connection to your ",(0,i.kt)("a",{parentName:"p",href:"https://docs.databricks.com/workspace/index.html#navigate-the-workspace"},"Databricks workspace"),"."),(0,i.kt)("p",null,"Please refer below video for step-by-step example"),(0,i.kt)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,i.kt)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,i.kt)("iframe",{src:"https://user-images.githubusercontent.com/121796483/217735090-41853091-ef2e-4d60-bdf6-62fe31a7ee3b.mp4",title:"Databricks Fabric",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Databricks Credentials")," - Here you will provide your Databricks Workspace URL and the ",(0,i.kt)("a",{parentName:"li",href:"https://docs.databricks.com/dev-tools/api/latest/authentication.html#generate-a-personal-access-token"},"Personal Access token")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Cluster Details")," - Here you would need to provide the ",(0,i.kt)("a",{parentName:"li",href:"https://docs.databricks.com/runtime/dbr.html#databricks-runtime"},"Databricks Runtime version"),", Executor and Drive Machine Types and Termination Timeout if any. These cluster details will be used when creating a cluster via Prophecy during Interactive development and for job clusters during Scheduled Databricks Job runs."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Job sizes")," - By default, you will see a Small Job size pre created. You can edit or add more Job sizes.Here you can provide total number of the Executors and Core and Memory for them.\nWhen you click on edit for a Job size, you will see two options: ",(0,i.kt)("em",{parentName:"li"},"Basic")," and ",(0,i.kt)("em",{parentName:"li"},"Advanced(Json)"),".")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Editing a Job",src:a(34608).Z,width:"540",height:"462"})),(0,i.kt)("p",null,"In Advanced Json you can just copy-paste your compute Json from Databricks. Please refer below video for example:"),(0,i.kt)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,i.kt)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,i.kt)("iframe",{src:"https://user-images.githubusercontent.com/121796483/218055564-76860e69-5811-41a7-b18a-6d96e19096ee.mp4",title:"Databricks Fabric",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Prophecy Library")," - These are some Scala and Python libraries written by Prophecy to provide additional functionalities on top of Spark. These would get automatically installed in your Spark execution environment when you attach to a cluster/create new cluster. These libraries are also publicly available on Maven central and Pypi respectively.")),(0,i.kt)("h3",{id:"livy"},(0,i.kt)("strong",{parentName:"h3"},"Livy")),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://livy.apache.org/"},"Apache Livy")," is a service that enables easy interaction with a Spark cluster over a REST interface. If you're running Spark-on-hadoop, most Hadoop distributions (CDP/MapR) come with livy bundled, you just need to enable it. For Spark-on-k8s, you can put a livy in the k8s cluster which exposes Spark over rest API."),(0,i.kt)("p",null,"Please refer below video for step-by-step example"),(0,i.kt)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,i.kt)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,i.kt)("iframe",{src:"https://user-images.githubusercontent.com/121796483/217732038-d01bbfbe-a140-4661-a279-1b4858ab2285.mp4",title:"Livy Fabric",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Spark Connection")," - Here you will provide the Livy URL, Authentication, Spark version and Scala version.")),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"The Spark and Scala versions are now mandatory with recent ",(0,i.kt)("a",{parentName:"p",href:"/release_notes/Feb_2023#spark-and-scala-versions-are-now-required-in-livy-fabrics"},"Release"),".\nIf users has an old Fabric which doesn't have Spark abd Scala versions present, an error (seen below) will appear when trying to attach to a cluster. User would need to update the Fabric from the metadata page or by clicking ",(0,i.kt)("inlineCode",{parentName:"p"},"Update Fabric")," button (seen below).\n",(0,i.kt)("img",{alt:"Fabric_misconfigured",src:a(77641).Z,width:"539",height:"179"})))),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Job sizes")," -\nBy default, you will see a Small Job size pre created. You can edit or add more Job sizes. A Job sie consists off"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Size of the Driver: Driver Core and Memory"),(0,i.kt)("li",{parentName:"ul"},"Size of the Executor: Core and Memory for each executor"),(0,i.kt)("li",{parentName:"ul"},"Total number of Executors"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Prophecy Library")," -\nThese are some Scala and Python libraries written by Prophecy to provide additional functionalities on top of Spark. These would get automatically installed in your Spark execution environment when you attach to a cluster/create new cluster. These libraries are also publicly available on Maven central and Pypi respectively.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Spark Config")," -\nThese are additional ",(0,i.kt)("a",{parentName:"p",href:"https://spark.apache.org/docs/latest/configuration.html#available-properties"},"Spark Properties")," which you can set which would be applied at Spark session initialisation.\nFor example if your Spark installation is configured to have dynamic allocation enabled, you can disable it for sessions created through Prophecy."))))}b.isMDXComponent=!0},77641:function(e,t,a){t.Z=a.p+"assets/images/fabric_misconfigured-c1bba4f511abe79291f82f8fcb674144.png"},34608:function(e,t,a){t.Z=a.p+"assets/images/job_size-c4f1d7559cbd45b766846baabac7ed16.png"}}]);