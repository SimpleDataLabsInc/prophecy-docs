"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[9571],{3905:(e,t,o)=>{o.d(t,{Zo:()=>p,kt:()=>f});var r=o(67294);function n(e,t,o){return t in e?Object.defineProperty(e,t,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[t]=o,e}function a(e,t){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),o.push.apply(o,r)}return o}function i(e){for(var t=1;t<arguments.length;t++){var o=null!=arguments[t]?arguments[t]:{};t%2?a(Object(o),!0).forEach((function(t){n(e,t,o[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):a(Object(o)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(o,t))}))}return e}function s(e,t){if(null==e)return{};var o,r,n=function(e,t){if(null==e)return{};var o,r,n={},a=Object.keys(e);for(r=0;r<a.length;r++)o=a[r],t.indexOf(o)>=0||(n[o]=e[o]);return n}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)o=a[r],t.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(n[o]=e[o])}return n}var l=r.createContext({}),c=function(e){var t=r.useContext(l),o=t;return e&&(o="function"==typeof e?e(t):i(i({},t),e)),o},p=function(e){var t=c(e.components);return r.createElement(l.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},b=r.forwardRef((function(e,t){var o=e.components,n=e.mdxType,a=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=c(o),b=n,f=d["".concat(l,".").concat(b)]||d[b]||u[b]||a;return o?r.createElement(f,i(i({ref:t},p),{},{components:o})):r.createElement(f,i({ref:t},p))}));function f(e,t){var o=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var a=o.length,i=new Array(a);i[0]=b;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[d]="string"==typeof e?e:n,i[1]=s;for(var c=2;c<a;c++)i[c]=o[c];return r.createElement.apply(null,i)}return r.createElement.apply(null,o)}b.displayName="MDXCreateElement"},61105:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var r=o(87462),n=(o(67294),o(3905));const a={title:"Low-code Jobs",id:"low-code-jobs",description:"Low-code Jobs",tags:["jobs","deployment","scheduling"]},i=void 0,s={unversionedId:"low-code-jobs/low-code-jobs",id:"low-code-jobs/low-code-jobs",title:"Low-code Jobs",description:"Low-code Jobs",source:"@site/docs/low-code-jobs/low-code-jobs.md",sourceDirName:"low-code-jobs",slug:"/low-code-jobs/",permalink:"/low-code-jobs/",draft:!1,tags:[{label:"jobs",permalink:"/tags/jobs"},{label:"deployment",permalink:"/tags/deployment"},{label:"scheduling",permalink:"/tags/scheduling"}],version:"current",frontMatter:{title:"Low-code Jobs",id:"low-code-jobs",description:"Low-code Jobs",tags:["jobs","deployment","scheduling"]},sidebar:"defaultSidebar",previous:{title:"Data Explorer",permalink:"/low-code-sql/data-explorer"},next:{title:"Databricks Jobs",permalink:"/low-code-jobs/databricks-jobs"}},l={},c=[],p={toc:c},d="wrapper";function u(e){let{components:t,...o}=e;return(0,n.kt)(d,(0,r.Z)({},p,o,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("p",null,"Once you have developed a Spark data Pipeline or an SQL Model using Prophecy, you will want to schedule it to run at some frequency. To\nsupport this, Prophecy provides you with an easy to use low-code interface to develop Jobs, using two different\nschedulers:"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("a",{parentName:"strong",href:"/low-code-jobs/databricks-jobs"},"Databricks Jobs"))," - for simpler data-Pipeline use-cases, where you just\norchestrate multiple data-Pipelines to run together. Databricks Jobs is a ",(0,n.kt)("strong",{parentName:"p"},"recommended")," scheduler, if you're\nDatabricks Native.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("a",{parentName:"strong",href:"/low-code-jobs/airflow/"},"Airflow"))," - for more complex use-cases, where you have to use various operators, or need\nany additional data pre-and-post-processing, you can interface from Prophecy with your production-ready Airflow deployment. To get started with your first Airflow jobs, try Prophecy Managed Airflow using this ",(0,n.kt)("a",{parentName:"p",href:"/getting-started/airflow-with-databricks"},"guide"),"."))),(0,n.kt)("p",null,"Alternatively, since Prophecy provides you native Spark code on Git, you can easily integrate with any other scheduler."))}u.isMDXComponent=!0}}]);