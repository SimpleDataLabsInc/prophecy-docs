"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[62001],{5067:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.25_Observability-5488e021ec6c36057f4ac60504e9304f.png"},10390:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.8_Complete_fabric-112c1d0ef2979116226b46db2b4e84fd.png"},13944:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.8_SF_Sql_connection-75aa10231a518c3993e9a80486d343d0.png"},28453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var t=i(96540);const o={},r=t.createContext(o);function s(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(r.Provider,{value:n},e.children)}},28651:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.18_Run_Job-c74a733a23a3662c9b2c4b204e5d6e77.png"},29382:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.17_Add_DBT_gem_details-e836f8bbe6f3f9d0771ffa1ec1a714f4.png"},31722:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.12_Add_email_gem-f2bba81229a0f484fdc25e31628bd56c.png"},34503:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.22_Pull_screen-b14f74d0c781f0e90f1ac0ac4107661b.png"},40263:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"Orchestration/airflow/airflow-tutorial","title":"Orchestration with Airflow","description":"A tutorial on orchestrating Spark and SQL jobs with Airflow","source":"@site/docs/Orchestration/airflow/getting-started-with-low-code-airflow.md","sourceDirName":"Orchestration/airflow","slug":"/Orchestration/airflow/airflow-tutorial","permalink":"/Orchestration/airflow/airflow-tutorial","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"airflow","permalink":"/tags/airflow"},{"inline":true,"label":"tutorial","permalink":"/tags/tutorial"}],"version":"current","frontMatter":{"title":"Orchestration with Airflow","id":"airflow-tutorial","description":"A tutorial on orchestrating Spark and SQL jobs with Airflow","tags":["airflow","tutorial"]},"sidebar":"mySidebar","previous":{"title":"Limits and Restrictions","permalink":"/Orchestration/airflow/prophecy-managed/prophecy_managed_airflow_fabric_limits"},"next":{"title":"Databricks Jobs","permalink":"/Orchestration/databricks-jobs"}}');var o=i(74848),r=i(28453);const s={title:"Orchestration with Airflow",id:"airflow-tutorial",description:"A tutorial on orchestrating Spark and SQL jobs with Airflow",tags:["airflow","tutorial"]},a=void 0,c={},d=[{value:"In this quick-start, we will show you how to use Prophecy Managed Airflow to Run and schedule your Spark and SQL pipelines",id:"in-this-quick-start-we-will-show-you-how-to-use-prophecy-managed-airflow-to-run-and-schedule-your-spark-and-sql-pipelines",level:4},{value:"You will need",id:"you-will-need",level:4},{value:"1. Setup Prophecy Fabric for Airflow",id:"1-setup-prophecy-fabric-for-airflow",level:2},{value:"1.1 Adding AWS Connection",id:"11-adding-aws-connection",level:3},{value:"1.3 Adding Email Connection",id:"13-adding-email-connection",level:3},{value:"1.3 Adding Databricks Spark Connection",id:"13-adding-databricks-spark-connection",level:3},{value:"1.4 Adding Databricks SQL Connection",id:"14-adding-databricks-sql-connection",level:3},{value:"1.5 Adding Snowflake SQL Connection",id:"15-adding-snowflake-sql-connection",level:3},{value:"2. Create an Airflow job",id:"2-create-an-airflow-job",level:2},{value:"2.1 Adding S3 file Sensor gem",id:"21-adding-s3-file-sensor-gem",level:3},{value:"2.2 Adding Email gem",id:"22-adding-email-gem",level:3},{value:"2.3 Adding Spark pipeline gem",id:"23-adding-spark-pipeline-gem",level:3},{value:"2.4 Adding SQL DBT gem",id:"24-adding-sql-dbt-gem",level:3},{value:"3. Run and Debug",id:"3-run-and-debug",level:2},{value:"4. Release and Schedule",id:"4-release-and-schedule",level:2},{value:"5. Monitor",id:"5-monitor",level:2},{value:"What\u2019s next?",id:"whats-next",level:2}];function l(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.p,{children:"Version 3.0.2 of our platform, Prophecy, introduces an exciting new integration: Orchestration with Airflow.\nNow, users can effortlessly create and manage Airflow jobs using a user-friendly drag-and-drop interface.\nThis empowers you to design and schedule intricate workflows without the need for coding expertise.\nThe tool seamlessly translates your designs into highly optimized Python Airflow code, stored in Git, ensuring complete accessibility and openness to all users.\nMoreover, you have the flexibility to enhance functionality by incorporating your custom operators and sensors via our Gem Builder interface."}),"\n",(0,o.jsx)("div",{style:{position:"relative","padding-bottom":"56.25%",height:0},children:(0,o.jsx)("iframe",{src:"https://www.loom.com/embed/e12bf4819ac84b93a3a6642df0f38450",frameborder:"0",webkitallowfullscreen:!0,mozallowfullscreen:!0,allowfullscreen:!0,style:{position:"absolute",top:0,left:0,width:"100%",height:"100%"}})}),"\n",(0,o.jsx)(n.h4,{id:"in-this-quick-start-we-will-show-you-how-to-use-prophecy-managed-airflow-to-run-and-schedule-your-spark-and-sql-pipelines",children:"In this quick-start, we will show you how to use Prophecy Managed Airflow to Run and schedule your Spark and SQL pipelines"}),"\n",(0,o.jsxs)(n.p,{children:["We'll take you step by step from connecting your compute cluster or warehouse to Prophecy Managed Airflow to creating your first ",(0,o.jsx)(n.a,{href:"https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html",children:"Airflow DAG"})," and scheduling it.\nBy the end of this training, you'll have an understanding of Airflow DAGs, be able to use our Visual interface to quickly create your DAG, and schedule this DAG on Prophecy Hosted Airflow. Let's dig in!"]}),"\n",(0,o.jsx)(n.h4,{id:"you-will-need",children:"You will need"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Databricks Account"}),"\n",(0,o.jsx)(n.li,{children:"A Prophecy project With Spark pipeline or SQL model running on Databricks"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"OR"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Snowflake Account"}),"\n",(0,o.jsx)(n.li,{children:"A Prophecy project with SQL model running on Snowflake"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"For this guide, let's create a job that gets activated whenever a new file is uploaded to an S3 bucket. Additionally, we'll configure it to send an email notification prior to initiating the execution of both the pipeline and SQL model."})}),"\n",(0,o.jsx)(n.h2,{id:"1-setup-prophecy-fabric-for-airflow",children:"1. Setup Prophecy Fabric for Airflow"}),"\n",(0,o.jsx)(n.p,{children:"Prophecy introduces the concept of a fabric to describe an execution environment. In this case, we create a fabric to connect to Airflow, and create and schedule DAGs in it.\nFor this guide, we would be using Prophecy Managed Airflow, so an external Airflow instance is not required."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Create Fabric",src:i(64488).A+"",width:"2880",height:"1084"})}),"\n",(0,o.jsxs)(n.p,{children:["Setting up a fabric is very straightforward. Click the ",(0,o.jsx)(n.strong,{children:"(1) Create Entity"})," button, and choose ",(0,o.jsx)(n.strong,{children:"(2) Create Fabric"})," option. The fabric creation is composed of two steps: Basic Info and Providers setup.\nOn the Basic Info screen, enter a ",(0,o.jsx)(n.strong,{children:"(1) Fabric Name"}),", ",(0,o.jsx)(n.strong,{children:"(2) Fabric Description"}),", and choose the ",(0,o.jsx)(n.strong,{children:"(3) Team"})," that\u2019s going to own the fabric."]}),"\n",(0,o.jsxs)(n.p,{children:["Once ready, click ",(0,o.jsx)(n.strong,{children:"(4) Continue"}),"."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Fill_fabric_details.png",src:i(87683).A+"",width:"2880",height:"1726"})}),"\n",(0,o.jsxs)(n.p,{children:["Since we\u2019re setting up a fabric connected to Airflow, choose ",(0,o.jsx)(n.strong,{children:"Airflow"})," as the ",(0,o.jsx)(n.strong,{children:"(1) Provider Type"})," and ",(0,o.jsx)(n.strong,{children:"Prophecy Managed"})," as the ",(0,o.jsx)(n.strong,{children:"(2) Provider"}),".\nFor connecting to Prophecy Managed Airflow, you don't need to provide any other details, so go ahead and click on ",(0,o.jsx)(n.strong,{children:"(3) Continue"}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"11-adding-aws-connection",children:"1.1 Adding AWS Connection"}),"\n",(0,o.jsxs)(n.p,{children:["To be able to trigger your Airflow job, using an S3 File Sensor, you need to have connection from Prophecy Managed Airflow to you S3 account. For this, we need to add an AWS Connection.\nClick on ",(0,o.jsx)(n.strong,{children:"(1) Add Connection"})," button. This Opens up the Connection form as shown."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Add_connection",src:i(88704).A+"",width:"2880",height:"1084"})}),"\n",(0,o.jsxs)(n.p,{children:["Select AWS in ",(0,o.jsx)(n.strong,{children:"(1) Connection Type"}),". Provide a ",(0,o.jsx)(n.strong,{children:"(2) Connection Name"})," to identify your connection, add a ",(0,o.jsx)(n.strong,{children:"(3) Description"})," of your choice, and provide the ",(0,o.jsx)(n.strong,{children:"(4) AWS Access Key ID"})," and ",(0,o.jsx)(n.strong,{children:"(5)AWS Secret Access Key"}),". Please check ",(0,o.jsx)(n.a,{href:"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html",children:"here"})," on how to get the access and secret key from AWS.\nOnce done, hit ",(0,o.jsx)(n.strong,{children:"(6) Save"}),"."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Add_AWS_connection",src:i(49455).A+"",width:"2880",height:"1726"})}),"\n",(0,o.jsx)(n.h3,{id:"13-adding-email-connection",children:"1.3 Adding Email Connection"}),"\n",(0,o.jsxs)(n.p,{children:["To be able to send Email via Airflow using an Email gem, you need to have Email connection in Prophecy Managed Airflow.\nClick again on Add Connection button and select Email in ",(0,o.jsx)(n.strong,{children:"(1) Connection Type"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["Provide a ",(0,o.jsx)(n.strong,{children:"(2) Connection Name"})," to identify your connection, add a ",(0,o.jsx)(n.strong,{children:"(3) Description"})," of your choice, and provide the ",(0,o.jsx)(n.strong,{children:"(4) Host"})," as your SMTP host example ",(0,o.jsx)(n.code,{children:"smtp.gmail.com"}),". Provide the login credentials for this server in ",(0,o.jsx)(n.strong,{children:"(5)Login"})," and ",(0,o.jsx)(n.strong,{children:"(6)Password"})," and provide your SMTP port in ",(0,o.jsx)(n.strong,{children:"(7) Port"}),".\nOnce done, hit ",(0,o.jsx)(n.strong,{children:"(8) Save"}),".\n",(0,o.jsx)(n.img,{alt:"Add_Email_connection.png",src:i(41617).A+"",width:"2880",height:"1726"})]}),"\n",(0,o.jsx)(n.h3,{id:"13-adding-databricks-spark-connection",children:"1.3 Adding Databricks Spark Connection"}),"\n",(0,o.jsx)(n.p,{children:"To be able to Run your Databricks pipelines, you need to have connection from Prophecy Managed Airflow to your Databricks Environment.\nClick again on Add Connection button."}),"\n",(0,o.jsxs)(n.p,{children:["Select Databricks Spark in ",(0,o.jsx)(n.strong,{children:"(1) Connection Type"}),". Now under the ",(0,o.jsx)(n.strong,{children:"(2) Fabric"}),", you would select the already created fabric for Databricks Spark and Prophecy would setup the connection.\nYou can provide a description in the ",(0,o.jsx)(n.strong,{children:"(3) Description"}),".\nOnce done, click ",(0,o.jsx)(n.strong,{children:"(4) Save"}),"."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Add_DB_Spark_connection.png",src:i(58480).A+"",width:"2880",height:"1726"})}),"\n",(0,o.jsx)(n.h3,{id:"14-adding-databricks-sql-connection",children:"1.4 Adding Databricks SQL Connection"}),"\n",(0,o.jsx)(n.p,{children:"To be able to Run your Databricks SQL models, you need to have connection from Prophecy Managed Airflow to your Databricks SQL Environment.\nClick again on Add Connection button."}),"\n",(0,o.jsxs)(n.p,{children:["Select Databricks SQL in ",(0,o.jsx)(n.strong,{children:"(1) Connection Type"}),". Now under the ",(0,o.jsx)(n.strong,{children:"(2) Fabric"}),", you would select the already created fabric for Databricks SQL and Prophecy would setup the connection.\nYou can provide a description in the ",(0,o.jsx)(n.strong,{children:"(3) Description"}),".\nOnce done, click ",(0,o.jsx)(n.strong,{children:"(4) Save"}),"."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Ad_DB_SQL_connection.png",src:i(84976).A+"",width:"2880",height:"1726"})}),"\n",(0,o.jsx)(n.h3,{id:"15-adding-snowflake-sql-connection",children:"1.5 Adding Snowflake SQL Connection"}),"\n",(0,o.jsx)(n.p,{children:"To be able to Run your Snowflake SQL models, you need to have connection from Prophecy Managed Airflow to your Snowflake SQL Environment.\nClick again on Add Connection button."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Add_SF_SQL_connection",src:i(13944).A+"",width:"2880",height:"1726"})}),"\n",(0,o.jsxs)(n.p,{children:["Select Snowflake SQL in ",(0,o.jsx)(n.strong,{children:"(1) Connection Type"}),". Now under the ",(0,o.jsx)(n.strong,{children:"(2) Fabric"}),", you would select the already created fabric for Snowflake SQL and Prophecy would setup the connection.\nYou can provide a description in the ",(0,o.jsx)(n.strong,{children:"(3) Description"}),".\nOnce done, click ",(0,o.jsx)(n.strong,{children:"(4) Save"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["After adding all connections, click ",(0,o.jsx)(n.strong,{children:"(1) Complete"}),"."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Complete_fabric",src:i(10390).A+"",width:"2880",height:"1726"})}),"\n",(0,o.jsx)(n.p,{children:"After creating the fabric, Lets create our first Airflow job."}),"\n",(0,o.jsx)(n.h2,{id:"2-create-an-airflow-job",children:"2. Create an Airflow job"}),"\n",(0,o.jsx)(n.p,{children:"A job is an entity that contains gems to represent a DAG consisting of various Tasks (pipelines/models/scripts, etc) which you can Run once or schedule to run at a frequency. Each job would represent an Airflow DAG in Python."}),"\n",(0,o.jsx)(n.p,{children:"Let's see how to create an Airflow job in Prophecy."}),"\n",(0,o.jsxs)(n.p,{children:["Click the ",(0,o.jsx)(n.strong,{children:"(1) Create Entity"})," button, and choose ",(0,o.jsx)(n.strong,{children:"(2) Create Job"})," option."]}),"\n",(0,o.jsxs)(n.p,{children:["In the side drawer that opens, you would provide the Basic Info of the job. Start by selecting the ",(0,o.jsx)(n.strong,{children:"(1) Project"})," in which you want to create the job. You can pick the existing Spark or SQL project here where you have created pipelines/Models.\nThen pick your development ",(0,o.jsx)(n.strong,{children:"(2) Branch"}),". Here you can pick an existing branch for development, or create a new one. Provide a ",(0,o.jsx)(n.strong,{children:"(3) Name"})," and pick ",(0,o.jsx)(n.strong,{children:"Airflow"})," in the ",(0,o.jsx)(n.strong,{children:"(4) Scheduler"}),". Select the ",(0,o.jsx)(n.strong,{children:"(5) Fabric"})," we created in Step 1.\nPick a ",(0,o.jsx)(n.strong,{children:"(6) Schedule"})," with which you want to schedule the job. Note, you can modify this again after testing before releasing your job.\nAdd a ",(0,o.jsx)(n.strong,{children:"(7) Description"}),", about the job you are creating. Once done, click ",(0,o.jsx)(n.strong,{children:"(8) Create New"}),"."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Create_Job",src:i(97786).A+"",width:"2880",height:"1084"})}),"\n",(0,o.jsxs)(n.p,{children:["This will take you to the ",(0,o.jsx)(n.strong,{children:"job editor"})," where you would be creating the actual DAG for the job.\nLet's start adding gems to our job now."]}),"\n",(0,o.jsx)(n.h3,{id:"21-adding-s3-file-sensor-gem",children:"2.1 Adding S3 file Sensor gem"}),"\n",(0,o.jsxs)(n.p,{children:["Click on ",(0,o.jsx)(n.strong,{children:"(1) Sensors"}),", and Drag the ",(0,o.jsx)(n.strong,{children:"(2) S3FileSensor gem"})," from the dropdown to the canvas. Then click the newly added gem and click ",(0,o.jsx)(n.strong,{children:"(3) Open"})," to open the gem Configurations.\n",(0,o.jsx)(n.img,{alt:"Add_S3_gem",src:i(92568).A+"",width:"2880",height:"1084"})]}),"\n",(0,o.jsxs)(n.p,{children:["Here, we will specify the S3 bucket/path on which we want to trigger the job.\nIn ",(0,o.jsx)(n.strong,{children:"(1) S3 Path(s)"})," specify the complete path of file in your Bucket. Airflow will check if this file exists in the specified bucket periodically and trigger the job when it arrives. Select the created Connection for AWS in ",(0,o.jsx)(n.strong,{children:"(2) Connection name"})," and hit ** (3) Save**."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Add_S3_gem_details",src:i(82976).A+"",width:"2880",height:"1726"})}),"\n",(0,o.jsx)(n.h3,{id:"22-adding-email-gem",children:"2.2 Adding Email gem"}),"\n",(0,o.jsxs)(n.p,{children:["Click on the ",(0,o.jsx)(n.strong,{children:"(1) Operators"}),", and Drag the ",(0,o.jsx)(n.strong,{children:"(2) Email gem"})," from the dropdown to the canvas. If you drag this closer to output port of the previous gem, it will get auto-connected to it. Then click the newly added gem and click ",(0,o.jsx)(n.strong,{children:"(3) Open"})," to open the gem Configurations."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Add_email_gem",src:i(31722).A+"",width:"2880",height:"1084"})}),"\n",(0,o.jsxs)(n.p,{children:["Here we will specify our Email configurations. In ",(0,o.jsx)(n.strong,{children:"(1) To"}),", add your Email id where you want to receive the notification Email when the job is triggered. Select the ",(0,o.jsx)(n.strong,{children:"(2) Connection name"}),", you created for Email in step 1.3.\nYou can provide a ",(0,o.jsx)(n.strong,{children:"(3) Subject"}),", for the Email and also add ",(0,o.jsx)(n.strong,{children:"(4) Email content"})," you want to add to your email. Here in the example, we are using a Airflow param available to access the execution time in a job.\nAdditionally, you can also add cc and bcc emails.\nOnce done, Click ",(0,o.jsx)(n.strong,{children:"(5) Save"}),"!"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Add_email_gem_details",src:i(78632).A+"",width:"2880",height:"1726"})}),"\n",(0,o.jsx)(n.h3,{id:"23-adding-spark-pipeline-gem",children:"2.3 Adding Spark pipeline gem"}),"\n",(0,o.jsxs)(n.p,{children:["If you have a Spark Databricks connection and a Spark project with pipeline, you can include Spark pipeline gems in the job. Click on ",(0,o.jsx)(n.strong,{children:"(1) Operators"}),", and Drag the ",(0,o.jsx)(n.strong,{children:"(2) Pipeline gem"})," from the dropdown to the canvas. Drag it close to the output port of the Email gem, so that it gets auto-connected. Then click the newly added gem and click ",(0,o.jsx)(n.strong,{children:"(3) Open"})," to open the gem configurations."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Add_Pipeline_Gem",src:i(81e3).A+"",width:"2880",height:"1084"})}),"\n",(0,o.jsx)(n.p,{children:"Here, you will select the pipeline and optionally override any config values for the pipeline."}),"\n",(0,o.jsxs)(n.p,{children:["Select the ",(0,o.jsx)(n.strong,{children:"(1) Pipeline to Schedule"})," you want to Run. As you select the pipeline, You would start seeing the Configurations defined in the pipeline. You would not be able to modify the schema of these configs but can override the Config values.\nPick (",(0,o.jsx)(n.strong,{children:"2) Fabric and Cluster size to run this Pipeline"})," for running this pipeline in Databricks. Here, select the fabric for which you already created connection in step 1.3. Once done, Click ",(0,o.jsx)(n.strong,{children:"(3) Save"}),"!"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Add_pipeline_gem_details",src:i(74696).A+"",width:"2880",height:"1726"})}),"\n",(0,o.jsx)(n.h3,{id:"24-adding-sql-dbt-gem",children:"2.4 Adding SQL DBT gem"}),"\n",(0,o.jsxs)(n.p,{children:["Click on ",(0,o.jsx)(n.strong,{children:"(1) Operators"}),", and Drag the ",(0,o.jsx)(n.strong,{children:"(2) DBT gem"})," from the dropdown to the canvas. Drag it close to the output port of the Pipeline gem, so that it gets auto-connected. Then click the newly added gem and click ",(0,o.jsx)(n.strong,{children:"(3) Open"})," to open the gem Configurations."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Add_DBT_Gem",src:i(64604).A+"",width:"2880",height:"1084"})}),"\n",(0,o.jsx)(n.p,{children:"Here, you will select the DBT project/model to Schedule, what SQL fabric to schedule it on, and other additional properties for running a DBT model."}),"\n",(0,o.jsxs)(n.p,{children:["Select the ",(0,o.jsx)(n.strong,{children:"(1) DBT commands"})," you want to run when scheduling your models. You can select all (",(0,o.jsx)(n.a,{href:"/extensibility/dependencies/sql-dependencies",children:"Dependencies"}),", ",(0,o.jsx)(n.a,{href:"/getting-started/tutorials/sql-with-databricks",children:"Seed"}),", Run and Test) here.\nSelect the ",(0,o.jsx)(n.strong,{children:"(2) DBT Project to Schedule"}),". And then select the ",(0,o.jsx)(n.strong,{children:"(3) SQL Warehouse Fabric"})," to schedule the Module on. Select the fabric for which connection was created in Step 1.4 or 1.5.\nIn ",(0,o.jsx)(n.strong,{children:"(4) Git reference"}),", select if you want to schedule a particular commit/tag or branch. Here you can select ",(0,o.jsx)(n.code,{children:"branch"})," for this guide and then in ",(0,o.jsx)(n.strong,{children:"(5) Reference Value"})," give the current branch name you are working on.\nYou can provide any additional ",(0,o.jsx)(n.strong,{children:"(6) Properties"})," for your run and then click ",(0,o.jsx)(n.strong,{children:"(7) Save"}),"!!"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Add_DBT_gem_details",src:i(29382).A+"",width:"1440",height:"863"})}),"\n",(0,o.jsx)(n.p,{children:"Congratulations!!! And just like that, you have created a very simple Airflow job with one Databricks pipeline Task and one DBT model Task."}),"\n",(0,o.jsx)(n.p,{children:"Let's go ahead and see how to Run and Schedule it."}),"\n",(0,o.jsx)(n.h2,{id:"3-run-and-debug",children:"3. Run and Debug"}),"\n",(0,o.jsx)(n.p,{children:"Now that we have our job ready, we can go ahead and run it."}),"\n",(0,o.jsxs)(n.p,{children:["Click on the ",(0,o.jsx)(n.strong,{children:"(1) Run button"})," to trigger the One-time run. This creates a temporary DAG and uploads to Airflow. User can check logs and status in the Prophecy UI itself. When you click on run, you will see a Job Details Toaster."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Run_job",src:i(28651).A+"",width:"1440",height:"542"})}),"\n",(0,o.jsxs)(n.p,{children:["Click on ",(0,o.jsx)(n.strong,{children:"(2) Details"})," to open the detailed logs of the Run.\nHere you can see all the steps involved in the Run and also detailed logs for each step."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.img,{alt:"Run_job_logs",src:i(98626).A+"",width:"2880",height:"1726"}),"\nClick on ",(0,o.jsx)(n.strong,{children:"(1) + button"}),", to open the logs for a particular step."]}),"\n",(0,o.jsx)(n.h2,{id:"4-release-and-schedule",children:"4. Release and Schedule"}),"\n",(0,o.jsx)(n.p,{children:"Once we have developed and tested the job, it\u2019s time to commit and push our code to our Git repository and release the job to our Airflow."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Enable_commit",src:i(43501).A+"",width:"2880",height:"1726"})}),"\n",(0,o.jsxs)(n.p,{children:["Start by toggling our job to be ",(0,o.jsx)(n.strong,{children:"(1) Enabled"}),". This enables the job on the Airflow and will ensure that the job follows the previously set interval. Without enabling the DAG is not uploaded to Airflow."]}),"\n",(0,o.jsxs)(n.p,{children:["Click on the ",(0,o.jsx)(n.strong,{children:"(2) Release"})," button in the top right corner or ",(0,o.jsx)(n.strong,{children:"(3) Commit"})," button in the middle of the footer (bottom of the screen) to start the commit process. This opens an easy-to-use Git management screen."]}),"\n",(0,o.jsx)(n.p,{children:"The process of deploying code is composed of 4 steps:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Commit:"})," We start by creating a named version of our code and uploading it to our development branch on the secure Git repository. On the left-hand side you can see the ",(0,o.jsx)(n.strong,{children:"Current branch"})," and the associated history of commits and on the right side, there\u2019s a list of ",(0,o.jsx)(n.strong,{children:"Entities changed"})," (models, jobs, etc) and their status. If everything looks good, type in the ",(0,o.jsx)(n.strong,{children:"(1) Commit message"})," which should clearly describe, in few sentences, all the changes that we\u2019ve introduced and click ",(0,o.jsx)(n.strong,{children:"(2) Commit"}),".\n",(0,o.jsx)(n.img,{alt:"commit_screen",src:i(74766).A+"",width:"2880",height:"1726"})]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Pull:"})," Before your changes can be safely merged into the ",(0,o.jsx)(n.strong,{children:"main"})," branch, we have to make sure that we\u2019re up-to-date with it. If your colleagues introduced any code on ",(0,o.jsx)(n.strong,{children:"main"})," we have to ",(0,o.jsx)(n.strong,{children:"Pull"})," it first. For this, simply click on ",(0,o.jsx)(n.strong,{children:"(1) Continue"})," and then ",(0,o.jsx)(n.strong,{children:"(2) Pull"}),".\n",(0,o.jsx)(n.img,{alt:"Pull_screen",src:i(34503).A+"",width:"1440",height:"542"})]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Merge:"})," Now that our development branch is up-to-date, we can merge it to master. Here we can either create a ",(0,o.jsx)(n.strong,{children:"Pull Request"})," or if ",(0,o.jsx)(n.strong,{children:"Merge"})," the changes to main directly if your main branch isn't protected. For now, click on ",(0,o.jsx)(n.strong,{children:"(1) Merge"})," to merge them directly. Once the code is merged, you can now see the latest commits present on your ",(0,o.jsx)(n.strong,{children:"main"})," branch.\n",(0,o.jsx)(n.img,{alt:"Merge_screen",src:i(98739).A+"",width:"2880",height:"1726"})]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Release:"})," Finally, now that our changes are all versioned on Git, we can release them to our scheduler. Simply specify a ",(0,o.jsx)(n.strong,{children:"(1) Release Version"})," number, e.g. ",(0,o.jsx)(n.code,{children:"1.0"})," , and the ",(0,o.jsx)(n.strong,{children:"(2) Release Note,"})," which should clearly outline the latest changes. When ready, click ",(0,o.jsx)(n.strong,{children:"(3) Release."}),"\n",(0,o.jsx)(n.img,{alt:"Release_screen",src:i(74267).A+"",width:"2880",height:"1726"})]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This will build and deploy your Airflow job on the Airflow fabric specified in the job and would schedule to run it on the specified schedule."}),"\n",(0,o.jsx)(n.h2,{id:"5-monitor",children:"5. Monitor"}),"\n",(0,o.jsxs)(n.p,{children:["During the release process Prophecy automatically packages, tests, and deploys your jobs to Airflow. Once the process is finished you can see the deployed and running job within your Airflow environment. You can monitor this job within Prophecy in the ",(0,o.jsx)(n.strong,{children:"Observability"})," page."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Airflow_Monitoring_page",src:i(5067).A+"",width:"1440",height:"542"})}),"\n",(0,o.jsxs)(n.p,{children:["Click on ",(0,o.jsx)(n.strong,{children:"(1) Observability icon"})," on the left side menu bar to take you to the Observability page. Then in ",(0,o.jsx)(n.strong,{children:"(2) Fabric Selection box"}),", choose the Airflow fabric your have. The Observability page will show all the Past and Current Runs of the jobs you released on this fabric. Switch between ",(0,o.jsx)(n.strong,{children:"Attention Required"}),", ",(0,o.jsx)(n.strong,{children:"All events"}),", ",(0,o.jsx)(n.strong,{children:"Job Runs"})," to find any Particular Run you are looking for. Click on the ",(0,o.jsx)(n.strong,{children:"(3) Details"})," button to open up the logs of any particular Run."]}),"\n",(0,o.jsx)(n.h2,{id:"whats-next",children:"What\u2019s next?"}),"\n",(0,o.jsx)(n.p,{children:"Great work! \ud83c\udf89"}),"\n",(0,o.jsx)(n.p,{children:"You've created your first Airflow job in Prophecy, ran it successfully, released, scheduled, and monitored the job. Take a moment to appreciate your accomplishment \ud83e\udd73."}),"\n",(0,o.jsx)(n.p,{children:"To continue learning and expanding your skills with Prophecy, feel free to explore other tutorials within our documentation!"}),"\n",(0,o.jsxs)(n.p,{children:["If you ever encounter any difficulties, don't hesitate to reach out to us (",(0,o.jsx)(n.a,{href:"mailto:contact.us@Prophecy.io",children:"contact.us@Prophecy.io"}),") or join our ",(0,o.jsx)(n.a,{href:"https://prophecy-io-support.slack.com/join/shared_invite/zt-moq3xzoj-~5MSJ6WPnZfz7bwsqWi8tQ#/shared-invite/email",children:"Slack community"})," for assistance. We're here to help!"]}),"\n",(0,o.jsx)(n.hr,{})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},41617:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.5_Email_connection-6f392517477fe16388989d4f089be6ed.png"},43501:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.20_start_commit-7a8c082c2fa41a0cafdafe163f6165fc.png"},49455:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.4_AWS_Connection-4c3d3bfbc9c76e839dcbb6d4e1ef3584.png"},58480:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.6_DB_Spark_connection-51d1e9b4ff69f1e17085ec10d3654309.png"},64488:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.1_Create_Fabric-49796b917d7155244e44ce3e3af6b5cd.png"},64604:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.16_Add_DBT_gem-4f8d7f58fb1272d3f34623896db75b3e.png"},74267:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.24_release_screen-b32cb4fe551f57d51245adbc73e8a84d.png"},74696:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.15_Add_pipeline_gem_details-eeb2d4df18f9331c0a63cfdc96143c49.png"},74766:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.21_commit_screen-6584d65e6e9d67933fd5b7250486d0f9.png"},78632:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.13_Add_email_gem_details-da3c19e263c58acfb80a805ecd66952f.png"},81e3:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.14_Add_pipeline_gem-45d2e7e3d46a095c899b6af03a1d4814.png"},82976:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.11_Add_s3_gem_details-54d44ee280c2cf7fceae32152dbedd46.png"},84976:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.7_DB_Sql_connection-e1de47abbd9b3265c2b03e8aafcca822.png"},87683:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.2_Create_Fabric-8cb092965a4dee63432186c2b6982924.png"},88704:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.3_Add_Connection-e52b200f2986cda3be090212a02ff872.png"},92568:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.10_Add_s3_gem-2a946a09ee73912bc8abaa7b6f0a5160.png"},97786:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.9_Create_Job-9fb3cbc0e3246cb19970d24284235919.png"},98626:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.19_Run_Job_logs-0323f46acaadbf4f38db46f45a8de52f.png"},98739:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/3.23_merge_screen-4403a56552e6b737d8e49ca482c2c88f.png"}}]);