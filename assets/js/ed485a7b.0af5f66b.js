"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[80365],{15680:(e,a,r)=>{r.d(a,{xA:()=>p,yg:()=>g});var t=r(96540);function i(e,a,r){return a in e?Object.defineProperty(e,a,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[a]=r,e}function n(e,a){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),r.push.apply(r,t)}return r}function o(e){for(var a=1;a<arguments.length;a++){var r=null!=arguments[a]?arguments[a]:{};a%2?n(Object(r),!0).forEach((function(a){i(e,a,r[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):n(Object(r)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(r,a))}))}return e}function s(e,a){if(null==e)return{};var r,t,i=function(e,a){if(null==e)return{};var r,t,i={},n=Object.keys(e);for(t=0;t<n.length;t++)r=n[t],a.indexOf(r)>=0||(i[r]=e[r]);return i}(e,a);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(t=0;t<n.length;t++)r=n[t],a.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(i[r]=e[r])}return i}var c=t.createContext({}),l=function(e){var a=t.useContext(c),r=a;return e&&(r="function"==typeof e?e(a):o(o({},a),e)),r},p=function(e){var a=l(e.components);return t.createElement(c.Provider,{value:a},e.children)},u="mdxType",y={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},d=t.forwardRef((function(e,a){var r=e.components,i=e.mdxType,n=e.originalType,c=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=l(r),d=i,g=u["".concat(c,".").concat(d)]||u[d]||y[d]||n;return r?t.createElement(g,o(o({ref:a},p),{},{components:r})):t.createElement(g,o({ref:a},p))}));function g(e,a){var r=arguments,i=a&&a.mdxType;if("string"==typeof e||i){var n=r.length,o=new Array(n);o[0]=d;var s={};for(var c in a)hasOwnProperty.call(a,c)&&(s[c]=a[c]);s.originalType=e,s[u]="string"==typeof e?e:i,o[1]=s;for(var l=2;l<n;l++)o[l]=r[l];return t.createElement.apply(null,o)}return t.createElement.apply(null,r)}d.displayName="MDXCreateElement"},54633:(e,a,r)=>{r.r(a),r.d(a,{assets:()=>c,contentTitle:()=>o,default:()=>y,frontMatter:()=>n,metadata:()=>s,toc:()=>l});var t=r(58168),i=(r(96540),r(15680));const n={title:"Databricks",id:"create-a-Fabric",description:"Configuring Databricks Fabric",sidebar_position:1,tags:["concepts","Fabric","databricks","livy","prophecyManaged"]},o=void 0,s={unversionedId:"Spark/fabrics/create-a-Fabric",id:"Spark/fabrics/create-a-Fabric",title:"Databricks",description:"Configuring Databricks Fabric",source:"@site/docs/Spark/fabrics/create-a-fabric.md",sourceDirName:"Spark/fabrics",slug:"/Spark/fabrics/create-a-Fabric",permalink:"/Spark/fabrics/create-a-Fabric",draft:!1,tags:[{label:"concepts",permalink:"/tags/concepts"},{label:"Fabric",permalink:"/tags/fabric"},{label:"databricks",permalink:"/tags/databricks"},{label:"livy",permalink:"/tags/livy"},{label:"prophecyManaged",permalink:"/tags/prophecy-managed"}],version:"current",sidebarPosition:1,frontMatter:{title:"Databricks",id:"create-a-Fabric",description:"Configuring Databricks Fabric",sidebar_position:1,tags:["concepts","Fabric","databricks","livy","prophecyManaged"]},sidebar:"defaultSidebar",previous:{title:"Prophecy Fabrics",permalink:"/Spark/fabrics/"},next:{title:"Amazon EMR",permalink:"/Spark/fabrics/EMR-fabric-configuration"}},c={},l=[{value:"<strong>Prophecy Managed</strong>",id:"prophecy-managed",level:3},{value:"<strong>Databricks</strong>",id:"databricks",level:3},{value:"<strong>Livy</strong>",id:"livy",level:3}],p={toc:l},u="wrapper";function y(e){let{components:a,...n}=e;return(0,i.yg)(u,(0,t.A)({},p,n,{components:a,mdxType:"MDXLayout"}),(0,i.yg)("p",null,"Prophecy gets you moving with ",(0,i.yg)("em",{parentName:"p"},"your")," Spark environment quickly."),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},(0,i.yg)("strong",{parentName:"p"},(0,i.yg)("a",{parentName:"strong",href:"#prophecy-managed"},"Prophecy Managed Databricks Fabric"))," - If you don't have a Databricks environment, use the Prophecy Managed Databricks Fabric to get started.")),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},(0,i.yg)("strong",{parentName:"p"},(0,i.yg)("a",{parentName:"strong",href:"#databricks"},"Databricks"))," - Create a Databricks Fabric to connect Prophecy to your existing Databricks environment. Think of a Fabric as connection to your ",(0,i.yg)("a",{parentName:"p",href:"https://docs.databricks.com/workspace/index.html#navigate-the-workspace"},"Databricks workspace"),".")),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},(0,i.yg)("strong",{parentName:"p"},(0,i.yg)("a",{parentName:"strong",href:"#Livy"},"Livy"))," - Create a generic Livy Fabric to connect Prophecy to any Spark cluster accessible via Apache Livy. See also ",(0,i.yg)("a",{parentName:"p",href:"/Spark/fabrics/EMR-fabric-configuration"},"Amazon EMR"),", ",(0,i.yg)("a",{parentName:"p",href:"/Spark/fabrics/azure-synapse-fabric-guide"},"Azure Synapse"),", or ",(0,i.yg)("a",{parentName:"p",href:"/Spark/fabrics/gcp-dataproc-fabric-guide"},"Google Cloud Dataproc")," Fabrics."))),(0,i.yg)("h3",{id:"prophecy-managed"},(0,i.yg)("strong",{parentName:"h3"},"Prophecy Managed")),(0,i.yg)("p",null,"Using this option, you can create a 14-Day Free Trial Fabric using Prophecy Managed Databricks. You can use this when trying out Prophecy and when you don't want to connect your own Spark Execution Environment to Prophecy. We already have some sample data and tables created to try out the different functionalities.\nPlease refer to the video below for a step-by-step example"),(0,i.yg)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,i.yg)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,i.yg)("iframe",{src:"https://user-images.githubusercontent.com/121796483/217787623-1cf01df2-54d6-4338-bd59-bd921e101ce9.mp4",title:"Databricks Fabric",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,i.yg)("p",null,"In this Fabric you can only change the ",(0,i.yg)("a",{parentName:"p",href:"https://docs.databricks.com/runtime/dbr.html#databricks-runtime"},"Databricks Runtime version"),". The auto-termination timeout, Executor and Driver Machine Type and Job sizes are uneditable."),(0,i.yg)("h3",{id:"databricks"},(0,i.yg)("strong",{parentName:"h3"},"Databricks")),(0,i.yg)("p",null,"To connect your own Databricks Workspace to Prophecy, you can use this option to create a Fabric. Think of a Fabric as connection to your ",(0,i.yg)("a",{parentName:"p",href:"https://docs.databricks.com/workspace/index.html#navigate-the-workspace"},"Databricks workspace"),". This Fabric enables Prophecy to connect to existing Spark clusters (or create new ones), execute Spark pipelines, read and write data, etc - all according to each user's permissions defined by their personal access token."),(0,i.yg)("p",null,"Please refer to the video below for a step-by-step example"),(0,i.yg)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,i.yg)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,i.yg)("iframe",{src:"https://user-images.githubusercontent.com/121796483/217735090-41853091-ef2e-4d60-bdf6-62fe31a7ee3b.mp4",title:"Databricks Fabric",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Databricks Credentials")," - Here you will provide your Databricks Workspace URL and ",(0,i.yg)("a",{parentName:"li",href:"https://docs.databricks.com/dev-tools/api/latest/authentication.html#generate-a-personal-access-token"},"Personal Access Token")," (PAT). The PAT must have permission to attach clusters. If you'd like to create clusters or read/write data from Prophecy, then these permissions should be enabled for the PAT as well. Keep in mind each user will need to use their own PAT in the Fabric. Prophecy respects the permissions scoped to each user."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Cluster Details")," - Here you would need to provide the ",(0,i.yg)("a",{parentName:"li",href:"https://docs.databricks.com/runtime/dbr.html#databricks-runtime"},"Databricks Runtime version"),", Executor and Drive Machine Types and Termination Timeout if any. These cluster details will be used when creating a cluster via Prophecy during Interactive development and for job clusters during Scheduled Databricks Job runs."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Job sizes")," - User can create Job sizes here using which clusters can be spawned while testing through prophecy IDE. Here you can provide Cluster mode, Databricks Runtime version, total number of the Executors, Core and Memory for them, etc. This provides all the options which are available on Databricks while spawning clusters through Databricks. We recommend using the smallest machines and smallest number of nodes appropriate for your use case.")),(0,i.yg)("p",null,(0,i.yg)("img",{alt:"Editing a Job",src:r(3507).A,width:"1283",height:"1068"})),(0,i.yg)("p",null,"In Json you can just copy-paste your compute Json from Databricks."),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Prophecy Library")," - These are some Scala and Python libraries written by Prophecy to provide additional functionalities on top of Spark. These would get automatically installed in your Spark execution environment when you attach to a cluster/create new cluster. These libraries are also publicly available on Maven central and Pypi respectively."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Metadata Connection")," - Optionally, enhance your Fabric by creating a ",(0,i.yg)("a",{parentName:"li",href:"/metadata/metadata-connections"},"Metadata Connection"),", recommended for users with hundreds or thousands of tables housed in their data provider(s).")),(0,i.yg)("h3",{id:"livy"},(0,i.yg)("strong",{parentName:"h3"},"Livy")),(0,i.yg)("p",null,(0,i.yg)("a",{parentName:"p",href:"https://livy.apache.org/"},"Apache Livy")," is a service that enables easy interaction with a Spark cluster over a REST interface. If you're running Spark-on-hadoop, most Hadoop distributions (CDP/MapR) come with livy bundled, you just need to enable it. For Spark-on-k8s, you can put a livy in the k8s cluster which exposes Spark over rest API."),(0,i.yg)("p",null,"Please refer to the below video for a step-by-step example, or learn how to configure an EMR Fabric with Livy ",(0,i.yg)("a",{parentName:"p",href:"/Spark/fabrics/EMR-fabric-configuration"},"here"),"."),(0,i.yg)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,i.yg)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,i.yg)("iframe",{src:"https://user-images.githubusercontent.com/121796483/217732038-d01bbfbe-a140-4661-a279-1b4858ab2285.mp4",title:"Livy Fabric",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Spark Connection")," - Here you will provide the Livy URL, Authentication, Spark version and Scala version. Spark and Scala versions are used when user tries to attach a cluster using this Fabric.")),(0,i.yg)("admonition",{type:"note"},(0,i.yg)("p",{parentName:"admonition"},"The Spark and Scala versions are now mandatory with recent ",(0,i.yg)("a",{parentName:"p",href:"/release_notes/2023/Feb_2023#spark-and-scala-versions-are-now-required-in-livy-Fabrics"},"Release"),".\nIf you have an older Fabric which doesn't have Spark and Scala versions present, an error (seen below) will appear when trying to attach to a cluster. Just update the Fabric from the metadata page or by clicking ",(0,i.yg)("inlineCode",{parentName:"p"},"Update Fabric")," button (seen below).\n",(0,i.yg)("img",{alt:"Fabric_misconfigured",src:r(96572).A,width:"539",height:"179"}))),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("p",{parentName:"li"},(0,i.yg)("strong",{parentName:"p"},"Job sizes")," -\nBy default, you will see a Small Job size pre created. You can edit or add more Job sizes. A Job size consists of"),(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Size of the Driver: Driver Core and Memory"),(0,i.yg)("li",{parentName:"ul"},"Size of the Executor: Core and Memory for each executor"),(0,i.yg)("li",{parentName:"ul"},"Total number of Executors"))),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("p",{parentName:"li"},(0,i.yg)("strong",{parentName:"p"},"Prophecy Library")," -\nThese are some Scala and Python libraries written by Prophecy to provide additional functionalities on top of Spark. These would get automatically installed in your Spark execution environment when you attach to a cluster/create new cluster. These libraries are also publicly available on Maven central and Pypi respectively.")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("p",{parentName:"li"},(0,i.yg)("strong",{parentName:"p"},"Spark Config")," -\nThese are additional ",(0,i.yg)("a",{parentName:"p",href:"https://spark.apache.org/docs/latest/configuration.html#available-properties"},"Spark Properties")," which you can set which would be applied at Spark session initialisation.\nFor example if your Spark installation is configured to have dynamic allocation enabled, you can disable it for sessions created through Prophecy."))))}y.isMDXComponent=!0},96572:(e,a,r)=>{r.d(a,{A:()=>t});const t=r.p+"assets/images/fabric_misconfigured-c1bba4f511abe79291f82f8fcb674144.png"},3507:(e,a,r)=>{r.d(a,{A:()=>t});const t=r.p+"assets/images/job_size_new-f8b58a9a3298e4d5e3b3efbff4b0d61c.png"}}]);