"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[5513],{3905:function(e,t,a){a.d(t,{Zo:function(){return m},kt:function(){return f}});var r=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},o=Object.keys(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var i=r.createContext({}),p=function(e){var t=r.useContext(i),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},m=function(e){var t=p(e.components);return r.createElement(i.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,o=e.originalType,i=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),u=p(a),f=n,d=u["".concat(i,".").concat(f)]||u[f]||c[f]||o;return a?r.createElement(d,l(l({ref:t},m),{},{components:a})):r.createElement(d,l({ref:t},m))}));function f(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=a.length,l=new Array(o);l[0]=u;var s={};for(var i in t)hasOwnProperty.call(t,i)&&(s[i]=t[i]);s.originalType=e,s.mdxType="string"==typeof e?e:n,l[1]=s;for(var p=2;p<o;p++)l[p]=a[p];return r.createElement.apply(null,l)}return r.createElement.apply(null,a)}u.displayName="MDXCreateElement"},2360:function(e,t,a){a.d(t,{Z:function(){return l}});var r=a(7294),n=a(6010),o="tabItem_OmH5";function l(e){var t=e.children,a=e.hidden,l=e.className;return r.createElement("div",{role:"tabpanel",className:(0,n.Z)(o,l),hidden:a},t)}},9877:function(e,t,a){a.d(t,{Z:function(){return f}});var r=a(7462),n=a(7294),o=a(2389),l=a(7392),s=a(7094),i=a(2466),p=a(6010),m="tabList_uSqn",c="tabItem_LplD";function u(e){var t,a,o,u=e.lazy,f=e.block,d=e.defaultValue,k=e.values,g=e.groupId,b=e.className,h=n.Children.map(e.children,(function(e){if((0,n.isValidElement)(e)&&void 0!==e.props.value)return e;throw new Error("Docusaurus error: Bad <Tabs> child <"+("string"==typeof e.type?e.type:e.type.name)+'>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.')})),N=null!=k?k:h.map((function(e){var t=e.props;return{value:t.value,label:t.label,attributes:t.attributes}})),v=(0,l.l)(N,(function(e,t){return e.value===t.value}));if(v.length>0)throw new Error('Docusaurus error: Duplicate values "'+v.map((function(e){return e.value})).join(", ")+'" found in <Tabs>. Every value needs to be unique.');var y=null===d?d:null!=(t=null!=d?d:null==(a=h.find((function(e){return e.props.default})))?void 0:a.props.value)?t:null==(o=h[0])?void 0:o.props.value;if(null!==y&&!N.some((function(e){return e.value===y})))throw new Error('Docusaurus error: The <Tabs> has a defaultValue "'+y+'" but none of its children has the corresponding value. Available values are: '+N.map((function(e){return e.value})).join(", ")+". If you intend to show no default tab, use defaultValue={null} instead.");var S=(0,s.U)(),w=S.tabGroupChoices,T=S.setTabGroupChoices,_=(0,n.useState)(y),x=_[0],D=_[1],C=[],E=(0,i.o5)().blockElementScrollPositionUntilNextRender;if(null!=g){var O=w[g];null!=O&&O!==x&&N.some((function(e){return e.value===O}))&&D(O)}var P=function(e){var t=e.currentTarget,a=C.indexOf(t),r=N[a].value;r!==x&&(E(t),D(r),null!=g&&T(g,r))},A=function(e){var t,a=null;switch(e.key){case"ArrowRight":var r=C.indexOf(e.currentTarget)+1;a=C[r]||C[0];break;case"ArrowLeft":var n=C.indexOf(e.currentTarget)-1;a=C[n]||C[C.length-1]}null==(t=a)||t.focus()};return n.createElement("div",{className:(0,p.Z)("tabs-container",m)},n.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,p.Z)("tabs",{"tabs--block":f},b)},N.map((function(e){var t=e.value,a=e.label,o=e.attributes;return n.createElement("li",(0,r.Z)({role:"tab",tabIndex:x===t?0:-1,"aria-selected":x===t,key:t,ref:function(e){return C.push(e)},onKeyDown:A,onFocus:P,onClick:P},o,{className:(0,p.Z)("tabs__item",c,null==o?void 0:o.className,{"tabs__item--active":x===t})}),null!=a?a:t)}))),u?(0,n.cloneElement)(h.filter((function(e){return e.props.value===x}))[0],{className:"margin-top--md"}):n.createElement("div",{className:"margin-top--md"},h.map((function(e,t){return(0,n.cloneElement)(e,{key:t,hidden:e.props.value!==x})}))))}function f(e){var t=(0,o.Z)();return n.createElement(u,(0,r.Z)({key:String(t)},e))}},4344:function(e,t,a){a.r(t),a.d(t,{assets:function(){return u},contentTitle:function(){return m},default:function(){return k},frontMatter:function(){return p},metadata:function(){return c},toc:function(){return f}});var r=a(7462),n=a(3366),o=(a(7294),a(3905)),l=a(9877),s=a(2360),i=["components"],p={title:"Kafka Stream",sidebar_position:9},m=void 0,c={unversionedId:"low-code-spark/gems/source-target/file/kafka-stream",id:"low-code-spark/gems/source-target/file/kafka-stream",title:"Kafka Stream",description:"Reads data from Kafka Stream in batch mode and writes data to Kafka.",source:"@site/docs/05-low-code-spark/05-gems/01-source-target/01-file/09-kafka-stream.md",sourceDirName:"05-low-code-spark/05-gems/01-source-target/01-file",slug:"/low-code-spark/gems/source-target/file/kafka-stream",permalink:"/low-code-spark/gems/source-target/file/kafka-stream",draft:!1,tags:[],version:"current",sidebarPosition:9,frontMatter:{title:"Kafka Stream",sidebar_position:9},sidebar:"defaultSidebar",previous:{title:"Fixed Format",permalink:"/low-code-spark/gems/source-target/file/fixed-format"},next:{title:"Warehouse",permalink:"/low-code-spark/gems/source-target/warehouse/intro"}},u={},f=[{value:"Source",id:"source",level:2},{value:"Source Parameters",id:"source-parameters",level:3},{value:"Source Example",id:"source-example",level:3},{value:"Spark Code",id:"spark-code",level:3},{value:"Target",id:"target",level:2},{value:"Target Parameters",id:"target-parameters",level:3},{value:"Target Example",id:"target-example",level:3},{value:"Spark Code",id:"spark-code-1",level:3},{value:"Source Pipeline Example",id:"source-pipeline-example",level:2},{value:"Spark Code used for script component",id:"spark-code-used-for-script-component",level:3}],d={toc:f};function k(e){var t=e.components,p=(0,n.Z)(e,i);return(0,o.kt)("wrapper",(0,r.Z)({},d,p,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"Reads data from Kafka Stream in batch mode and writes data to Kafka."),(0,o.kt)("h2",{id:"source"},"Source"),(0,o.kt)("p",null,"Reads data from kafka stream in batch mode.\nData is read only incrementally from the last offset stored in the metadata table. If metadata table\nis not present, then data with ",(0,o.kt)("inlineCode",{parentName:"p"},"earliest")," offset would be read."),(0,o.kt)("h3",{id:"source-parameters"},"Source Parameters"),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:"left"},"Parameter"),(0,o.kt)("th",{parentName:"tr",align:"left"},"Description"),(0,o.kt)("th",{parentName:"tr",align:"left"},"Required"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"Broker List"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Comma separated list of kafka brokers"),(0,o.kt)("td",{parentName:"tr",align:"left"},"True")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"Group Id"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Kafka consumer group id"),(0,o.kt)("td",{parentName:"tr",align:"left"},"True")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"Session Timeout"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Session timeout for kafka. (Default value set to 6000s)"),(0,o.kt)("td",{parentName:"tr",align:"left"},"False")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"Security Protocol"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Security protocol for kafka (Default value set to SASL_SSL)"),(0,o.kt)("td",{parentName:"tr",align:"left"},"True")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"SASL Mechanism"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Default SASL Mechanism for SASL_SSL (Default value set to SCRAM-SHA-256)"),(0,o.kt)("td",{parentName:"tr",align:"left"},"True")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"Credential Type"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Credential Type provider (Databricks Secrets or Username/Password)"),(0,o.kt)("td",{parentName:"tr",align:"left"},"True")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"Credential Scope"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Scope to use for databricks secrets"),(0,o.kt)("td",{parentName:"tr",align:"left"},"True")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"Kafka Topic"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Comma separated list of kafka topics"),(0,o.kt)("td",{parentName:"tr",align:"left"},"True")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"Metadata Table"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Table name which would be used to store offsets for each topic, partition"),(0,o.kt)("td",{parentName:"tr",align:"left"},"True")))),(0,o.kt)("h3",{id:"source-example"},"Source Example"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Example usage of Filter",src:a(8146).Z,width:"3024",height:"1590"})),(0,o.kt)("h3",{id:"spark-code"},"Spark Code"),(0,o.kt)(l.Z,{mdxType:"Tabs"},(0,o.kt)(s.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},'def KafkaSource(spark: SparkSession) -> DataFrame:\n    from delta.tables import DeltaTable\n    import json\n    from pyspark.dbutils import DBUtils\n\n    if spark.catalog._jcatalog.tableExists(f"metadata.kafka_offsets"):\n        offset_dict = {}\n\n        for row in DeltaTable.forName(spark, f"metadata.kafka_offsets").toDF().collect():\n            if row["topic"] in offset_dict.keys():\n                offset_dict[row["topic"]].update({row["partition"] : row["max_offset"] + 1})\n            else:\n                offset_dict[row["topic"]] = {row["partition"] : row["max_offset"] + 1}\n\n        return (spark.read\\\n            .format("kafka")\\\n            .options(\n              **{\n                "kafka.sasl.jaas.config": (\n                  f"kafkashaded.org.apache.kafka.common.security.scram.ScramLoginModule"\n                  + f\' required username="{DBUtils(spark).secrets.get(scope = "test", key = "username")}" password="{DBUtils(spark).secrets.get(scope = "test", key = "password")}";\'\n                ),\n                "kafka.sasl.mechanism": "SCRAM-SHA-256",\n                "kafka.security.protocol": "SASL_SSL",\n                "kafka.bootstrap.servers": "broker1.aws.com:9094,broker2.aws.com:9094",\n                "kafka.session.timeout.ms": "6000",\n                "group.id": "group_id_1",\n                "subscribe": "my_first_topic,my_second_topic",\n                "startingOffsets": json.dumps(offset_dict),\n              }\n            )\\\n            .load()\\\n            .withColumn("value", col("value").cast("string"))\\\n            .withColumn("key", col("key").cast("string")))\n    else:\n        return (spark.read\\\n            .format("kafka")\\\n            .options(\n              **{\n                "kafka.sasl.jaas.config": (\n                  f"kafkashaded.org.apache.kafka.common.security.scram.ScramLoginModule"\n                  + f\' required username="{DBUtils(spark).secrets.get(scope = "test", key = "username")}" password="{DBUtils(spark).secrets.get(scope = "test", key = "password")}";\'\n                ),\n                "kafka.sasl.mechanism": "SCRAM-SHA-256",\n                "kafka.security.protocol": "SASL_SSL",\n                "kafka.bootstrap.servers": "broker1.aws.com:9094,broker2.aws.com:9094",\n                "kafka.session.timeout.ms": "6000",\n                "group.id": "group_id_1",\n                "subscribe": "my_first_topic,my_second_topic"\n              }\n            )\\\n            .load()\\\n            .withColumn("value", col("value").cast("string"))\\\n            .withColumn("key", col("key").cast("string")))\n\n'))),(0,o.kt)(s.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},"Coming Soon\n")))),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"target"},"Target"),(0,o.kt)("p",null,"Publishes the dataframe to kafka topic(s) as json messages."),(0,o.kt)("h3",{id:"target-parameters"},"Target Parameters"),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:"left"},"Parameter"),(0,o.kt)("th",{parentName:"tr",align:"left"},"Description"),(0,o.kt)("th",{parentName:"tr",align:"left"},"Required"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"Broker List"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Comma separated list of kafka brokers"),(0,o.kt)("td",{parentName:"tr",align:"left"},"True")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"Security Protocol"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Security protocol for kafka (Default value set to SASL_SSL)"),(0,o.kt)("td",{parentName:"tr",align:"left"},"True")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"SASL Mechanism"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Default SASL Mechanism for SASL_SSL (Default value set to SCRAM-SHA-256)"),(0,o.kt)("td",{parentName:"tr",align:"left"},"True")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"Credential Type"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Credential Type provider (Databricks Secrets or Username/Password)"),(0,o.kt)("td",{parentName:"tr",align:"left"},"True")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"Credential Scope"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Scope to use for databricks secrets"),(0,o.kt)("td",{parentName:"tr",align:"left"},"True")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"Kafka Topic"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Comma separated list of kafka topics"),(0,o.kt)("td",{parentName:"tr",align:"left"},"True")))),(0,o.kt)("h3",{id:"target-example"},"Target Example"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Example usage of Filter",src:a(3778).Z,width:"3042",height:"1452"})),(0,o.kt)("h3",{id:"spark-code-1"},"Spark Code"),(0,o.kt)(l.Z,{mdxType:"Tabs"},(0,o.kt)(s.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},'def KafkaTarget(spark: SparkSession, in0: DataFrame):\n    df1 = in0.select(to_json(struct("*")).alias("value"))\n    df2 = df1.selectExpr("CAST(value AS STRING)")\n    df2.write\\\n        .format("kafka")\\\n        .options(\n          **{\n            "kafka.sasl.jaas.config": (\n              f"kafkashaded.org.apache.kafka.common.security.scram.ScramLoginModule"\n              + f\' required username="{DBUtils(spark).secrets.get(scope = "test", key = "username")}" password="{DBUtils(spark).secrets.get(scope = "test", key = "password")}";\'\n            ),\n            "kafka.sasl.mechanism": "SCRAM-SHA-256",\n            "kafka.security.protocol": "SASL_SSL",\n            "kafka.bootstrap.servers": "broker1.aws.com:9094,broker2.aws.com:9094",\n            "topic": "my_first_topic,my_second_topic",\n          }\n        )\\\n        .save()\n\n'))),(0,o.kt)(s.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},"Coming Soon\n\n")))),(0,o.kt)("h1",{id:"example-pipelines"},"Example Pipelines"),(0,o.kt)("h2",{id:"source-pipeline-example"},"Source Pipeline Example"),(0,o.kt)("p",null,"In this example we would be reading json messages from kafka stream, parse them, remove any null messages\nand then finally save it into a delta table."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Example usage of Filter",src:a(5438).Z,width:"1139",height:"584"})),(0,o.kt)("p",null,"Also once the data is successfully written into our target, we would be updating the ",(0,o.kt)("inlineCode",{parentName:"p"},"metadata.kafka_offsets")," table."),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"metadata.kafka_offsets")," table would save the max offset read for each topic, partition combination."),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:"left"},"topic"),(0,o.kt)("th",{parentName:"tr",align:"left"},"partition"),(0,o.kt)("th",{parentName:"tr",align:"left"},"max_offset"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"my_first_topic"),(0,o.kt)("td",{parentName:"tr",align:"left"},"0"),(0,o.kt)("td",{parentName:"tr",align:"left"},"10")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"my_first_topic"),(0,o.kt)("td",{parentName:"tr",align:"left"},"1"),(0,o.kt)("td",{parentName:"tr",align:"left"},"5")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"my_second_topic"),(0,o.kt)("td",{parentName:"tr",align:"left"},"0"),(0,o.kt)("td",{parentName:"tr",align:"left"},"10")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"my_second_topic"),(0,o.kt)("td",{parentName:"tr",align:"left"},"1"),(0,o.kt)("td",{parentName:"tr",align:"left"},"5")))),(0,o.kt)("p",null,"This table would help us in below:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Build the pipeline interactively without committing any offset"),(0,o.kt)("li",{parentName:"ol"},"For batch production workflows this would help us to keep track on what offsets to read from in the subsequent run"),(0,o.kt)("li",{parentName:"ol"},"In case we want to relay older messages again from a particular offset, we can simply update the metadata table.")),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"For production workflows the phase for update offset script gem should be greater than the phase of\ntarget gem (like in our example, phase for target gem is 0 and updateOffsets gem is 1).\nThis is to ensure that offsets are only updated in the table post data is successfully written."))),(0,o.kt)("h3",{id:"spark-code-used-for-script-component"},"Spark Code used for script component"),(0,o.kt)(l.Z,{mdxType:"Tabs"},(0,o.kt)(s.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},'def UpdateOffsets(spark: SparkSession, in0: DataFrame):\n\n    if not ("SColumnExpression" in locals()):\n        from delta.tables import DeltaTable\n        import pyspark.sql.functions as f\n        metadataTable = "metadata.kafka_offsets"\n        metaDataDf = in0.groupBy("partition", "topic").agg(f.max(f.col("`offset`").cast("int")).alias("max_offset"))\n\n        if not spark.catalog._jcatalog.tableExists(metadataTable):\n            metaDataDf.write.format("delta").mode("overwrite").saveAsTable(metadataTable)\n        else:\n            DeltaTable\\\n                .forName(spark, metadataTable)\\\n                .alias("target")\\\n                .merge(\n                  metaDataDf.alias("source"),\n                  (\n                    (col("source.`partition`") == col("target.`partition`"))\n                    & (col("source.`topic`") == col("target.`topic`"))\n                  )\n                )\\\n                .whenMatchedUpdateAll()\\\n                .whenNotMatchedInsertAll()\\\n                .execute()\n\n'))),(0,o.kt)(s.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},"Coming Soon\n\n")))))}k.isMDXComponent=!0},5438:function(e,t,a){t.Z=a.p+"assets/images/kafka_pipeline_eg-f97290ea76491916f47acf312a5ea95b.gif"},8146:function(e,t,a){t.Z=a.p+"assets/images/kafka_source_eg_1-5d9b36695526379b9a62cf152b6170bf.png"},3778:function(e,t,a){t.Z=a.p+"assets/images/kafka_target_eg_1-498e2012164f661686168fbd52aebaaf.png"}}]);