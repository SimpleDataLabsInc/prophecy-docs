"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[25069],{25318:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/synth_0_1_requirements-2c772b3704bbb6be6781bf853266fdaf.png"},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var s=i(96540);const t={},d=s.createContext(t);function r(e){const n=s.useContext(d);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(d.Provider,{value:n},e.children)}},35148:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"extensibility/dependencies/spark-dependencies","title":"Spark dependencies","description":"Reuse Spark dependencies in pipelines and jobs","source":"@site/docs/extensibility/dependencies/spark-dependencies.md","sourceDirName":"extensibility/dependencies","slug":"/engineers/spark-dependencies","permalink":"/engineers/spark-dependencies","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"dependencies","permalink":"/tags/dependencies"},{"inline":true,"label":"maven","permalink":"/tags/maven"},{"inline":true,"label":"extensibility","permalink":"/tags/extensibility"}],"version":"current","frontMatter":{"title":"Spark dependencies","id":"spark-dependencies","slug":"/engineers/spark-dependencies","description":"Reuse Spark dependencies in pipelines and jobs","tags":["dependencies","maven","extensibility"]},"sidebar":"platformSidebar","previous":{"title":"Dependencies","permalink":"/engineers/dependencies"},"next":{"title":"Prophecy libraries","permalink":"/engineers/prophecy-libraries"}}');var t=i(74848),d=i(28453);const r={title:"Spark dependencies",id:"spark-dependencies",slug:"/engineers/spark-dependencies",description:"Reuse Spark dependencies in pipelines and jobs",tags:["dependencies","maven","extensibility"]},a=void 0,o={},c=[{value:"Scope",id:"scope",level:2},{value:"Manage dependencies",id:"manage-dependencies",level:2},{value:"Add dependencies",id:"add-dependencies",level:3},{value:"Update dependencies",id:"update-dependencies",level:3},{value:"Install dependencies on a Spark cluster",id:"install-dependencies-on-a-spark-cluster",level:2},{value:"Storage",id:"storage",level:2},{value:"Build System template",id:"build-system-template",level:2},{value:"Jobs support",id:"jobs-support",level:3},{value:"Managing dependencies for WHL format deployments",id:"managing-dependencies-for-whl-format-deployments",level:3},{value:"Run the PBT command",id:"run-the-pbt-command",level:4},{value:"Configuring Spark version",id:"configuring-spark-version",level:4}];function l(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,d.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.admonition,{title:"Enterprise",type:"edition",children:(0,t.jsxs)(n.p,{children:["Available for ",(0,t.jsx)(n.a,{href:"/getting-started/editions/",children:"Enterprise Edition"})," only."]})}),"\n",(0,t.jsx)(n.p,{children:"Dependencies allow you to make use of a variety of packages in your data pipelines and jobs. There are three types of dependencies for Spark projects:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Package Hub Dependencies."})," When you import a project from the Package Hub as a dependency, you gain access to all its components, including pipelines, gems, and business rules for use in your own project. If a new version of the project is published, you can update your dependency to take advantage of the latest changes."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsxs)(n.strong,{children:["Python (",(0,t.jsx)(n.a,{href:"https://pypi.org/",children:"PyPI"}),") Dependencies."]})," Python packages hosted on PyPI can be imported in your project using standard PyPI coordinates. This is useful for including popular libraries or your own published packages in PySpark pipelines."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsxs)(n.strong,{children:["Scala (",(0,t.jsx)(n.a,{href:"https://mvnrepository.com/",children:"Maven"}),") Dependencies."]})," These are JAR packages hosted on Maven Central or other Maven-compatible repositories. You can add them to Scala projects using Maven coordinates."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"scope",children:"Scope"}),"\n",(0,t.jsx)(n.p,{children:"Dependencies can be stored at the project or pipeline level. Project-level dependencies are available to all pipelines in the project. We recommend that you store dependencies at the project level to minimize time spent connecting to clusters when switching between pipelines."}),"\n",(0,t.jsx)(n.h2,{id:"manage-dependencies",children:"Manage dependencies"}),"\n",(0,t.jsx)(n.p,{children:"There are two primary ways to manage project dependencies:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["A) Open your project in the ",(0,t.jsx)(n.strong,{children:"Pipeline editor"}),". Then, click ",(0,t.jsx)(n.strong,{children:"\u2026 > Dependencies"})," from the project header."]}),"\n",(0,t.jsxs)(n.li,{children:["B) Open your project from the ",(0,t.jsx)(n.strong,{children:"Metadata"})," page. Then, click on the ",(0,t.jsx)(n.strong,{children:"Dependencies"})," tab."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"add-dependencies",children:"Add dependencies"}),"\n",(0,t.jsxs)(n.p,{children:["The table below describes the fields available when you select ",(0,t.jsx)(n.strong,{children:"Add Dependency"}),"."]}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Parameter"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Scope"}),(0,t.jsx)(n.td,{children:"The dependency is enabled at the project level or the pipeline level."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Type"}),(0,t.jsx)(n.td,{children:"The dependency is either from the Package Hub, Scala (Maven) or Python (PyPI)."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Name"}),(0,t.jsx)(n.td,{children:"This will identify the dependency."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Version/Package/Coordinates"}),(0,t.jsxs)(n.td,{children:["For Package Hub dependencies, input the package version. ",(0,t.jsx)("br",{}),"For Scala, use the Maven coordinates in the ",(0,t.jsx)(n.code,{children:"groupId:artifcatId:version"})," format. ",(0,t.jsx)("br",{}),"For Python, use the package and the version number."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Repository (Advanced)"}),(0,t.jsx)(n.td,{children:"If you'd like to use an external repository (like your organization\u2019s repository), you can specify the link to it in this field."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Exclusion (Advanced)"}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.strong,{children:"Scala only"})," \u2014 This is an optional list of ",(0,t.jsx)(n.code,{children:"groupId:artifactId"})," pairs of dependencies you'd like to exclude. Learn more about dependency exclusion ",(0,t.jsx)(n.a,{href:"https://maven.apache.org/guides/introduction/introduction-to-optional-and-excludes-dependencies.html#dependency-exclusions",children:"here"}),"."]})]})]})]}),"\n",(0,t.jsxs)(n.p,{children:["Once you save a new dependency, Prophecy will validate it to make sure the dependency coordinates are valid and accessible. If\nthat fails, you should see an ",(0,t.jsx)(n.strong,{children:"invalid coordinates"})," error."]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["In rare cases, your dependency might be only accessible to the Spark cluster or the build system, but not to Prophecy\nitself. If you're confident that the dependency is correct, but the validation error shows up, it's safe to press ",(0,t.jsx)(n.strong,{children:"Save\nAnyways"})," to ignore that warning."]})}),"\n",(0,t.jsx)(n.h3,{id:"update-dependencies",children:"Update dependencies"}),"\n",(0,t.jsxs)(n.p,{children:["If a dependency update is available, the ",(0,t.jsx)(n.strong,{children:"Update"})," label should appear next to the dependency. Click the label to update the individual dependency."]}),"\n",(0,t.jsx)(n.p,{children:"Spark projects often have multiple dependencies. To simultaneously update all dependencies to the latest version, you have two options:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["A) Click ",(0,t.jsx)(n.strong,{children:"Update all dependencies"})," in the Dependencies panel of the project editor."]}),"\n",(0,t.jsxs)(n.li,{children:["B) Click ",(0,t.jsx)(n.strong,{children:"Update all dependencies"})," in the Dependencies tab of the project metadata."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Update dependencies",src:i(61895).A+"",width:"2880",height:"1084"})}),"\n",(0,t.jsx)(n.admonition,{type:"caution",children:(0,t.jsx)(n.p,{children:"Connecting a Prophecy project to a Spark cluster with a different dependency version will prompt a cluster restart."})}),"\n",(0,t.jsx)(n.h2,{id:"install-dependencies-on-a-spark-cluster",children:"Install dependencies on a Spark cluster"}),"\n",(0,t.jsx)(n.p,{children:"When you connect a pipeline to a cluster, dependencies are automatically installed on that cluster. However, there are two cases that can prevent automatic installation:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dependency on the cluster's Spark and Scala versions."})," A few dependencies depend on your cluster\u2019s Spark and Scala versions. You can usually find these version requirements in the respective repositories. For example, take a look at the ",(0,t.jsx)(n.a,{href:"https://mvnrepository.com/artifact/com.crealytics/spark-excel",children:"Spark Excel"})," page in the Maven Repository."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Required manual installation on cluster"}),". Additionally, certain dependencies must be installed directly on your cluster. This is documented per dependency."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Update Spark cluster",src:i(25318).A+"",width:"2880",height:"912"})}),"\n",(0,t.jsx)(n.h2,{id:"storage",children:"Storage"}),"\n",(0,t.jsxs)(n.p,{children:["Dependencies are natively saved within your build system files. For example, if you're using Scala, dependencies are saved in the\n",(0,t.jsx)(n.strong,{children:"pom.xml"})," file. If you're using Python, dependencies are saved in the ",(0,t.jsx)(n.strong,{children:"setup.py"})," file."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Storage code for Scala and Python",src:i(93679).A+"",width:"2846",height:"900"})}),"\n",(0,t.jsx)(n.h2,{id:"build-system-template",children:"Build System template"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.strong,{children:"Build System"})," template in the Settings tab of a project defines how build files are generated for all pipelines within a project. You can edit the template to include specific items you need during the build process\u2014for example, adding the JaCoCo plugin to enable coverage report generation."]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.p,{children:"For any help required to enable the template for older projects, please reach out to the Prophecy support team."})}),"\n",(0,t.jsx)(n.h3,{id:"jobs-support",children:"Jobs support"}),"\n",(0,t.jsxs)(n.p,{children:["When the Build System template is enabled for a project and you ",(0,t.jsx)(n.a,{href:"#add-dependencies",children:"add a dependency"}),", Prophecy will automatically update your ",(0,t.jsx)(n.strong,{children:"pom.xml"})," or ",(0,t.jsx)(n.strong,{children:"setup.py"})," files to include it."]}),"\n",(0,t.jsxs)(n.p,{children:["Though not recommended, if templating is disabled and you still want to add dependencies that are visible to your pipelines when scheduled, you can manually edit the ",(0,t.jsx)(n.strong,{children:"pom.xml"})," or ",(0,t.jsx)(n.strong,{children:"setup.py"})," files. Below is an example for a Scala project."]}),"\n",(0,t.jsxs)(n.p,{children:["To add the ",(0,t.jsx)(n.code,{children:"io.github.etspaceman:scalacheck-faker_2.12:7.0.0"})," dependency, edit the ",(0,t.jsx)(n.strong,{children:"pom.xml"})," like so:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"...\n<dependencies>\n   <dependency>\n       <groupId>io.github.etspaceman</groupId>\n       <artifactId>scalacheck-faker_2.12</artifactId>\n       <version>7.0.0</version>\n   </dependency>\n\n   ...\n</dependencies>\n...\n"})}),"\n",(0,t.jsx)(n.h3,{id:"managing-dependencies-for-whl-format-deployments",children:"Managing dependencies for WHL format deployments"}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["Special consideration for Scala dependencies in Pyspark projects must be given when deploying WHL files outside of PBT.\nIf you do not create jobs in the Prophecy editor or use the ",(0,t.jsx)(n.code,{children:"pbt deploy"})," or ",(0,t.jsx)(n.code,{children:"pbt deploy-v2"})," commands, then this\nsection will help track those Scala dependencies."]})}),"\n",(0,t.jsxs)(n.p,{children:["When manually deploying pipelines using WHL format, you need to account for dependencies in both Python and Scala.\nWHL files inherently record Python dependencies, which ensures Python-related packages are handled during deployment.\nYou can use the following option in ",(0,t.jsx)(n.a,{href:"/engineers/prophecy-build-tool",children:"Prophecy Build Tool (PBT)"})," to\ngenerate and include Scala dependency metadata in your deployment."]}),"\n",(0,t.jsx)(n.h4,{id:"run-the-pbt-command",children:"Run the PBT command"}),"\n",(0,t.jsx)(n.p,{children:"Run the following PBT command in your project directory to capture Scala dependencies and include them in the WHL package:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"pbt build-v2 --add-pom-xml-python --path .\n"})}),"\n",(0,t.jsx)(n.p,{children:"This command will:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Generate a dummy ",(0,t.jsx)(n.code,{children:"pom.xml"})," and ",(0,t.jsx)(n.code,{children:"MAVEN_COORDINATES"})," containing Maven dependencies.\n(These files contain the same information in different formats for your convenience.)"]}),"\n",(0,t.jsxs)(n.li,{children:["Add these files to the WHL package under the directory: ",(0,t.jsx)(n.code,{children:"{package_name}-1.0.data/data/"})]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"configuring-spark-version",children:"Configuring Spark version"}),"\n",(0,t.jsx)(n.p,{children:"Set the SPARK_VERSION environment variable to specify the Spark version you intend to use in your execution environment."}),"\n",(0,t.jsxs)(n.p,{children:["The version must end with ",(0,t.jsx)(n.code,{children:".0"}),". To set the environment variable, run a command like ",(0,t.jsx)(n.code,{children:"export SPARK_VERSION=3.4.0"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["If SPARK_VERSION is not set, PBT will set the Spark version in the Maven coordinate with the placeholder string ",(0,t.jsx)(n.code,{children:"{{REPLACE_ME}}"}),"."]})]})}function p(e={}){const{wrapper:n}={...(0,d.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},61895:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/update-all-dependencies-65d9428f299c1de7dc4e5ffafb702e91.png"},93679:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/dependencies-storage-75027ef62946ad019ace834194ce35d3.png"}}]);