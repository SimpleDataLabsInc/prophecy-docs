---
sidebar_position: 1
title: Databricks Jobs
description: Databricks jobs
id: databricks-jobs
draft: true
tags:
  - jobs
  - deployment
---

Once you have developed a spark workflow using prophecy, you will want to schedule it to run at some frequency. To
support this, prophecy integrates with Airflow and provides a low-code interface to develop Airflow DAGs.

## Development

## Configuration

## Deployment

## Monitoring

## Guides

1. [How to trigger a job from another job?](/tutorials/low-code-jobs/multi-jobs-trigger)
2. [How to design a reliable CI/CD process?](/tutorials/low-code-jobs/reliable-ci-cd)
