"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[39444],{15680:(e,n,t)=>{t.d(n,{xA:()=>d,yg:()=>g});var o=t(96540);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,o,a=function(e,n){if(null==e)return{};var t,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=o.createContext({}),p=function(e){var n=o.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},d=function(e){var n=p(e.components);return o.createElement(s.Provider,{value:n},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},u=o.forwardRef((function(e,n){var t=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),c=p(t),u=a,g=c["".concat(s,".").concat(u)]||c[u]||m[u]||r;return t?o.createElement(g,i(i({ref:n},d),{},{components:t})):o.createElement(g,i({ref:n},d))}));function g(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var r=t.length,i=new Array(r);i[0]=u;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[c]="string"==typeof e?e:a,i[1]=l;for(var p=2;p<r;p++)i[p]=t[p];return o.createElement.apply(null,i)}return o.createElement.apply(null,t)}u.displayName="MDXCreateElement"},3514:(e,n,t)=>{t.d(n,{A:()=>y});var o=t(96540),a=t(20053),r=t(84142),i=t(75489),l=t(16654),s=t(21312);const p={cardContainer:"cardContainer_fWXF",cardTitle:"cardTitle_rnsV",cardDescription:"cardDescription_PWke"};function d(e){let{href:n,children:t}=e;return o.createElement(i.A,{href:n,className:(0,a.A)("card padding--lg",p.cardContainer)},t)}function c(e){let{href:n,icon:t,title:r,description:i}=e;return o.createElement(d,{href:n},o.createElement("h2",{className:(0,a.A)("text--truncate",p.cardTitle),title:r},t," ",r),i&&o.createElement("p",{className:(0,a.A)("text--truncate",p.cardDescription),title:i},i))}function m(e){let{item:n}=e;const t=(0,r._o)(n);return t?o.createElement(c,{href:t,icon:"\ud83d\uddc3\ufe0f",title:n.label,description:n.description??(0,s.T)({message:"{count} items",id:"theme.docs.DocCard.categoryDescription",description:"The default description for a category card in the generated index about how many items this category includes"},{count:n.items.length})}):null}function u(e){let{item:n}=e;const t=(0,l.A)(n.href)?"\ud83d\udcc4\ufe0f":"\ud83d\udd17",a=(0,r.cC)(n.docId??void 0);return o.createElement(c,{href:n.href,icon:t,title:n.label,description:n.description??a?.description})}function g(e){let{item:n}=e;switch(n.type){case"link":return o.createElement(u,{item:n});case"category":return o.createElement(m,{item:n});default:throw new Error(`unknown item type ${JSON.stringify(n)}`)}}function h(e){let{className:n}=e;const t=(0,r.$S)();return o.createElement(y,{items:t.items,className:n})}function y(e){const{items:n,className:t}=e;if(!n)return o.createElement(h,e);const i=(0,r.d1)(n);return o.createElement("section",{className:(0,a.A)("row",t)},i.map(((e,n)=>o.createElement("article",{key:n,className:"col col--6 margin-bottom--lg"},o.createElement(g,{item:e})))))}},19365:(e,n,t)=>{t.d(n,{A:()=>i});var o=t(96540),a=t(20053);const r={tabItem:"tabItem_Ymn6"};function i(e){let{children:n,hidden:t,className:i}=e;return o.createElement("div",{role:"tabpanel",className:(0,a.A)(r.tabItem,i),hidden:t},n)}},11470:(e,n,t)=>{t.d(n,{A:()=>v});var o=t(58168),a=t(96540),r=t(20053),i=t(23104),l=t(56347),s=t(57485),p=t(31682),d=t(89466);function c(e){return function(e){return a.Children.map(e,(e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:n,label:t,attributes:o,default:a}}=e;return{value:n,label:t,attributes:o,default:a}}))}function m(e){const{values:n,children:t}=e;return(0,a.useMemo)((()=>{const e=n??c(t);return function(e){const n=(0,p.X)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function u(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function g(e){let{queryString:n=!1,groupId:t}=e;const o=(0,l.W6)(),r=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,s.aZ)(r),(0,a.useCallback)((e=>{if(!r)return;const n=new URLSearchParams(o.location.search);n.set(r,e),o.replace({...o.location,search:n.toString()})}),[r,o])]}function h(e){const{defaultValue:n,queryString:t=!1,groupId:o}=e,r=m(e),[i,l]=(0,a.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!u({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const o=t.find((e=>e.default))??t[0];if(!o)throw new Error("Unexpected error: 0 tabValues");return o.value}({defaultValue:n,tabValues:r}))),[s,p]=g({queryString:t,groupId:o}),[c,h]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[o,r]=(0,d.Dv)(t);return[o,(0,a.useCallback)((e=>{t&&r.set(e)}),[t,r])]}({groupId:o}),y=(()=>{const e=s??c;return u({value:e,tabValues:r})?e:null})();(0,a.useLayoutEffect)((()=>{y&&l(y)}),[y]);return{selectedValue:i,selectValue:(0,a.useCallback)((e=>{if(!u({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);l(e),p(e),h(e)}),[p,h,r]),tabValues:r}}var y=t(92303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function b(e){let{className:n,block:t,selectedValue:l,selectValue:s,tabValues:p}=e;const d=[],{blockElementScrollPositionUntilNextRender:c}=(0,i.a_)(),m=e=>{const n=e.currentTarget,t=d.indexOf(n),o=p[t].value;o!==l&&(c(n),s(o))},u=e=>{let n=null;switch(e.key){case"Enter":m(e);break;case"ArrowRight":{const t=d.indexOf(e.currentTarget)+1;n=d[t]??d[0];break}case"ArrowLeft":{const t=d.indexOf(e.currentTarget)-1;n=d[t]??d[d.length-1];break}}n?.focus()};return a.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":t},n)},p.map((e=>{let{value:n,label:t,attributes:i}=e;return a.createElement("li",(0,o.A)({role:"tab",tabIndex:l===n?0:-1,"aria-selected":l===n,key:n,ref:e=>d.push(e),onKeyDown:u,onClick:m},i,{className:(0,r.A)("tabs__item",f.tabItem,i?.className,{"tabs__item--active":l===n})}),t??n)})))}function S(e){let{lazy:n,children:t,selectedValue:o}=e;const r=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=r.find((e=>e.props.value===o));return e?(0,a.cloneElement)(e,{className:"margin-top--md"}):null}return a.createElement("div",{className:"margin-top--md"},r.map(((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==o}))))}function C(e){const n=h(e);return a.createElement("div",{className:(0,r.A)("tabs-container",f.tabList)},a.createElement(b,(0,o.A)({},e,n)),a.createElement(S,(0,o.A)({},e,n)))}function v(e){const n=(0,y.A)();return a.createElement(C,(0,o.A)({key:String(n)},e))}},53065:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>m,contentTitle:()=>d,default:()=>y,frontMatter:()=>p,metadata:()=>c,toc:()=>u});var o=t(58168),a=(t(96540),t(15680)),r=t(11470),i=t(19365),l=t(3514),s=t(84142);const p={sidebar_position:1,title:"Gem builder",id:"gem-builder",description:"Gem builder",tags:["gem builder"]},d=void 0,c={unversionedId:"Spark/extensibility/gem-builder/gem-builder",id:"Spark/extensibility/gem-builder/gem-builder",title:"Gem builder",description:"Gem builder",source:"@site/docs/Spark/extensibility/gem-builder/gem-builder.md",sourceDirName:"Spark/extensibility/gem-builder",slug:"/Spark/extensibility/gem-builder/",permalink:"/Spark/extensibility/gem-builder/",draft:!1,tags:[{label:"gem builder",permalink:"/tags/gem-builder"}],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Gem builder",id:"gem-builder",description:"Gem builder",tags:["gem builder"]},sidebar:"defaultSidebar",previous:{title:"User-defined functions",permalink:"/Spark/extensibility/udfs"},next:{title:"Optimization functions",permalink:"/Spark/extensibility/gem-builder/optimization-functions"}},m={},u=[{value:"Getting Started",id:"getting-started",level:2},{value:"Tutorial",id:"tutorial",level:2},{value:"Defining a Gem",id:"defining-a-gem",level:2},{value:"Example",id:"example",level:3},{value:"Parent Class",id:"parent-class",level:3},{value:"Properties Classes",id:"properties-classes",level:3},{value:"Dialog (UI)",id:"dialog-ui",level:3},{value:"Validation",id:"validation",level:3},{value:"State Changes",id:"state-changes",level:3},{value:"Component Code",id:"component-code",level:3},{value:"Source/Target Gems",id:"sourcetarget-gems",level:2},{value:"What&#39;s next",id:"whats-next",level:2}],g={toc:u},h="wrapper";function y(e){let{components:n,...p}=e;return(0,a.yg)(h,(0,o.A)({},g,p,{components:n,mdxType:"MDXLayout"}),(0,a.yg)("admonition",{title:"Enterprise Only",type:"caution"},(0,a.yg)("p",{parentName:"admonition"},"Please ",(0,a.yg)("a",{parentName:"p",href:"https://www.prophecy.io/request-a-demo"},"contact us")," to learn more about the Enterprise offering.")),(0,a.yg)("p",null,"Each Prophecy Pipeline is composed of individual operations, or ",(0,a.yg)("a",{parentName:"p",href:"/concepts/project/gems"},"Gems"),", that perform actions on data. While Prophecy offers dozens of Gems out-of-the-box, some data practitioners want to extend this idea and create their own Gems. Gem Builder allows enterprise users to add custom Gems. Create custom source, target, and transformation Gems, publish, and your team can utilize your custom Gem."),(0,a.yg)("div",{class:"video-container"},(0,a.yg)("iframe",{src:"https://www.youtube.com/embed/K23pOatAeVE",title:"YouTube video player",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:!0})),(0,a.yg)("br",null),(0,a.yg)("h2",{id:"getting-started"},"Getting Started"),(0,a.yg)("p",null,"Custom Gem logic can be shared with other users within the Team and Organization. Navigate to the Gem listing to review Prophecy-defined and User-defined Gems. Add a new Gem or modify an existing Gem. Specify Gem name, preferred language, and Gem category. Paste/Write your code specification at the prompt. Click ",(0,a.yg)("inlineCode",{parentName:"p"},"Preview")," to review the UX. Fill in some values and click ",(0,a.yg)("inlineCode",{parentName:"p"},"save")," to check the Python or Scala code generated. When the Gem is ready, ",(0,a.yg)("inlineCode",{parentName:"p"},"Publish"),"! The new Custom Gem is available to use in Pipelines!"),(0,a.yg)("p",null,"Please refer below video for a step-by-step example:"),(0,a.yg)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,a.yg)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,a.yg)("iframe",{src:"https://user-images.githubusercontent.com/121796483/215807557-c64d2e96-9f2b-47d8-b5ed-7b449dba3246.mp4",title:"Gem builder",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,a.yg)("h2",{id:"tutorial"},"Tutorial"),(0,a.yg)("p",null,"The Gem builder is a tool that enables users to create any custom Gems or modify existing ones. There are two types of Gems:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"DataSource Gems"),": These Gems enable the reading and writing of data from or to various data sources"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Transform Gems"),": These Gems apply transformations/joins/any other custom logic onto any DataFrame(s) that are passed into them.")),(0,a.yg)("p",null,"Programmatically, a Gem is a component with the following parts:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"The ",(0,a.yg)("strong",{parentName:"li"},"Gem UI Component")," to get user information from the screen (This code is rendered on the Prophecy UI)"),(0,a.yg)("li",{parentName:"ul"},"The ",(0,a.yg)("strong",{parentName:"li"},"Gem Code Logic")," which is how the Gem acts within the context of a Pipeline.")),(0,a.yg)("p",null,"Gem code can be written using either Python or Scala."),(0,a.yg)("h2",{id:"defining-a-gem"},"Defining a Gem"),(0,a.yg)("h3",{id:"example"},"Example"),(0,a.yg)(r.A,{mdxType:"Tabs"},(0,a.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-py"},'from prophecy.cb.server.base.ComponentBuilderBase import *\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import *\n\nfrom prophecy.cb.ui.UISpecUtil import getColumnsToHighlight2, validateSColumn\nfrom prophecy.cb.ui.uispec import *\nfrom prophecy.cb.util.StringUtils import isBlank\n\n\nclass Filter(ComponentSpec):\n    name: str = "Filter"\n    category: str = "Transform"\n\n    def optimizeCode(self) -> bool:\n        return True\n\n    @dataclass(frozen=True)\n    class FilterProperties(ComponentProperties):\n        columnsSelector: List[str] = field(default_factory=list)\n        condition: SColumn = SColumn("lit(True)")\n\n    def dialog(self) -> Dialog:\n        return Dialog("Filter").addElement(\n            ColumnsLayout(height="100%")\n                .addColumn(PortSchemaTabs(selectedFieldsProperty=("columnsSelector")).importSchema(), "2fr")\n                .addColumn(StackLayout(height=("100%"))\n                .addElement(TitleElement("Filter Condition"))\n                .addElement(\n                Editor(height=("100%")).withSchemaSuggestions().bindProperty("condition.expression")\n            ), "5fr"))\n\n    def validate(self, component: Component[FilterProperties]) -> List[Diagnostic]:\n        return validateSColumn(component.properties.condition, "condition", component)\n\n    def onChange(self, oldState: Component[FilterProperties], newState: Component[FilterProperties]) -> Component[\n        FilterProperties]:\n        newProps = newState.properties\n        usedColExps = getColumnsToHighlight2([newProps.condition], newState)\n        return newState.bindProperties(replace(newProps, columnsSelector=usedColExps))\n\n    class FilterCode(ComponentCode):\n        def __init__(self, newProps):\n            self.props: Filter.FilterProperties = newProps\n\n        def apply(self, spark: SparkSession, in0: DataFrame) -> DataFrame:\n            return in0.filter(self.props.condition.column())\n'))),(0,a.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-scala"},'package io.prophecy.core.instructions.all\nimport io.prophecy.core.instructions.spec._\nimport io.prophecy.core.program.WorkflowContext\nimport org.apache.spark.sql.{DataFrame, SparkSession}\n\nobject Filter extends ComponentSpec {\n  val name: String = "Filter"\n  val category: String = "Transform"\n  override def optimizeCode: Boolean = true\n\n    type PropertiesType = FilterProperties\n  case class FilterProperties(\n    @Property("Columns selector")\n    columnsSelector: List[String] = Nil,\n    @Property("Filter", "Predicate expression to filter rows of incoming dataframe")\n    condition: SColumn = SColumn("lit(true)")\n  ) extends ComponentProperties\n\n  def dialog: Dialog = Dialog("Filter")\n    .addElement(\n      ColumnsLayout(height = Some("100%"))\n        .addColumn(\n          PortSchemaTabs(selectedFieldsProperty = Some("columnsSelector")).importSchema(),\n          "2fr"\n        )\n        .addColumn(\n          StackLayout(height = Some("100%"))\n            .addElement(TitleElement("Filter Condition"))\n            .addElement(\n              Editor(height = Some("100%"))\n                .withSchemaSuggestions()\n                .bindProperty("condition.expression")\n            ),\n          "5fr"\n        )\n    )\n\n  def validate(component: Component)(implicit context: WorkflowContext): List[Diagnostic] = {\n    val diagnostics =\n      validateSColumn(component.properties.condition, "condition", component)\n    diagnostics.toList\n  }\n\n  def onChange(oldState: Component, newState: Component)(implicit context: WorkflowContext): Component = {\n    val newProps = newState.properties\n    val portId = newState.ports.inputs.head.id\n\n    val expressions = getColumnsToHighlight(List(newProps.condition), newState)\n\n    newState.copy(properties = newProps.copy(columnsSelector = expressions))\n  }\n\n  class FilterCode(props: PropertiesType)(implicit context: WorkflowContext) extends ComponentCode {\n\n    def apply(spark: SparkSession, in: DataFrame): DataFrame = {\n      val out = in.filter(props.condition.column)\n      out\n    }\n\n  }\n\n}\n\n')))),(0,a.yg)("h3",{id:"parent-class"},"Parent Class"),(0,a.yg)("p",null,"Every Gem class needs to extend a parent class from which it inherits the representation of the overall Gem. This includes the UI and the logic.\nFor transform Gems, you need to extend ",(0,a.yg)("inlineCode",{parentName:"p"},"ComponentSpec")," (like in the example above), and for Source/Target Gems you need to extend ",(0,a.yg)("inlineCode",{parentName:"p"},"DatasetSpec"),". We will see the difference between the two at the end."),(0,a.yg)("p",null,"First thing you give after this is the name and category of your Gem, ",(0,a.yg)("inlineCode",{parentName:"p"},'"Filter"')," and ",(0,a.yg)("inlineCode",{parentName:"p"},'"Transform"')," in this example."),(0,a.yg)("p",null,"Another thing to note here is ",(0,a.yg)("inlineCode",{parentName:"p"},"optimizeCode"),". This flag can be set to ",(0,a.yg)("inlineCode",{parentName:"p"},"True")," or ",(0,a.yg)("inlineCode",{parentName:"p"},"False")," value depending on whether we want the Prophecy Optimizer to run on this code to simplify it.\nIn most cases, it's best to leave this value as ",(0,a.yg)("inlineCode",{parentName:"p"},"True"),"."),(0,a.yg)(r.A,{mdxType:"Tabs"},(0,a.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-py"},'class Filter(ComponentSpec):\nname: str = "Filter"\n    category: str = "Transform"\n    def optimizeCode(self) -> bool:\n        return True\n'))),(0,a.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-scala"},'object Filter extends ComponentSpec {\nval name: String = "Filter"\nval category: String = "Transform"\noverride def optimizeCode: Boolean = true\n')))),(0,a.yg)("h3",{id:"properties-classes"},"Properties Classes"),(0,a.yg)("p",null,"There is one class (seen here as ",(0,a.yg)("inlineCode",{parentName:"p"},"FilterProperties"),") that contains a list of the properties to be made available to the user for this particular Gem. Think of these as all the values a user fills out within the template of this Gem, or any other UI state that you need to maintain (seen here as ",(0,a.yg)("inlineCode",{parentName:"p"},"columnsSelector")," and ",(0,a.yg)("inlineCode",{parentName:"p"},"condition"),")."),(0,a.yg)("admonition",{type:"caution"},(0,a.yg)("p",{parentName:"admonition"},"The content of these ",(0,a.yg)("inlineCode",{parentName:"p"},"Properties")," classes is persisted in JSON and stored in Git.")),(0,a.yg)("p",null,"These properties can be ",(0,a.yg)("strong",{parentName:"p"},"set")," in the ",(0,a.yg)("inlineCode",{parentName:"p"},"dialog")," function by taking input from user-controlled UI elements.\nThe properties are then available for reading in the following functions:\n",(0,a.yg)("inlineCode",{parentName:"p"},"validate"),", ",(0,a.yg)("inlineCode",{parentName:"p"},"onChange"),", ",(0,a.yg)("inlineCode",{parentName:"p"},"apply")),(0,a.yg)(r.A,{mdxType:"Tabs"},(0,a.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-py"},'@dataclass(frozen=True)\n    class FilterProperties(ComponentProperties):\n        columnsSelector: List[str] = field(default_factory=list)\n        condition: SColumn = SColumn("lit(True)")\n'))),(0,a.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-scala"},'    case class FilterProperties(\n    @Property("Columns selector")\n    columnsSelector: List[String] = Nil,\n    @Property("Filter", "Predicate expression to filter rows of incoming dataframe")\n    condition: SColumn = SColumn("lit(true)")\n  ) extends ComponentProperties\n\n')))),(0,a.yg)("p",null,"Additional information on these functions are available in the following sections."),(0,a.yg)("h3",{id:"dialog-ui"},"Dialog (UI)"),(0,a.yg)("p",null,"The ",(0,a.yg)("inlineCode",{parentName:"p"},"dialog")," function contains code specific to how the Gem UI should look to the user."),(0,a.yg)(r.A,{mdxType:"Tabs"},(0,a.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-py"},'def dialog(self) -> Dialog:\n        return Dialog("Filter").addElement(\n            ColumnsLayout(height="100%")\n                .addColumn(PortSchemaTabs(selectedFieldsProperty=("columnsSelector")).importSchema(), "2fr")\n                .addColumn(StackLayout(height=("100%"))\n                .addElement(TitleElement("Filter Condition"))\n                .addElement(\n                Editor(height=("100%")).withSchemaSuggestions().bindProperty("condition.expression")\n            ), "5fr"))\n'))),(0,a.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-scala"},'def dialog: Dialog = Dialog("Filter")\n    .addElement(\n      ColumnsLayout(height = Some("100%"))\n        .addColumn(\n          PortSchemaTabs(selectedFieldsProperty = Some("columnsSelector")).importSchema(),\n          "2fr"\n        )\n        .addColumn(\n          StackLayout(height = Some("100%"))\n            .addElement(TitleElement("Filter Condition"))\n            .addElement(\n              Editor(height = Some("100%"))\n                .withSchemaSuggestions()\n                .bindProperty("condition.expression")\n            ),\n          "5fr"\n        )\n    )\n')))),(0,a.yg)("p",null,"The above Dialog code in the filter is rendered on UI like this:"),(0,a.yg)("p",null,(0,a.yg)("img",{alt:"Dialog",src:t(95256).A,width:"1636",height:"846"})),(0,a.yg)("p",null,"There are various UI components that can be defined for custom Gems such as scroll boxes, tabs, buttons, and more! These UI components can be grouped together in various types of panels to create a custom user experience when using the Gem."),(0,a.yg)("p",null,"After the Dialog object is defined, it's serialized as JSON, sent to the UI, and rendered there."),(0,a.yg)("p",null,"Depending on what kind of Gem is being created, either a ",(0,a.yg)("inlineCode",{parentName:"p"},"Dialog")," or a ",(0,a.yg)("inlineCode",{parentName:"p"},"DatasetDialog")," needs to be defined."),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("p",{parentName:"li"},"The ",(0,a.yg)("strong",{parentName:"p"},"Transformation Dialog"),": The Dialog for Transformation Gems (any Gem that is not a Dataset Gem) is created using the ",(0,a.yg)("inlineCode",{parentName:"p"},"dialog")," method, which must return a Dialog object.")),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("p",{parentName:"li"},"The ",(0,a.yg)("strong",{parentName:"p"},"Dataset Dialog"),": The Dialog for a ",(0,a.yg)("a",{parentName:"p",href:"/Spark/gems/source-target/"},"Source/Target")," Gem is a ",(0,a.yg)("inlineCode",{parentName:"p"},"DatasetDialog")," object. You will need to have ",(0,a.yg)("inlineCode",{parentName:"p"},"source")," and ",(0,a.yg)("inlineCode",{parentName:"p"},"target")," methods defined."))),(0,a.yg)("p",null,"Column Selector: This is a special property that you should add if you want to select the columns from UI and then highlight the used columns using the ",(0,a.yg)("inlineCode",{parentName:"p"},"onChange")," function.\nIt is recommended to try out this dialogue code in Gem builder UI and see how each of these elements looks in UI."),(0,a.yg)("h3",{id:"validation"},"Validation"),(0,a.yg)("p",null,"The ",(0,a.yg)("inlineCode",{parentName:"p"},"validate")," method performs validation checks so that in the case where there's any issue with any inputs provided for the user an Error can be displayed. In our example case, this would happen if the Filter condition is empty. Similarly, you can add any validation on your properties."),(0,a.yg)(r.A,{mdxType:"Tabs"},(0,a.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-py"},'def validate(self, component: Component[FilterProperties]) -> List[Diagnostic]:\n        return validateSColumn(component.properties.condition, "condition", component)\n\n'))),(0,a.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-scala"},'def validate(component: Component)(implicit context: WorkflowContext): List[Diagnostic] = {\n    val diagnostics =\n      validateSColumn(component.properties.condition, "condition", component)\n    diagnostics.toList\n  }\n')))),(0,a.yg)("h3",{id:"state-changes"},"State Changes"),(0,a.yg)("p",null,"The ",(0,a.yg)("inlineCode",{parentName:"p"},"onChange")," method is given for the UI State transformations. You are given both the previous and the new incoming state and can merge or modify the state as needed. The properties of the Gem are also accessible to this function, so functions like selecting columns, etc. are possible to add from here."),(0,a.yg)(r.A,{mdxType:"Tabs"},(0,a.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-py"},"def onChange(self, oldState: Component[FilterProperties], newState: Component[FilterProperties]) -> Component[\n        FilterProperties]:\n        newProps = newState.properties\n        usedColExps = getColumnsToHighlight2([newProps.condition], newState)\n        return newState.bindProperties(replace(newProps, columnsSelector=usedColExps))\n\n"))),(0,a.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-scala"},"def onChange(oldState: Component, newState: Component)(implicit context: WorkflowContext): Component = {\n    val newProps = newState.properties\n    val portId = newState.ports.inputs.head.id\n\n    val expressions = getColumnsToHighlight(List(newProps.condition), newState)\n\n    newState.copy(properties = newProps.copy(columnsSelector = expressions))\n  }\n")))),(0,a.yg)("h3",{id:"component-code"},"Component Code"),(0,a.yg)("p",null,"The last class used here is ",(0,a.yg)("inlineCode",{parentName:"p"},"FilterCode")," which is inherited from ",(0,a.yg)("inlineCode",{parentName:"p"},"ComponentCode")," class. This class contains the actual Spark code that needs to run on your Spark cluster. Here the above User Defined properties are accessible using ",(0,a.yg)("inlineCode",{parentName:"p"},"self.props.{property}"),". The Spark code for the Gem logic is defined in the apply function. Input/Output of apply method can only be DataFrame or list of DataFrames or empty.\nFor example, we are calling the ",(0,a.yg)("inlineCode",{parentName:"p"},".filter()")," method in this example in the apply function."),(0,a.yg)(r.A,{mdxType:"Tabs"},(0,a.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-py"},"class FilterCode(ComponentCode):\ndef __init__(self, newProps):\nself.props: Filter.FilterProperties = newProps\n\n    def apply(self, spark: SparkSession, in0: DataFrame) -> DataFrame:\n            return in0.filter(self.props.condition.column())\n"))),(0,a.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-scala"},"class FilterCode(props: PropertiesType)(implicit context: WorkflowContext) extends ComponentCode {\n\n    def apply(spark: SparkSession, in: DataFrame): DataFrame = {\n      val out = in.filter(props.condition.column)\n      out\n    }\n\n  }\n")))),(0,a.yg)("p",null,"You can preview the component in the Gem Builder to see how it looks. You can modify the properties and then save it to preview the generated Spark code which will eventually run on your cluster."),(0,a.yg)("p",null,"To assist the Spark Catalyst Optimizer to build scalable code, Prophecy performs some minor optimizations to the code\ngenerated by the ",(0,a.yg)("inlineCode",{parentName:"p"},"apply()")," method."),(0,a.yg)("admonition",{type:"info"},(0,a.yg)("p",{parentName:"admonition"},"For details on our optimization functions, see ",(0,a.yg)("a",{parentName:"p",href:"/Spark/extensibility/gem-builder/optimization-functions"},"Optimization functions"),".")),(0,a.yg)("h2",{id:"sourcetarget-gems"},"Source/Target Gems"),(0,a.yg)("p",null,"Source/Target Gems are Gems that you use to read/write your Datasets into DataFrames. There are certain differences between how you define a Source/Target Gem and a Transformation Gem. For example, a Source/Target Gem will have two ",(0,a.yg)("inlineCode",{parentName:"p"},"dialog")," and two ",(0,a.yg)("inlineCode",{parentName:"p"},"apply")," functions each for Source and Target respectively. Let's look at them with an example."),(0,a.yg)(r.A,{mdxType:"Tabs"},(0,a.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-py"},'from pyspark.sql import SparkSession, DataFrame\nfrom pyspark.sql.types import StructType\n\nfrom prophecy.cb.server.base.ComponentBuilderBase import ComponentCode, Diagnostic, SeverityLevelEnum\nfrom prophecy.cb.server.base.DatasetBuilderBase import DatasetSpec, DatasetProperties, Component\nfrom prophecy.cb.ui.uispec import *\n\n\nclass ParquetFormat(DatasetSpec):\n    name: str = "parquet"\n    datasetType: str = "File"\n\n    def optimizeCode(self) -> bool:\n        return True\n\n    @dataclass(frozen=True)\n    class ParquetProperties(DatasetProperties):\n        schema: Optional[StructType] = None\n        description: Optional[str] = ""\n        useSchema: Optional[bool] = False\n        path: str = ""\n        mergeSchema: Optional[bool] = None\n        datetimeRebaseMode: Optional[str] = None\n        int96RebaseMode: Optional[str] = None\n        compression: Optional[str] = None\n        partitionColumns: Optional[List[str]] = None\n        writeMode: Optional[str] = None\n        pathGlobFilter: Optional[str] = None\n        modifiedBefore: Optional[str] = None\n        modifiedAfter: Optional[str] = None\n        recursiveFileLookup: Optional[bool] = None\n\n    def sourceDialog(self) -> DatasetDialog:\n        return DatasetDialog("parquet") \\\n            .addSection("LOCATION", TargetLocation("path")) \\\n            .addSection(\n            "PROPERTIES",\n            ColumnsLayout(gap=("1rem"), height=("100%"))\n                .addColumn(\n                ScrollBox().addElement(\n                    StackLayout(height=("100%"))\n                        .addElement(\n                        StackItem(grow=(1)).addElement(\n                            FieldPicker(height=("100%"))\n                                .addField(\n                                TextArea("Description", 2, placeholder="Dataset description..."),\n                                "description",\n                                True\n                            )\n                                .addField(Checkbox("Use user-defined schema"), "useSchema", True)\n                                .addField(Checkbox("Merge schema"), "mergeSchema")\n                                .addField(\n                                SelectBox("Datetime Rebase Mode")\n                                    .addOption("EXCEPTION", "EXCEPTION")\n                                    .addOption("CORRECTED", "CORRECTED")\n                                    .addOption("LEGACY", "LEGACY"),\n                                "datetimeRebaseMode"\n                            )\n                                .addField(\n                                SelectBox("Int96 Rebase Mode")\n                                    .addOption("EXCEPTION", "EXCEPTION")\n                                    .addOption("CORRECTED", "CORRECTED")\n                                    .addOption("LEGACY", "LEGACY"),\n                                "int96RebaseMode"\n                            )\n                                .addField(Checkbox("Recursive File Lookup"), "recursiveFileLookup")\n                                .addField(TextBox("Path Global Filter").bindPlaceholder(""), "pathGlobFilter")\n                                .addField(TextBox("Modified Before").bindPlaceholder(""), "modifiedBefore")\n                                .addField(TextBox("Modified After").bindPlaceholder(""), "modifiedAfter")\n                        )\n                    )\n                ),\n                "auto"\n            )\n                .addColumn(SchemaTable("").bindProperty("schema"), "5fr")\n        ) \\\n            .addSection(\n            "PREVIEW",\n            PreviewTable("").bindProperty("schema")\n        )\n\n    def targetDialog(self) -> DatasetDialog:\n        return DatasetDialog("parquet") \\\n            .addSection("LOCATION", TargetLocation("path")) \\\n            .addSection(\n            "PROPERTIES",\n            ColumnsLayout(gap=("1rem"), height=("100%"))\n                .addColumn(\n                ScrollBox().addElement(\n                    StackLayout(height=("100%")).addElement(\n                        StackItem(grow=(1)).addElement(\n                            FieldPicker(height=("100%"))\n                                .addField(\n                                TextArea("Description", 2, placeholder="Dataset description..."),\n                                "description",\n                                True\n                            )\n                                .addField(\n                                SelectBox("Write Mode")\n                                    .addOption("error", "error")\n                                    .addOption("overwrite", "overwrite")\n                                    .addOption("append", "append")\n                                    .addOption("ignore", "ignore"),\n                                "writeMode"\n                            )\n                                .addField(\n                                SchemaColumnsDropdown("Partition Columns")\n                                    .withMultipleSelection()\n                                    .bindSchema("schema")\n                                    .showErrorsFor("partitionColumns"),\n                                "partitionColumns"\n                            )\n                                .addField(\n                                SelectBox("Compression Codec")\n                                    .addOption("none", "none")\n                                    .addOption("uncompressed", "uncompressed")\n                                    .addOption("gzip", "gzip")\n                                    .addOption("lz4", "lz4")\n                                    .addOption("snappy", "snappy")\n                                    .addOption("lzo", "lzo")\n                                    .addOption("brotli", "brotli")\n                                    .addOption("zstd", "zstd"),\n                                "compression"\n                            )\n                        )\n                    )\n                ),\n                "auto"\n            )\n                .addColumn(SchemaTable("").isReadOnly().withoutInferSchema().bindProperty("schema"), "5fr")\n        )\n\n    def validate(self, component: Component) -> list:\n        diagnostics = super(ParquetFormat, self).validate(component)\n        if len(component.properties.path) == 0:\n            diagnostics.append(\n                Diagnostic("properties.path", "path variable cannot be empty [Location]", SeverityLevelEnum.Error))\n        return diagnostics\n\n    def onChange(self, oldState: Component, newState: Component) -> Component:\n        return newState\n\n    class ParquetFormatCode(ComponentCode):\n        def __init__(self, props):\n            self.props: ParquetFormat.ParquetProperties = props\n\n        def sourceApply(self, spark: SparkSession) -> DataFrame:\n            reader = spark.read.format("parquet")\n            if self.props.mergeSchema is not None:\n                reader = reader.option("mergeSchema", self.props.mergeSchema)\n            if self.props.datetimeRebaseMode is not None:\n                reader = reader.option("datetimeRebaseMode", self.props.datetimeRebaseMode)\n            if self.props.int96RebaseMode is not None:\n                reader = reader.option("int96RebaseMode", self.props.int96RebaseMode)\n            if self.props.modifiedBefore is not None:\n                reader = reader.option("modifiedBefore", self.props.modifiedBefore)\n            if self.props.modifiedAfter is not None:\n                reader = reader.option("modifiedAfter", self.props.modifiedAfter)\n            if self.props.recursiveFileLookup is not None:\n                reader = reader.option("recursiveFileLookup", self.props.recursiveFileLookup)\n            if self.props.pathGlobFilter is not None:\n                reader = reader.option("pathGlobFilter", self.props.pathGlobFilter)\n\n            if self.props.schema is not None and self.props.useSchema:\n                reader = reader.schema(self.props.schema)\n\n            return reader.load(self.props.path)\n\n        def targetApply(self, spark: SparkSession, in0: DataFrame):\n            writer = in0.write.format("parquet")\n            if self.props.compression is not None:\n                writer = writer.option("compression", self.props.compression)\n\n            if self.props.writeMode is not None:\n                writer = writer.mode(self.props.writeMode)\n            if self.props.partitionColumns is not None and len(self.props.partitionColumns) > 0:\n                writer = writer.partitionBy(*self.props.partitionColumns)\n\n            writer.save(self.props.path)\n\n'))),(0,a.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-scala"},'package io.prophecy.core.instructions.all.datasets\n\nimport io.prophecy.core.instructions.all._\nimport io.prophecy.core.instructions.spec._\nimport io.prophecy.core.program.WorkflowContext\nimport org.apache.spark.sql.{DataFrame, SparkSession}\nimport org.apache.spark.sql.types.StructType\nimport io.prophecy.libs._\n\nobject ParquetFormat extends DatasetSpec {\n\n  val name: String = "parquet"\n  val datasetType: String = "File"\n\n  type PropertiesType = ParquetProperties\n  case class ParquetProperties(\n    @Property("Schema")\n    schema: Option[StructType] = None,\n    @Property("Description")\n    description: Option[String] = Some(""),\n    @Property("useSchema")\n    useSchema: Option[Boolean] = Some(false),\n    @Property("Path")\n    path: String = "",\n    @Property(\n      "",\n      "(default is the value specified in spark.sql.parquet.mergeSchema(false)): sets whether we should merge schemas collected from all Parquet part-files. This will override spark.sql.parquet.mergeSchema."\n    )\n    mergeSchema: Option[Boolean] = None,\n    @Property(\n      "datetimeRebaseMode",\n      "The datetimeRebaseMode option allows to specify the rebasing mode for the values of the DATE, TIMESTAMP_MILLIS, TIMESTAMP_MICROS logical types from the Julian to Proleptic Gregorian calendar."\n    )\n    datetimeRebaseMode: Option[String] = None,\n    @Property(\n      "int96RebaseMode",\n      "The int96RebaseMode option allows to specify the rebasing mode for INT96 timestamps from the Julian to Proleptic Gregorian calendar."\n    )\n    int96RebaseMode: Option[String] = None,\n    @Property("compression", "(default: none) compression codec to use when saving to file.")\n    compression: Option[String] = None,\n    @Property("partitionColumns", "Partitioning column.")\n    partitionColumns: Option[List[String]] = None,\n    @Property("Write Mode", """(default: "error") Specifies the behavior when data or table already exists.""")\n    writeMode: Option[String] = None,\n    @Property(\n      "",\n      "an optional glob pattern to only include files with paths matching the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter. It does not change the behavior of partition discovery."\n    )\n    pathGlobFilter: Option[String] = None,\n    @Property(\n      "",\n      "(batch only): an optional timestamp to only include files with modification times occurring before the specified Time. The provided timestamp must be in the following form: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)"\n    )\n    modifiedBefore: Option[String] = None,\n    @Property(\n      "",\n      "(batch only): an optional timestamp to only include files with modification times occurring after the specified Time. The provided timestamp must be in the following form: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)"\n    )\n    modifiedAfter: Option[String] = None,\n    @Property("", "recursively scan a directory for files. Using this option disables partition discovery")\n    recursiveFileLookup: Option[Boolean] = None\n  ) extends DatasetProperties\n\n  def sourceDialog: DatasetDialog = DatasetDialog("parquet")\n    .addSection("LOCATION", TargetLocation("path"))\n    .addSection(\n      "PROPERTIES",\n      ColumnsLayout(gap = Some("1rem"), height = Some("100%"))\n        .addColumn(\n          ScrollBox().addElement(\n            StackLayout(height = Some("100%"))\n              .addElement(\n                StackItem(grow = Some(1)).addElement(\n                  FieldPicker(height = Some("100%"))\n                    .addField(\n                      TextArea("Description", 2, placeholder = "Dataset description..."),\n                      "description",\n                      true\n                    )\n                    .addField(Checkbox("Use user-defined schema"), "useSchema", true)\n                    .addField(Checkbox("Merge schema"), "mergeSchema")\n                    .addField(\n                      SelectBox("Datetime Rebase Mode")\n                        .addOption("EXCEPTION", "EXCEPTION")\n                        .addOption("CORRECTED", "CORRECTED")\n                        .addOption("LEGACY", "LEGACY"),\n                      "datetimeRebaseMode"\n                    )\n                    .addField(\n                      SelectBox("Int96 Rebase Mode")\n                        .addOption("EXCEPTION", "EXCEPTION")\n                        .addOption("CORRECTED", "CORRECTED")\n                        .addOption("LEGACY", "LEGACY"),\n                      "int96RebaseMode"\n                    )\n                    .addField(Checkbox("Recursive File Lookup"), "recursiveFileLookup")\n                    .addField(TextBox("Path Global Filter").bindPlaceholder(""), "pathGlobFilter")\n                    .addField(TextBox("Modified Before").bindPlaceholder(""), "modifiedBefore")\n                    .addField(TextBox("Modified After").bindPlaceholder(""), "modifiedAfter")\n                )\n              )\n          ),\n          "auto"\n        )\n        .addColumn(SchemaTable("").bindProperty("schema"), "5fr")\n    )\n    .addSection(\n      "PREVIEW",\n      PreviewTable("").bindProperty("schema")\n    )\n\n  def targetDialog: DatasetDialog = DatasetDialog("parquet")\n    .addSection("LOCATION", TargetLocation("path"))\n    .addSection(\n      "PROPERTIES",\n      ColumnsLayout(gap = Some("1rem"), height = Some("100%"))\n        .addColumn(\n          ScrollBox().addElement(\n            StackLayout(height = Some("100%")).addElement(\n              StackItem(grow = Some(1)).addElement(\n                FieldPicker(height = Some("100%"))\n                  .addField(\n                    TextArea("Description", 2, placeholder = "Dataset description..."),\n                    "description",\n                    true\n                  )\n                  .addField(\n                    SelectBox("Write Mode")\n                      .addOption("error", "error")\n                      .addOption("overwrite", "overwrite")\n                      .addOption("append", "append")\n                      .addOption("ignore", "ignore"),\n                    "writeMode"\n                  )\n                  .addField(\n                    SchemaColumnsDropdown("Partition Columns")\n                      .withMultipleSelection()\n                      .bindSchema("schema")\n                      .showErrorsFor("partitionColumns"),\n                    "partitionColumns"\n                  )\n                  .addField(\n                    SelectBox("Compression Codec")\n                      .addOption("none", "none")\n                      .addOption("uncompressed", "uncompressed")\n                      .addOption("gzip", "gzip")\n                      .addOption("lz4", "lz4")\n                      .addOption("snappy", "snappy")\n                      .addOption("lzo", "lzo")\n                      .addOption("brotli", "brotli")\n                      .addOption("zstd", "zstd"),\n                    "compression"\n                  )\n              )\n            )\n          ),\n          "auto"\n        )\n        .addColumn(SchemaTable("").isReadOnly().withoutInferSchema().bindProperty("schema"), "5fr")\n    )\n\n  override def validate(component: Component)(implicit context: WorkflowContext): List[Diagnostic] = {\n    import scala.collection.mutable.ListBuffer\n    val diagnostics = ListBuffer[Diagnostic]()\n    diagnostics ++= super.validate(component)\n\n    if (component.properties.path.isEmpty) {\n      diagnostics += Diagnostic("properties.path", "path variable cannot be empty [Location]", SeverityLevel.Error)\n    }\n    if (component.properties.schema.isEmpty) {\n      // diagnostics += Diagnostic("properties.schema", "Schema cannot be empty [Properties]", SeverityLevel.Error)\n    }\n\n    diagnostics.toList\n  }\n\n  def onChange(oldState: Component, newState: Component)(implicit context: WorkflowContext): Component = newState\n\n  class ParquetFormatCode(props: ParquetProperties) extends ComponentCode {\n\n    def sourceApply(spark: SparkSession): DataFrame = {\n      var reader = spark.read\n        .format("parquet")\n        .option("mergeSchema", props.mergeSchema)\n        .option("datetimeRebaseMode", props.datetimeRebaseMode)\n        .option("int96RebaseMode", props.int96RebaseMode)\n        .option("modifiedBefore", props.modifiedBefore)\n        .option("modifiedAfter", props.modifiedAfter)\n        .option("recursiveFileLookup", props.recursiveFileLookup)\n        .option("pathGlobFilter", props.pathGlobFilter)\n\n      if (props.useSchema.isDefined && props.useSchema.get)\n        props.schema.foreach(schema \u21d2 reader = reader.schema(schema))\n\n      reader.load(props.path)\n    }\n\n    def targetApply(spark: SparkSession, in: DataFrame): Unit = {\n      var writer = in.write\n        .format("parquet")\n        .option("compression", props.compression)\n\n      props.writeMode.foreach { mode \u21d2\n        writer = writer.mode(mode)\n      }\n      props.partitionColumns.foreach(pcols \u21d2\n        writer = pcols match {\n          case Nil \u21d2 writer\n          case _ \u21d2 writer.partitionBy(pcols: _*)\n        }\n      )\n      writer.save(props.path)\n    }\n\n  }\n\n}\n\n\n')))),(0,a.yg)("p",null,"Here you can see the differences between a Transform Gem and a DataSource Gem."),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},"The Source/Target Gem extends ",(0,a.yg)("inlineCode",{parentName:"li"},"DatasetSpec"),"."),(0,a.yg)("li",{parentName:"ol"},"It has two Dialog functions: ",(0,a.yg)("inlineCode",{parentName:"li"},"sourceDialog")," and ",(0,a.yg)("inlineCode",{parentName:"li"},"targetDialog"),". They return both a ",(0,a.yg)("inlineCode",{parentName:"li"},"DatasetDialog")," object, whereas for any Transform Gem, the dialog function returns a ",(0,a.yg)("inlineCode",{parentName:"li"},"Dialog")," object."),(0,a.yg)("li",{parentName:"ol"},"The ",(0,a.yg)("inlineCode",{parentName:"li"},"ComponentCode")," class has two apply functions: ",(0,a.yg)("inlineCode",{parentName:"li"},"sourceApply")," and ",(0,a.yg)("inlineCode",{parentName:"li"},"targetApply")," for Source and Target modes respectively.")),(0,a.yg)("p",null,"There is no change in ",(0,a.yg)("inlineCode",{parentName:"p"},"onChange")," and ",(0,a.yg)("inlineCode",{parentName:"p"},"validate")," functions."),(0,a.yg)("h2",{id:"whats-next"},"What's next"),(0,a.yg)("p",null,"To learn more about the Gem builder and additional optimization options, see the following page:"),(0,a.yg)(l.A,{items:(0,s.$S)().items,mdxType:"DocCardList"}))}y.isMDXComponent=!0},95256:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/gem-builder-ui-b0bd1d841891aeb22ddef5398815b8f0.png"}}]);