---
title: Deployment
id: deployment
description: Shipping low-code-sql projects
sidebar_position: 3
tags:
  - SQL
  - deployment
  - airflow
  - databricks
  - jobs
  - scheduling
---

text:

1. **text with link** - for simpler data-Pipeline use-cases, where you just
   orchestrate multiple data-Pipelines to run together. Databricks Jobs is a **recommended** scheduler, if you're
   Databricks Native.

2. **text with link** - for more complex use-cases, where you have to use various operators, or need
   any additional data pre-and-post-processing, you can interface from Prophecy with your production-ready Airflow deployment. To get started with your first Airflow jobs, try Prophecy Managed Airflow using this guide.

Alternatively, since Prophecy provides you native Spark code on Git, you can easily integrate with any other scheduler.
