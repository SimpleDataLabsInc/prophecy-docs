"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[23852],{469:(e,t,s)=>{s.d(t,{A:()=>a});const a=s.p+"assets/images/kafka_target_eg_1-498e2012164f661686168fbd52aebaaf.png"},10466:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>h,contentTitle:()=>d,default:()=>f,frontMatter:()=>c,metadata:()=>a,toc:()=>p});const a=JSON.parse('{"id":"Spark/gems/source-target/file/kafka","title":"Kafka","description":"Parameters and properties to read from and write to Kafka files","source":"@site/docs/Spark/gems/source-target/file/kafka-stream.md","sourceDirName":"Spark/gems/source-target/file","slug":"/engineers/kafka","permalink":"/engineers/kafka","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"gems","permalink":"/tags/gems"},{"inline":true,"label":"file","permalink":"/tags/file"},{"inline":true,"label":"kafka","permalink":"/tags/kafka"}],"version":"current","frontMatter":{"title":"Kafka","id":"kafka","slug":"/engineers/kafka","description":"Parameters and properties to read from and write to Kafka files","tags":["gems","file","kafka"]},"sidebar":"mySidebar","previous":{"title":"JSON","permalink":"/engineers/json"},"next":{"title":"ORC","permalink":"/engineers/orc"}}');var r=s(74848),n=s(28453),i=s(97071),o=s(37244),l=s(56778);const c={title:"Kafka",id:"kafka",slug:"/engineers/kafka",description:"Parameters and properties to read from and write to Kafka files",tags:["gems","file","kafka"]},d=void 0,h={},p=[{value:"Source",id:"source",level:2},{value:"Source location",id:"source-location",level:3},{value:"Source properties",id:"source-properties",level:3},{value:"Example",id:"source-example",level:3},{value:"Compiled code",id:"source-code",level:3},{value:"Target",id:"target",level:2},{value:"Target location",id:"target-location",level:3},{value:"Target properties",id:"target-properties",level:3},{value:"Example",id:"target-example",level:3},{value:"Compiled code",id:"target-code",level:3},{value:"Example Pipeline",id:"example-pipeline",level:2},{value:"Source Pipeline Example",id:"source-pipeline-example",level:3},{value:"Metadata Table",id:"metadata-table",level:4},{value:"Spark Code used for script component",id:"spark-code-used-for-script-component",level:4}];function u(e){const t={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,n.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.A,{python_package_name:"ProphecySparkBasicsPython",python_package_version:"0.2.49+",scala_package_name:"ProphecySparkBasicsScala",scala_package_version:"0.0.1+",scala_lib:"",python_lib:"1.9.24",uc_single:"Not Supported",uc_shared:"14.3+",livy:"Not Supported"}),"\n",(0,r.jsxs)(t.p,{children:["The Kafka file type is used in ",(0,r.jsx)(t.a,{href:"https://kafka.apache.org/",children:"Apache Kafka"}),". Read and write Kafka files using a Source or Target gem."]}),"\n",(0,r.jsx)(t.h2,{id:"source",children:"Source"}),"\n",(0,r.jsxs)(t.p,{children:["The Source gem reads data from Kafka stream in batch mode and allows you to optionally specify the following additional properties. This means that Kafka only reads data incrementally from the last offset stored in the specified Metadata table. If the Metadata table is not present, then Kafka reads data from the ",(0,r.jsx)(t.code,{children:"earliest"})," offset."]}),"\n",(0,r.jsx)(t.h3,{id:"source-location",children:"Source location"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Parameter"}),(0,r.jsx)(t.th,{children:"Description"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Bootstrap Server/Broker List"}),(0,r.jsx)(t.td,{children:"Comma separated list of Kafka brokers."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Group Id (Optional)"}),(0,r.jsx)(t.td,{children:"Kafka consumer group ID. Used to identify the consumer group for offset management."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Session timeout (in ms)"}),(0,r.jsxs)(t.td,{children:["Session timeout for Kafka consumer. Default: ",(0,r.jsx)(t.code,{children:"6000"})]})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Security Protocol"}),(0,r.jsxs)(t.td,{children:["Security protocol for Kafka. Default value is ",(0,r.jsx)(t.code,{children:"NO_AUTH"}),"."]})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"SASL Mechanisms"}),(0,r.jsxs)(t.td,{children:["SASL Mechanism for authentication. Default value is ",(0,r.jsx)(t.code,{children:"NO_AUTH"}),"."]})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Credentials"}),(0,r.jsxs)(t.td,{children:["How to provide your credentials. ",(0,r.jsx)("br",{}),"You can select: ",(0,r.jsx)(t.code,{children:"Databricks Secrets"}),", ",(0,r.jsx)(t.code,{children:"Username & Password"}),", ",(0,r.jsx)(t.code,{children:"Environment variables"}),", or ",(0,r.jsx)(t.code,{children:"None"})]})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Kafka topic"}),(0,r.jsx)(t.td,{children:"Comma separated list of Kafka topics."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Store offsets read per partition in Delta table"}),(0,r.jsx)(t.td,{children:"Whether to store offsets read per partition in Delta table. Default: false"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Metadata Table"}),(0,r.jsx)(t.td,{children:"Delta table to store offsets for each topic and partition."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Use SSL Trust Store"}),(0,r.jsx)(t.td,{children:"Enable SSL trust store configuration."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Trust Store Location"}),(0,r.jsx)(t.td,{children:"Path to the SSL trust store file. Required when SSL Trust Store is enabled."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Trust Store Password"}),(0,r.jsx)(t.td,{children:"Password for the SSL trust store file. Required when SSL Trust Store is enabled."})]})]})]}),"\n",(0,r.jsx)(t.h3,{id:"source-properties",children:"Source properties"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Property name"}),(0,r.jsx)(t.th,{children:"Description"}),(0,r.jsx)(t.th,{children:"Default"})]})}),(0,r.jsx)(t.tbody,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Kerberos service name for Kafka SASL"}),(0,r.jsx)(t.td,{children:"Name of your Kerberos service to use in Kafka."}),(0,r.jsx)(t.td,{children:"None"})]})})]}),"\n",(0,r.jsx)(t.h3,{id:"source-example",children:"Example"}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"Example usage of Filter",src:s(49017).A+"",width:"3024",height:"1590"})}),"\n",(0,r.jsx)(t.h3,{id:"source-code",children:"Compiled code"}),"\n",(0,r.jsx)(t.admonition,{type:"tip",children:(0,r.jsxs)(t.p,{children:["To see the compiled code of your project, ",(0,r.jsx)(t.a,{href:"/engineers/pipelines#project-editor",children:"switch to the Code view"})," in the project header."]})}),"\n","\n",(0,r.jsx)(o.A,{children:(0,r.jsx)(l.A,{value:"py",label:"Python",children:(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-py",children:'def KafkaSource(spark: SparkSession) -> DataFrame:\n    from delta.tables import DeltaTable\n    import json\n    from pyspark.dbutils import DBUtils\n\n    if spark.catalog._jcatalog.tableExists(f"metadata.kafka_offsets"):\n        offset_dict = {}\n\n        for row in DeltaTable.forName(spark, f"metadata.kafka_offsets").toDF().collect():\n            if row["topic"] in offset_dict.keys():\n                offset_dict[row["topic"]].update({row["partition"] : row["max_offset"] + 1})\n            else:\n                offset_dict[row["topic"]] = {row["partition"] : row["max_offset"] + 1}\n\n        return (spark.read\\\n            .format("kafka")\\\n            .options(\n              **{\n                "kafka.security.protocol": "NO_AUTH",\n                "kafka.sasl.mechanism": "NO_AUTH",\n                "kafka.bootstrap.servers": "broker1.aws.com:9094,broker2.aws.com:9094",\n                "kafka.session.timeout.ms": "6000",\n                "group.id": "group_id_1",\n                "subscribe": "my_first_topic,my_second_topic",\n                "startingOffsets": json.dumps(offset_dict),\n                "kafka.ssl.truststore.location": "dbfs:/Volumes/tmp/kafka.client.truststore.jks",\n                "kafka.ssl.truststore.password": "password",\n              }\n            )\\\n            .load()\\\n            .withColumn("value", col("value").cast("string"))\\\n            .withColumn("key", col("key").cast("string")))\n    else:\n        return (spark.read\\\n            .format("kafka")\\\n            .options(\n              **{\n                "kafka.security.protocol": "NO_AUTH",\n                "kafka.sasl.mechanism": "NO_AUTH",\n                "kafka.bootstrap.servers": "broker1.aws.com:9094,broker2.aws.com:9094",\n                "kafka.session.timeout.ms": "6000",\n                "group.id": "group_id_1",\n                "subscribe": "my_first_topic,my_second_topic",\n                "kafka.ssl.truststore.location": "dbfs:/Volumes/tmp/kafka.client.truststore.jks",\n                "kafka.ssl.truststore.password": "password",\n              }\n            )\\\n            .load()\\\n            .withColumn("value", col("value").cast("string"))\\\n            .withColumn("key", col("key").cast("string")))\n'})})})}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h2,{id:"target",children:"Target"}),"\n",(0,r.jsxs)(t.p,{children:["The Target gem writes data to each row from the ",(0,r.jsx)(t.code,{children:"Dataframe"})," to a Kafka topic as JSON messages and allows you to optionally specify the following additional properties."]}),"\n",(0,r.jsx)(t.h3,{id:"target-location",children:"Target location"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Parameter"}),(0,r.jsx)(t.th,{children:"Description"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Bootstrap Server/Broker List"}),(0,r.jsx)(t.td,{children:"Comma separated list of Kafka brokers."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Security Protocol"}),(0,r.jsxs)(t.td,{children:["Security protocol for Kafka. Default value is ",(0,r.jsx)(t.code,{children:"NO_AUTH"}),"."]})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"SASL Mechanisms"}),(0,r.jsxs)(t.td,{children:["SASL Mechanism for authentication. Default value is ",(0,r.jsx)(t.code,{children:"NO_AUTH"}),"."]})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Credentials"}),(0,r.jsxs)(t.td,{children:["How to provide your credentials. ",(0,r.jsx)("br",{}),"You can select: ",(0,r.jsx)(t.code,{children:"Databricks Secrets"}),", ",(0,r.jsx)(t.code,{children:"Username & Password"}),", ",(0,r.jsx)(t.code,{children:"Environment variables"}),", or ",(0,r.jsx)(t.code,{children:"None"})]})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Kafka topic"}),(0,r.jsx)(t.td,{children:"Comma separated list of Kafka topics."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Message Unique Key (Optional)"}),(0,r.jsx)(t.td,{children:"Key to help determine which partition to write the data to. Used for message partitioning."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Use SSL Trust Store"}),(0,r.jsx)(t.td,{children:"Enable SSL trust store configuration. When enabled, requires Trust Store Location and Trust Store Password."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Trust Store Location"}),(0,r.jsx)(t.td,{children:"Path to the SSL trust store file. Required when SSL Trust Store is enabled."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Trust Store Password"}),(0,r.jsx)(t.td,{children:"Password for the SSL trust store file. Required when SSL Trust Store is enabled."})]})]})]}),"\n",(0,r.jsx)(t.h3,{id:"target-properties",children:"Target properties"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Property name"}),(0,r.jsx)(t.th,{children:"Description"}),(0,r.jsx)(t.th,{children:"Default"})]})}),(0,r.jsx)(t.tbody,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"Kerberos service name for Kafka SASL"}),(0,r.jsx)(t.td,{children:"Name of your Kerberos service to use in Kafka."}),(0,r.jsx)(t.td,{children:"None"})]})})]}),"\n",(0,r.jsx)(t.h3,{id:"target-example",children:"Example"}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"Example usage of Filter",src:s(469).A+"",width:"3042",height:"1452"})}),"\n",(0,r.jsx)(t.h3,{id:"target-code",children:"Compiled code"}),"\n",(0,r.jsx)(t.admonition,{type:"tip",children:(0,r.jsxs)(t.p,{children:["To see the compiled code of your project, ",(0,r.jsx)(t.a,{href:"/engineers/pipelines#project-editor",children:"switch to the Code view"})," in the project header."]})}),"\n",(0,r.jsx)(o.A,{children:(0,r.jsx)(l.A,{value:"py",label:"Python",children:(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-py",children:'def KafkaTarget(spark: SparkSession, in0: DataFrame):\n    df1 = in0.select(to_json(struct("*")).alias("value"))\n    df2 = df1.selectExpr("CAST(value AS STRING)")\n    df2.write\\\n        .format("kafka")\\\n        .options(\n          **{\n            "kafka.security.protocol": "NO_AUTH",\n            "kafka.sasl.mechanism": "NO_AUTH",\n            "kafka.bootstrap.servers": "broker1.aws.com:9094,broker2.aws.com:9094",\n            "topic": "my_first_topic,my_second_topic",\n            "kafka.ssl.truststore.location": "dbfs:/Volumes/tmp/kafka.client.truststore.jks",\n            "kafka.ssl.truststore.password": "password",\n          }\n        )\\\n        .save()\n'})})})}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h2,{id:"example-pipeline",children:"Example Pipeline"}),"\n",(0,r.jsx)(t.h3,{id:"source-pipeline-example",children:"Source Pipeline Example"}),"\n",(0,r.jsx)(t.p,{children:"In this example, you read JSON messages from Kafka, parse them, remove any null messages, and persist the data to a Delta table."}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"Example usage of Filter",src:s(58817).A+"",width:"1139",height:"584"})}),"\n",(0,r.jsx)(t.admonition,{type:"tip",children:(0,r.jsxs)(t.p,{children:["To see the compiled code of your project, ",(0,r.jsx)(t.a,{href:"/engineers/pipelines#project-editor",children:"switch to the Code view"})," in the project header."]})}),"\n",(0,r.jsx)(t.h4,{id:"metadata-table",children:"Metadata Table"}),"\n",(0,r.jsx)(t.p,{children:"To avoid reprocessing messages on subsequent pipeline runs, update a table with the last processed offsets for each Kafka partition and topic. When you run the pipeline, the table only gets a batch of messages that arrived since the previously-processed offset."}),"\n",(0,r.jsxs)(t.p,{children:["In this example, you update ",(0,r.jsx)(t.code,{children:"metadata.kafka_offsets"}),", which has the following structure:"]}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{style:{textAlign:"left"},children:"topic"}),(0,r.jsx)(t.th,{style:{textAlign:"left"},children:"partition"}),(0,r.jsx)(t.th,{style:{textAlign:"left"},children:"max_offset"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"my_first_topic"}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"0"}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"10"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"my_first_topic"}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"1"}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"5"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"my_second_topic"}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"0"}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"10"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"my_second_topic"}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"1"}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"5"})]})]})]}),"\n",(0,r.jsx)(t.p,{children:"Taking this approach provides you the with following benefits:"}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsx)(t.li,{children:"Builds the pipeline interactively without committing any offsets."}),"\n",(0,r.jsx)(t.li,{children:"Production workflows only consume messages that arrived since the previously-processed offset."}),"\n",(0,r.jsx)(t.li,{children:"You can replay old messages by modifying the Metadata table."}),"\n"]}),"\n",(0,r.jsx)(t.admonition,{type:"note",children:(0,r.jsxs)(t.p,{children:["For production workflows the ",(0,r.jsx)(t.a,{href:"/engineers/gems#gem-phase",children:"phase"})," for the ",(0,r.jsx)(t.code,{children:"Script"})," gem that updates the offsets should be greater than the phase of the Target gem. This ensures that offsets only update in the table after Prophecy safely persists the data to the Target."]})}),"\n",(0,r.jsx)(t.h4,{id:"spark-code-used-for-script-component",children:"Spark Code used for script component"}),"\n",(0,r.jsx)(t.admonition,{type:"tip",children:(0,r.jsxs)(t.p,{children:["To see the compiled code of your project, ",(0,r.jsx)(t.a,{href:"/engineers/pipelines#project-editor",children:"switch to the Code view"})," in the project header."]})}),"\n",(0,r.jsx)(o.A,{children:(0,r.jsx)(l.A,{value:"py",label:"Python",children:(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-py",children:'def UpdateOffsets(spark: SparkSession, in0: DataFrame):\n\n    if not ("SColumnExpression" in locals()):\n        from delta.tables import DeltaTable\n        import pyspark.sql.functions as f\n        metadataTable = "metadata.kafka_offsets"\n        metaDataDf = in0.groupBy("partition", "topic").agg(f.max(f.col("`offset`").cast("int")).alias("max_offset"))\n\n        if not spark.catalog._jcatalog.tableExists(metadataTable):\n            metaDataDf.write.format("delta").mode("overwrite").saveAsTable(metadataTable)\n        else:\n            DeltaTable\\\n                .forName(spark, metadataTable)\\\n                .alias("target")\\\n                .merge(\n                  metaDataDf.alias("source"),\n                  (\n                    (col("source.`partition`") == col("target.`partition`"))\n                    & (col("source.`topic`") == col("target.`topic`"))\n                  )\n                )\\\n                .whenMatchedUpdateAll()\\\n                .whenNotMatchedInsertAll()\\\n                .execute()\n'})})})})]})}function f(e={}){const{wrapper:t}={...(0,n.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},28453:(e,t,s)=>{s.d(t,{R:()=>i,x:()=>o});var a=s(96540);const r={},n=a.createContext(r);function i(e){const t=a.useContext(n);return a.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),a.createElement(n.Provider,{value:t},e.children)}},37244:(e,t,s)=>{s.d(t,{A:()=>v});var a=s(96540),r=s(18215),n=s(44319),i=s(56347),o=s(94280),l=s(73024),c=s(58417),d=s(44031);function h(e){return a.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function p(e){const{values:t,children:s}=e;return(0,a.useMemo)(()=>{const e=t??function(e){return h(e).map(({props:{value:e,label:t,attributes:s,default:a}})=>({value:e,label:t,attributes:s,default:a}))}(s);return function(e){const t=(0,c.XI)(e,(e,t)=>e.value===t.value);if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[t,s])}function u({value:e,tabValues:t}){return t.some(t=>t.value===e)}function f({queryString:e=!1,groupId:t}){const s=(0,i.W6)(),r=function({queryString:e=!1,groupId:t}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:e,groupId:t});return[(0,l.aZ)(r),(0,a.useCallback)(e=>{if(!r)return;const t=new URLSearchParams(s.location.search);t.set(r,e),s.replace({...s.location,search:t.toString()})},[r,s])]}function x(e){const{defaultValue:t,queryString:s=!1,groupId:r}=e,n=p(e),[i,l]=(0,a.useState)(()=>function({defaultValue:e,tabValues:t}){if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!u({value:e,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${t.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const s=t.find(e=>e.default)??t[0];if(!s)throw new Error("Unexpected error: 0 tabValues");return s.value}({defaultValue:t,tabValues:n})),[c,h]=f({queryString:s,groupId:r}),[x,m]=function({groupId:e}){const t=function(e){return e?`docusaurus.tab.${e}`:null}(e),[s,r]=(0,d.Dv)(t);return[s,(0,a.useCallback)(e=>{t&&r.set(e)},[t,r])]}({groupId:r}),j=(()=>{const e=c??x;return u({value:e,tabValues:n})?e:null})();(0,o.A)(()=>{j&&l(j)},[j]);return{selectedValue:i,selectValue:(0,a.useCallback)(e=>{if(!u({value:e,tabValues:n}))throw new Error(`Can't select invalid tab value=${e}`);l(e),h(e),m(e)},[h,m,n]),tabValues:n}}var m=s(46916);const j={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var g=s(74848);function k({className:e,block:t,selectedValue:s,selectValue:a,tabValues:i}){const o=[],{blockElementScrollPositionUntilNextRender:l}=(0,n.a_)(),c=e=>{const t=e.currentTarget,r=o.indexOf(t),n=i[r].value;n!==s&&(l(t),a(n))},d=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const s=o.indexOf(e.currentTarget)+1;t=o[s]??o[0];break}case"ArrowLeft":{const s=o.indexOf(e.currentTarget)-1;t=o[s]??o[o.length-1];break}}t?.focus()};return(0,g.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":t},e),children:i.map(({value:e,label:t,attributes:a})=>(0,g.jsx)("li",{role:"tab",tabIndex:s===e?0:-1,"aria-selected":s===e,ref:e=>{o.push(e)},onKeyDown:d,onClick:c,...a,className:(0,r.A)("tabs__item",j.tabItem,a?.className,{"tabs__item--active":s===e}),children:t??e},e))})}function b({lazy:e,children:t,selectedValue:s}){const n=(Array.isArray(t)?t:[t]).filter(Boolean);if(e){const e=n.find(e=>e.props.value===s);return e?(0,a.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,g.jsx)("div",{className:"margin-top--md",children:n.map((e,t)=>(0,a.cloneElement)(e,{key:t,hidden:e.props.value!==s}))})}function y(e){const t=x(e);return(0,g.jsxs)("div",{className:(0,r.A)("tabs-container",j.tabList),children:[(0,g.jsx)(k,{...t,...e}),(0,g.jsx)(b,{...t,...e})]})}function v(e){const t=(0,m.A)();return(0,g.jsx)(y,{...e,children:h(e.children)},String(t))}},49017:(e,t,s)=>{s.d(t,{A:()=>a});const a=s.p+"assets/images/kafka_source_eg_1-5d9b36695526379b9a62cf152b6170bf.png"},56778:(e,t,s)=>{s.d(t,{A:()=>i});s(96540);var a=s(18215);const r={tabItem:"tabItem_Ymn6"};var n=s(74848);function i({children:e,hidden:t,className:s}){return(0,n.jsx)("div",{role:"tabpanel",className:(0,a.A)(r.tabItem,s),hidden:t,children:e})}},58817:(e,t,s)=>{s.d(t,{A:()=>a});const a=s.p+"assets/images/kafka_pipeline_eg-f97290ea76491916f47acf312a5ea95b.gif"},97071:(e,t,s)=>{s.d(t,{A:()=>r});s(96540);var a=s(74848);function r(e){return(0,a.jsxs)("div",{children:[e.python_package_name&&e.python_package_version&&(0,a.jsx)("a",{href:"https://docs.prophecy.io/engineers/package-hub/",children:(0,a.jsxs)("span",{className:"badge badge-dependency",children:[e.python_package_name," ",e.python_package_version]})}),e.scala_package_name&&e.scala_package_version&&(0,a.jsx)("a",{href:"https://docs.prophecy.io/engineers/package-hub",children:(0,a.jsxs)("span",{className:"badge badge-dependency",children:[e.scala_package_name," ",e.scala_package_version]})}),e.python_lib&&(0,a.jsx)("a",{href:"https://docs.prophecy.io/extensibility/dependencies/prophecy-libraries",children:(0,a.jsxs)("span",{className:"badge badge-dependency",children:["ProphecyLibsPython ",e.python_lib]})}),e.scala_lib&&(0,a.jsx)("a",{href:"https://docs.prophecy.io/extensibility/dependencies/prophecy-libraries",children:(0,a.jsxs)("span",{className:"badge badge-dependency",children:["ProphecyLibsScala ",e.scala_lib]})}),e.uc_single&&(0,a.jsx)("a",{href:"https://docs.prophecy.io/administration/fabrics/Spark-fabrics/databricks/ucshared",children:(0,a.jsxs)("span",{className:"badge badge-spark",children:["UC Dedicated Cluster ",e.uc_single]})}),e.uc_shared&&(0,a.jsx)("a",{href:"https://docs.prophecy.io/administration/fabrics/Spark-fabrics/databricks/ucshared",children:(0,a.jsxs)("span",{className:"badge badge-spark",children:["UC Standard Cluster ",e.uc_shared]})}),e.livy&&(0,a.jsx)("a",{href:"https://docs.prophecy.io/administration/fabrics/Spark-fabrics/livy",children:(0,a.jsxs)("span",{className:"badge badge-spark",children:["Livy ",e.livy]})}),(0,a.jsx)("br",{}),(0,a.jsx)("br",{})]})}}}]);