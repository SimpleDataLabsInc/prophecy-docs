"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[51827],{10875:(e,i,t)=>{t.r(i),t.d(i,{assets:()=>o,contentTitle:()=>c,default:()=>h,frontMatter:()=>n,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"administration/fabrics/Spark-fabrics/databricks/databricks","title":"Databricks","description":"Connect Prophecy to your existing Databricks workspace","source":"@site/docs/administration/fabrics/Spark-fabrics/databricks/databricks.md","sourceDirName":"administration/fabrics/Spark-fabrics/databricks","slug":"/administration/fabrics/Spark-fabrics/databricks/","permalink":"/administration/fabrics/Spark-fabrics/databricks/","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"fabric","permalink":"/tags/fabric"},{"inline":true,"label":"databricks","permalink":"/tags/databricks"}],"version":"current","frontMatter":{"title":"Databricks","id":"databricks","description":"Connect Prophecy to your existing Databricks workspace","tags":["fabric","databricks"]},"sidebar":"platformSidebar","previous":{"title":"Spark fabrics","permalink":"/administration/fabrics/Spark-fabrics/Fabrics"},"next":{"title":"Databricks serverless","permalink":"/administration/fabrics/Spark-fabrics/databricks/databricks-serverless"}}');var s=t(74848),a=t(28453);const n={title:"Databricks",id:"databricks",description:"Connect Prophecy to your existing Databricks workspace",tags:["fabric","databricks"]},c=void 0,o={},d=[{value:"Basic Info",id:"basic-info",level:2},{value:"Providers",id:"providers",level:2},{value:"Credentials",id:"credentials",level:3},{value:"Authentication methods",id:"authentication-methods",level:4},{value:"OAuth methods",id:"oauth-methods",level:4},{value:"Job Sizes",id:"job-sizes",level:3},{value:"Prophecy Library",id:"prophecy-library",level:3},{value:"Artifacts",id:"artifacts",level:3}];function l(e){const i={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.p,{children:"Create a Databricks fabric to connect Prophecy to your existing Databricks workspace. With a Databricks fabric, you can connect to existing Spark clusters or create new ones, run Spark pipelines, and read or write data, depending on your Databricks permissions."}),"\n",(0,s.jsx)(i.p,{children:"The following sections describe the parameters needed to set up a Databricks fabric."}),"\n",(0,s.jsxs)(i.admonition,{type:"info",children:[(0,s.jsx)(i.p,{children:"Databricks Runtime 16.4 supports both Scala 2.12 and 2.13, but defaults to Scala 2.13. Databricks Runtime 17.0 and later only support Scala 2.13."}),(0,s.jsx)(i.p,{children:"If your cluster defaults to Scala 2.13 but your Prophecy installation uses libraries built for Scala 2.12, you might see the following error:"}),(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{children:"Library installation attempted on the driver node of cluster 0805-161652-bmq6jitu and failed. Cannot resolve Maven library coordinates. Verify library details, repository access, or Maven repository availability. Error code: ERROR_MAVEN_LIBRARY_RESOLUTION, error message: Library resolution failed because unresolved dependency: io.prophecy:prophecy-libs_2.13:3.5.0-8.11.1: not found\n"})}),(0,s.jsx)(i.p,{children:"To fix this, update Prophecy to version 4.2.0.1 or later, which adds support for Scala 2.13."})]}),"\n",(0,s.jsx)(i.h2,{id:"basic-info",children:"Basic Info"}),"\n",(0,s.jsxs)(i.p,{children:["The ",(0,s.jsx)(i.strong,{children:"Basic Info"})," tab includes the following parameters:"]}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Parameter"}),(0,s.jsx)(i.th,{children:"Description"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Name"}),(0,s.jsx)(i.td,{children:"Name used to identify the project."})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Description (Optional)"}),(0,s.jsx)(i.td,{children:"Description of the project."})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Team"}),(0,s.jsx)(i.td,{children:"Each fabric is associated with one team. All team members will be able to access the fabric in their projects."})]})]})]}),"\n",(0,s.jsx)(i.h2,{id:"providers",children:"Providers"}),"\n",(0,s.jsxs)(i.p,{children:["The ",(0,s.jsx)(i.strong,{children:"Providers"})," tab lets you configure the execution environment settings."]}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Parameter"}),(0,s.jsx)(i.th,{children:"Description"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Provider Type"}),(0,s.jsx)(i.td,{children:"Type of fabric to create (in this case, Spark)."})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Provider"}),(0,s.jsx)(i.td,{children:"Provider of the execution environment (in this case, Databricks)."})]})]})]}),"\n",(0,s.jsx)(i.h3,{id:"credentials",children:"Credentials"}),"\n",(0,s.jsx)(i.p,{children:"Fill out the credentials section to verify your Databricks credentials."}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Parameter"}),(0,s.jsx)(i.th,{children:"Description"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Databricks Workspace URL"}),(0,s.jsx)(i.td,{children:"The URL that points to the workspace that the fabric will use as the execution environment."})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Authentication Method"}),(0,s.jsxs)(i.td,{children:["The method Prophecy will use to authenticate Databricks connections. Access level is tied to the authenticated user\u2019s permissions. At minimum, the authenticated user must have permission to attach clusters in Databricks to use the connection in Prophecy. Some ",(0,s.jsx)(i.a,{href:"https://docs.databricks.com/aws/en/admin/clusters/policy-families",children:"policies"})," additionally require Databricks Workspace Admin permissions."]})]})]})]}),"\n",(0,s.jsx)(i.h4,{id:"authentication-methods",children:"Authentication methods"}),"\n",(0,s.jsxs)(i.p,{children:["Prophecy supports multiple ",(0,s.jsx)(i.a,{href:"https://docs.databricks.com/aws/en/dev-tools/auth",children:"Databricks authentication methods"}),"."]}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Method"}),(0,s.jsx)(i.th,{children:"Description"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Personal Access Token"}),(0,s.jsx)(i.td,{children:"Provide a Databricks Personal Access Token to authenticate the connection. Each user who connects to the fabric will have to provide their own PAT. Prophecy auto-refreshes PATs for AAD users."})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"OAuth"}),(0,s.jsx)(i.td,{children:"Log in with your Databricks account information. Each user who connects to the fabric will have to log in individually."})]})]})]}),"\n",(0,s.jsx)(i.h4,{id:"oauth-methods",children:"OAuth methods"}),"\n",(0,s.jsx)(i.p,{children:"There are two different OAuth methods for Databricks OAuth:"}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Method"}),(0,s.jsx)(i.th,{children:"Authenticated Identity"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"User-based OAuth (U2M)"}),(0,s.jsx)(i.td,{children:"Each user signs in with their own Databricks account. Access is scoped to the individual\u2019s permissions."})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Service Principal OAuth (M2M)"}),(0,s.jsx)(i.td,{children:"Uses a shared service principal identity. Suitable for automation and scheduling."})]})]})]}),"\n",(0,s.jsxs)(i.p,{children:["When you configure a Databricks fabric and select OAuth, the OAuth method is ",(0,s.jsx)(i.strong,{children:"automatically determined"})," by the context."]}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Interactive pipeline execution always uses U2M. U2M cannot be used for scheduled pipeline runs."}),"\n",(0,s.jsxs)(i.li,{children:["Scheduled jobs in deployed projects always use M2M. To schedule jobs using the fabric, you ",(0,s.jsx)(i.strong,{children:"must"})," provide a Service Principal Client ID and Service Principal Client Secret during fabric setup."]}),"\n"]}),"\n",(0,s.jsx)(i.admonition,{type:"info",children:(0,s.jsxs)(i.p,{children:["To leverage OAuth for a Databricks Spark fabric, you or an admin must first create a corresponding ",(0,s.jsx)(i.a,{href:"/oauth-setup",children:"app registration"}),". The fabric will always use the default Databricks app registration."]})}),"\n",(0,s.jsx)(i.h3,{id:"job-sizes",children:"Job Sizes"}),"\n",(0,s.jsx)(i.p,{children:"Job sizes define the cluster configurations that Prophecy can spawn to run pipelines. We recommend choosing the smallest machine types and the fewest nodes necessary for your use case to optimize cost and performance."}),"\n",(0,s.jsxs)(i.p,{children:["By default, Prophecy includes a single job size that uses ",(0,s.jsx)(i.a,{href:"https://docs.databricks.com/aws/en/compute#databricks-runtime",children:"Databricks Runtime 14.3"}),". You can modify this default configuration or define additional job sizes using the Prophecy UI."]}),"\n",(0,s.jsx)(i.p,{children:"To create or update a job size, use the form view or switch to the JSON editor to paste your existing compute configuration from Databricks."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"Job Size configuration",src:t(27195).A+"",width:"2870",height:"1610"})}),"\n",(0,s.jsx)(i.admonition,{type:"note",children:(0,s.jsxs)(i.p,{children:["The job size configuration mirrors the compute configuration in Databricks. To learn more about compute configuration in Databricks, visit their ",(0,s.jsx)(i.a,{href:"https://docs.databricks.com/aws/en/compute/configure",children:"reference guide"}),"."]})}),"\n",(0,s.jsxs)(i.admonition,{type:"caution",children:[(0,s.jsxs)(i.p,{children:["When using Unity Catalog clusters with standard (formerly shared) access mode, note their ",(0,s.jsx)(i.a,{href:"https://docs.databricks.com/en/compute/access-mode-limitations.html#shared-access-mode-limitations-on-unity-catalog",children:"particular limitations"}),". You can see all supported Prophecy features in our ",(0,s.jsx)(i.a,{href:"./ucshared",children:"UC standard cluster support"})," documentation."]}),(0,s.jsxs)(i.p,{children:["If using ",(0,s.jsx)(i.strong,{children:"Auto"})," for the cluster access mode, you may encounter different pipeline behavior in Prophecy depending on the cluster Databricks allocates to you."]})]}),"\n",(0,s.jsx)(i.h3,{id:"prophecy-library",children:"Prophecy Library"}),"\n",(0,s.jsx)(i.p,{children:"Prophecy libraries are Scala and Python libraries that extend the functionality of Apache Spark. These libraries are automatically installed in your Spark execution environment when you attach to a cluster or create a new one."}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:(0,s.jsx)(i.strong,{children:"Resolution mode"})}),(0,s.jsx)(i.th,{children:(0,s.jsx)(i.strong,{children:"Description"})})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Public Central (Default)"}),(0,s.jsx)(i.td,{children:"Retrieve Prophecy libraries from the public artifact repository. Use Maven for Scala projects and PyPI for Python projects."})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Custom Artifactory"}),(0,s.jsx)(i.td,{children:"Retrieve Prophecy libraries from an Artifactory URL."})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"File System"}),(0,s.jsxs)(i.td,{children:["Retrieve Prophecy libraries from a file system. For example, you can add the public S3 bucket path: ",(0,s.jsx)(i.code,{children:"s3://prophecy-public-bucket/prophecy-libs/"})]})]})]})]}),"\n",(0,s.jsx)(i.admonition,{title:"Whitelist Prophecy libraries",type:"info",children:(0,s.jsxs)(i.p,{children:["To use Prophecy libraries in Databricks environments that have enabled Unity Catalog, you must whitelist the required Maven coordinates or JAR paths. Find instructions ",(0,s.jsx)(i.a,{href:"/admin/dbx-whitelist-plibs",children:"here"}),"."]})}),"\n",(0,s.jsx)(i.admonition,{type:"note",children:(0,s.jsxs)(i.p,{children:["A full list of public paths can be found in the documentation on ",(0,s.jsx)(i.a,{href:"/engineers/prophecy-libraries#download-prophecy-libraries",children:"Prophecy libraries"}),". You also can set up ",(0,s.jsx)(i.a,{href:"/admin/dbx-volumes-plibs",children:"Prophecy libraries in your Databricks volumes"}),"."]})}),"\n",(0,s.jsx)(i.h3,{id:"artifacts",children:"Artifacts"}),"\n",(0,s.jsx)(i.p,{children:"Prophecy supports Databricks volumes. When you run a Python or Scala pipeline via a job, you must bundle them as whl/jar artifacts. These artifacts must then be made accessible to the Databricks job in order to use them as a library installed on the cluster. You can designate a path to a volume for uploading the whl/jar files under Artifacts."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"Artifact settings",src:t(88172).A+"",width:"2866",height:"1610"})})]})}function h(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},27195:(e,i,t)=>{t.d(i,{A:()=>r});const r=t.p+"assets/images/dbx-job-size-a150c981e85785c5088e93ef4dc3a84e.png"},28453:(e,i,t)=>{t.d(i,{R:()=>n,x:()=>c});var r=t(96540);const s={},a=r.createContext(s);function n(e){const i=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function c(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:n(e.components),r.createElement(a.Provider,{value:i},e.children)}},88172:(e,i,t)=>{t.d(i,{A:()=>r});const r=t.p+"assets/images/dbx-fabric-settings-91534ff65987f57c60d283aea9c7dfd3.png"}}]);