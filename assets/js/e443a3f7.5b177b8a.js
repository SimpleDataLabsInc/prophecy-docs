"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[18458],{15680:(e,t,r)=>{r.d(t,{xA:()=>c,yg:()=>b});var a=r(96540);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function i(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function o(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?i(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function s(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},i=Object.keys(e);for(a=0;a<i.length;a++)r=i[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)r=i[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var l=a.createContext({}),p=function(e){var t=a.useContext(l),r=t;return e&&(r="function"==typeof e?e(t):o(o({},t),e)),r},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},g="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),g=p(r),u=n,b=g["".concat(l,".").concat(u)]||g[u]||d[u]||i;return r?a.createElement(b,o(o({ref:t},c),{},{components:r})):a.createElement(b,o({ref:t},c))}));function b(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=r.length,o=new Array(i);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[g]="string"==typeof e?e:n,o[1]=s;for(var p=2;p<i;p++)o[p]=r[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,r)}u.displayName="MDXCreateElement"},30523:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var a=r(58168),n=(r(96540),r(15680));const i={title:"Multi Jobs Trigger",id:"multi-jobs-trigger",description:"Complex pipeline interactions and timing",tags:["scheduling","jobs","trigger","pipelines","tutorial"]},o=void 0,s={unversionedId:"Orchestration/multi-jobs-trigger",id:"Orchestration/multi-jobs-trigger",title:"Multi Jobs Trigger",description:"Complex pipeline interactions and timing",source:"@site/docs/Orchestration/multi-jobs-trigger.md",sourceDirName:"Orchestration",slug:"/Orchestration/multi-jobs-trigger",permalink:"/Orchestration/multi-jobs-trigger",draft:!1,tags:[{label:"scheduling",permalink:"/tags/scheduling"},{label:"jobs",permalink:"/tags/jobs"},{label:"trigger",permalink:"/tags/trigger"},{label:"pipelines",permalink:"/tags/pipelines"},{label:"tutorial",permalink:"/tags/tutorial"}],version:"current",frontMatter:{title:"Multi Jobs Trigger",id:"multi-jobs-trigger",description:"Complex pipeline interactions and timing",tags:["scheduling","jobs","trigger","pipelines","tutorial"]},sidebar:"mySidebar",previous:{title:"Databricks Jobs",permalink:"/Orchestration/databricks-jobs"},next:{title:"Extensibility",permalink:"/extensibility/"}},l={},p=[{value:"Time-based Approach",id:"time-based-approach",level:2},{value:"Trigger-based Approach",id:"trigger-based-approach",level:2},{value:"Deploying Jobs",id:"deploying-jobs",level:3},{value:"Job trigger",id:"job-trigger",level:3}],c={toc:p},g="wrapper";function d(e){let{components:t,...i}=e;return(0,n.yg)(g,(0,a.A)({},c,i,{components:t,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"To better structure your projects, sometimes you would like to create multiple different jobs that trigger only a specific set of pipelines. E.g. when using the ",(0,n.yg)("a",{parentName:"p",href:"https://www.prophecy.io/blogs/prophecy-with-delta#bronze-silver-gold-layers"},"Bronze, Silver, Gold")," architecture, one might want to have a project for each one of the stages and run each stage sequentially - run ",(0,n.yg)("em",{parentName:"p"},"Gold")," after ",(0,n.yg)("em",{parentName:"p"},"Silver")," is finished and ",(0,n.yg)("em",{parentName:"p"},"Silver")," and after ",(0,n.yg)("em",{parentName:"p"},"Bronze"),"."),(0,n.yg)("p",null,"However, this poses a question: How to schedule multiple jobs together?"),(0,n.yg)("h2",{id:"time-based-approach"},"Time-based Approach"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Data pipeline",src:r(82948).A,width:"2576",height:"850"})),(0,n.yg)("p",null,"One traditional approach is to schedule the sequential jobs to run at different time intervals. E.g. the ",(0,n.yg)("em",{parentName:"p"},"first job")," can\nrun at 7am and the ",(0,n.yg)("em",{parentName:"p"},"second job")," can run an hour later. This works well, if there's no data dependencies between those\njobs, or we're confident the ",(0,n.yg)("em",{parentName:"p"},"first job")," is going to always finish before the ",(0,n.yg)("em",{parentName:"p"},"second job"),"."),(0,n.yg)("p",null,"But what would happen if our ",(0,n.yg)("em",{parentName:"p"},"first job")," (e.g. bronze ingestion) hasn't yet finished, but the ",(0,n.yg)("em",{parentName:"p"},"second job")," (e.g. silver\ncleanup) is about to start? This could potentially result in only partially processed data or even break the downstream\njobs completely. Recoverability and maintenance also becomes more difficult."),(0,n.yg)("h2",{id:"trigger-based-approach"},"Trigger-based Approach"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Data pipeline",src:r(21114).A,width:"2576",height:"850"})),(0,n.yg)("p",null,"This is where, it might be worth to explore the trigger-based approach. Using this approach, we place additional\ntriggers in our upstream jobs that trigger the jobs that should be executed after those finished."),(0,n.yg)("p",null,"To achieve that we can leverage the ",(0,n.yg)("inlineCode",{parentName:"p"},"Script")," Gem\nand ",(0,n.yg)("a",{parentName:"p",href:"https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunNow"},"Databricks Jobs API"),"."),(0,n.yg)("p",null,"To be able to trigger a job from within another job, we need to:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Deploy the job we want to trigger and find it's Databricks ",(0,n.yg)("inlineCode",{parentName:"li"},"job_id")),(0,n.yg)("li",{parentName:"ol"},"Add a ",(0,n.yg)("inlineCode",{parentName:"li"},"Script")," Gem to the scheduled job that triggers the other one")),(0,n.yg)("h3",{id:"deploying-jobs"},"Deploying Jobs"),(0,n.yg)("p",null,"First of all, to be able to trigger one job from another, we need to release it and get it\u2019s Databricks job id."),(0,n.yg)("p",null,"Please note that this job is disabled - as we\u2019re only going to run it from a manual API, instead of a time-based\ntrigger."),(0,n.yg)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,n.yg)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,n.yg)("iframe",{src:"https://fast.wistia.net/embed/iframe/0f08c3ppuc?videoFoam=true",title:"Deploying Jobs Video",allow:"autoplay; fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,n.yg)("script",{src:"https://fast.wistia.net/assets/external/E-v1.js",async:!0}),(0,n.yg)("h3",{id:"job-trigger"},"Job trigger"),(0,n.yg)("p",null,"Once we have the ID of the job that we'd like to trigger, we can go ahead and create a ",(0,n.yg)("inlineCode",{parentName:"p"},"Script")," Gem in our upstream job\nthat's going to run it."),(0,n.yg)("p",null,"Insert the following script to trigger a job:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},"import requests\n\n# STEP 1: Enter your workspace ID here\ndomain = 'https://dbc-147abc45-b6c7.cloud.databricks.com'\n# STEP 2: Ensure you have a workspace.token secret created and accessible\ntoken = dbutils.secrets.get(scope='workspace', key='token')\n\nresponse = requests.post(\n    '%s/api/2.1/jobs/run-now' % (domain),\n    headers={'Authorization': 'Bearer %s' % token},\n    # STEP 3: Enter a job_id you'd like to trigger\n    json={'job_id': '549136548916411'}\n)\n\nif response.status_code == 200:\n    print(response.json())\nelse:\n    raise Exception('An error occurred triggering the job. Complete error: %s' % (response.json()))\n")),(0,n.yg)("p",null,"Make sure to specify the following arguments:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Databricks workspace url - ",(0,n.yg)("a",{parentName:"li",href:"https://docs.databricks.com/workspace/workspace-details.html#workspace-instance-names-urls-and-ids"},"How to find it?")),(0,n.yg)("li",{parentName:"ol"},"Databricks token - ",(0,n.yg)("a",{parentName:"li",href:"https://docs.databricks.com/dev-tools/api/latest/authentication.html#generate-a-personal-access-token"},"How to generate it?")),(0,n.yg)("li",{parentName:"ol"},"The Databricks job id as previously embedded")),(0,n.yg)("admonition",{type:"caution"},(0,n.yg)("p",{parentName:"admonition"},"Please note, that it's not recommended to store your Databricks token within the code directly, as that creates a\npotential venue for the attacker. A better approach is to leverage Databricks secrets. Check\nout ",(0,n.yg)("a",{parentName:"p",href:"https://docs.databricks.com/security/secrets/secrets.html#create-a-secret-in-a-databricks-backed-scope"},"this guide"),"\nto learn how to create Databricks secrets.")))}d.isMDXComponent=!0},82948:(e,t,r)=>{r.d(t,{A:()=>a});const a=r.p+"assets/images/jobs-tigger-time-based-649f522e8ee643b80d9facaf5588b9b8.png"},21114:(e,t,r)=>{r.d(t,{A:()=>a});const a=r.p+"assets/images/jobs-tigger-trigger-based-b5eed9ac15b9428b382457b81ea37d2f.png"}}]);