"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[29407],{15680:(e,t,a)=>{a.d(t,{xA:()=>g,yg:()=>c});var r=a(96540);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},l=Object.keys(e);for(r=0;r<l.length;r++)a=l[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(r=0;r<l.length;r++)a=l[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var p=r.createContext({}),s=function(e){var t=r.useContext(p),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},g=function(e){var t=s(e.components);return r.createElement(p.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,l=e.originalType,p=e.parentName,g=i(e,["components","mdxType","originalType","parentName"]),d=s(a),m=n,c=d["".concat(p,".").concat(m)]||d[m]||u[m]||l;return a?r.createElement(c,o(o({ref:t},g),{},{components:a})):r.createElement(c,o({ref:t},g))}));function c(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var l=a.length,o=new Array(l);o[0]=m;var i={};for(var p in t)hasOwnProperty.call(t,p)&&(i[p]=t[p]);i.originalType=e,i[d]="string"==typeof e?e:n,o[1]=i;for(var s=2;s<l;s++)o[s]=a[s];return r.createElement.apply(null,o)}return r.createElement.apply(null,a)}m.displayName="MDXCreateElement"},88490:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>o,default:()=>u,frontMatter:()=>l,metadata:()=>i,toc:()=>s});var r=a(58168),n=(a(96540),a(15680));const l={title:"CSV",id:"csv",description:"CSV",tags:["gems","file","csv"]},o=void 0,i={unversionedId:"Spark/gems/source-target/file/csv",id:"Spark/gems/source-target/file/csv",title:"CSV",description:"CSV",source:"@site/docs/Spark/gems/source-target/file/csv.md",sourceDirName:"Spark/gems/source-target/file",slug:"/Spark/gems/source-target/file/csv",permalink:"/Spark/gems/source-target/file/csv",draft:!1,tags:[{label:"gems",permalink:"/tags/gems"},{label:"file",permalink:"/tags/file"},{label:"csv",permalink:"/tags/csv"}],version:"current",frontMatter:{title:"CSV",id:"csv",description:"CSV",tags:["gems","file","csv"]},sidebar:"mySidebar",previous:{title:"Avro",permalink:"/Spark/gems/source-target/file/avro"},next:{title:"Delta",permalink:"/Spark/gems/source-target/file/delta"}},p={},s=[{value:"Source",id:"source",level:2},{value:"Source Parameters",id:"source-parameters",level:3},{value:"Target",id:"target",level:2},{value:"Target Parameters",id:"target-parameters",level:3},{value:"Supported Write Modes",id:"supported-write-modes",level:3},{value:"Produce a single output file",id:"produce-a-single-output-file",level:3}],g={toc:s},d="wrapper";function u(e){let{components:t,...a}=e;return(0,n.yg)(d,(0,r.A)({},g,a,{components:t,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"Read or write delimited files such as CSV (Comma-separated Values) or TSV (Tab-separated Values) in Prophecy."),(0,n.yg)("h2",{id:"source"},"Source"),(0,n.yg)("h3",{id:"source-parameters"},"Source Parameters"),(0,n.yg)("p",null,"CSV ",(0,n.yg)("strong",{parentName:"p"},(0,n.yg)("em",{parentName:"strong"},"Source"))," supports all the available ",(0,n.yg)("a",{parentName:"p",href:"https://spark.apache.org/docs/latest/sql-data-sources-csv.html"},"Spark read options for CSV"),"."),(0,n.yg)("p",null,"The below list contains the additional parameters to read a CSV file:"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Parameter"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Dataset Name"),(0,n.yg)("td",{parentName:"tr",align:null},"Name of the dataset.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Location"),(0,n.yg)("td",{parentName:"tr",align:null},"Location of the file to be loaded. You can read from a file location, Sharepoint (Python only), or SFTP (Python only).")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Schema"),(0,n.yg)("td",{parentName:"tr",align:null},"Schema to applied on the loaded data. Schema can be defined/edited as JSON or inferred using ",(0,n.yg)("inlineCode",{parentName:"td"},"Infer Schema")," button.")))),(0,n.yg)("h2",{id:"target"},"Target"),(0,n.yg)("h3",{id:"target-parameters"},"Target Parameters"),(0,n.yg)("p",null,"CSV ",(0,n.yg)("strong",{parentName:"p"},(0,n.yg)("em",{parentName:"strong"},"Target"))," supports all the available ",(0,n.yg)("a",{parentName:"p",href:"https://spark.apache.org/docs/latest/sql-data-sources-csv.html"},"Spark write options for CSV"),"."),(0,n.yg)("p",null,"The below list contains the additional parameters to write a CSV file:"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Parameter"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"),(0,n.yg)("th",{parentName:"tr",align:null},"Required"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Dataset Name"),(0,n.yg)("td",{parentName:"tr",align:null},"Name of the dataset."),(0,n.yg)("td",{parentName:"tr",align:null},"True")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Location"),(0,n.yg)("td",{parentName:"tr",align:null},"Location of the file(s) to be loaded. For example, ",(0,n.yg)("inlineCode",{parentName:"td"},"dbfs:/data/output.csv"),"."),(0,n.yg)("td",{parentName:"tr",align:null},"True")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Write Mode"),(0,n.yg)("td",{parentName:"tr",align:null},"How to handle existing data. See ",(0,n.yg)("a",{parentName:"td",href:"#supported-write-modes"},"this table")," for a list of available options."),(0,n.yg)("td",{parentName:"tr",align:null},"False")))),(0,n.yg)("h3",{id:"supported-write-modes"},"Supported Write Modes"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Write Mode"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"overwrite"),(0,n.yg)("td",{parentName:"tr",align:null},"If data already exists, overwrite with the contents of the DataFrame.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"append"),(0,n.yg)("td",{parentName:"tr",align:null},"If data already exists, append the contents of the DataFrame.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"ignore"),(0,n.yg)("td",{parentName:"tr",align:null},"If data already exists, do nothing with the contents of the DataFrame. This is similar to a ",(0,n.yg)("inlineCode",{parentName:"td"},"CREATE TABLE IF NOT EXISTS")," in SQL.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"error"),(0,n.yg)("td",{parentName:"tr",align:null},"If data already exists, throw an exception.")))),(0,n.yg)("h3",{id:"produce-a-single-output-file"},"Produce a single output file"),(0,n.yg)("p",null,"Because of Spark's distributed nature, output files are written as multiple separate partition files by default. If you require a single output file, you can add and enable the ",(0,n.yg)("strong",{parentName:"p"},"Create single CSV file")," property in the ",(0,n.yg)("strong",{parentName:"p"},"Properties")," tab of the Target gem."))}u.isMDXComponent=!0}}]);