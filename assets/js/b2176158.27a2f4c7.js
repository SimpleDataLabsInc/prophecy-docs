"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[2815],{3905:function(e,n,t){t.d(n,{Zo:function(){return d},kt:function(){return u}});var o=t(67294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,o,a=function(e,n){if(null==e)return{};var t,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)t=r[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=o.createContext({}),p=function(e){var n=o.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},d=function(e){var n=p(e.components);return o.createElement(s.Provider,{value:n},e.children)},m={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},c=o.forwardRef((function(e,n){var t=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),c=p(t),u=a,h=c["".concat(s,".").concat(u)]||c[u]||m[u]||r;return t?o.createElement(h,i(i({ref:n},d),{},{components:t})):o.createElement(h,i({ref:n},d))}));function u(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var r=t.length,i=new Array(r);i[0]=c;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l.mdxType="string"==typeof e?e:a,i[1]=l;for(var p=2;p<r;p++)i[p]=t[p];return o.createElement.apply(null,i)}return o.createElement.apply(null,t)}c.displayName="MDXCreateElement"},72360:function(e,n,t){t.d(n,{Z:function(){return i}});var o=t(67294),a=t(86010),r="tabItem_OmH5";function i(e){var n=e.children,t=e.hidden,i=e.className;return o.createElement("div",{role:"tabpanel",className:(0,a.Z)(r,i),hidden:t},n)}},9877:function(e,n,t){t.d(n,{Z:function(){return u}});var o=t(87462),a=t(67294),r=t(72389),i=t(67392),l=t(7094),s=t(12466),p=t(86010),d="tabList_uSqn",m="tabItem_LplD";function c(e){var n,t,r,c=e.lazy,u=e.block,h=e.defaultValue,f=e.values,g=e.groupId,y=e.className,k=a.Children.map(e.children,(function(e){if((0,a.isValidElement)(e)&&void 0!==e.props.value)return e;throw new Error("Docusaurus error: Bad <Tabs> child <"+("string"==typeof e.type?e.type:e.type.name)+'>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.')})),b=null!=f?f:k.map((function(e){var n=e.props;return{value:n.value,label:n.label,attributes:n.attributes}})),v=(0,i.l)(b,(function(e,n){return e.value===n.value}));if(v.length>0)throw new Error('Docusaurus error: Duplicate values "'+v.map((function(e){return e.value})).join(", ")+'" found in <Tabs>. Every value needs to be unique.');var C=null===h?h:null!=(n=null!=h?h:null==(t=k.find((function(e){return e.props.default})))?void 0:t.props.value)?n:null==(r=k[0])?void 0:r.props.value;if(null!==C&&!b.some((function(e){return e.value===C})))throw new Error('Docusaurus error: The <Tabs> has a defaultValue "'+C+'" but none of its children has the corresponding value. Available values are: '+b.map((function(e){return e.value})).join(", ")+". If you intend to show no default tab, use defaultValue={null} instead.");var S=(0,l.U)(),w=S.tabGroupChoices,T=S.setTabGroupChoices,N=(0,a.useState)(C),P=N[0],x=N[1],F=[],D=(0,s.o5)().blockElementScrollPositionUntilNextRender;if(null!=g){var E=w[g];null!=E&&E!==P&&b.some((function(e){return e.value===E}))&&x(E)}var O=function(e){var n=e.currentTarget,t=F.indexOf(n),o=b[t].value;o!==P&&(D(n),x(o),null!=g&&T(g,o))},G=function(e){var n,t=null;switch(e.key){case"ArrowRight":var o=F.indexOf(e.currentTarget)+1;t=F[o]||F[0];break;case"ArrowLeft":var a=F.indexOf(e.currentTarget)-1;t=F[a]||F[F.length-1]}null==(n=t)||n.focus()};return a.createElement("div",{className:(0,p.Z)("tabs-container",d)},a.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,p.Z)("tabs",{"tabs--block":u},y)},b.map((function(e){var n=e.value,t=e.label,r=e.attributes;return a.createElement("li",(0,o.Z)({role:"tab",tabIndex:P===n?0:-1,"aria-selected":P===n,key:n,ref:function(e){return F.push(e)},onKeyDown:G,onFocus:O,onClick:O},r,{className:(0,p.Z)("tabs__item",m,null==r?void 0:r.className,{"tabs__item--active":P===n})}),null!=t?t:n)}))),c?(0,a.cloneElement)(k.filter((function(e){return e.props.value===P}))[0],{className:"margin-top--md"}):a.createElement("div",{className:"margin-top--md"},k.map((function(e,n){return(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==P})}))))}function u(e){var n=(0,r.Z)();return a.createElement(c,(0,o.Z)({key:String(n)},e))}},11443:function(e,n,t){t.r(n),t.d(n,{assets:function(){return c},contentTitle:function(){return d},default:function(){return f},frontMatter:function(){return p},metadata:function(){return m},toc:function(){return u}});var o=t(87462),a=t(63366),r=(t(67294),t(3905)),i=t(9877),l=t(72360),s=["components"],p={sidebar_position:3,title:"Gem builder",id:"gem-builder",description:"Gem-Builder",tags:[]},d=void 0,m={unversionedId:"low-code-spark/extensibility/gem-builder",id:"low-code-spark/extensibility/gem-builder",title:"Gem builder",description:"Gem-Builder",source:"@site/docs/low-code-spark/extensibility/gem-builder.md",sourceDirName:"low-code-spark/extensibility",slug:"/low-code-spark/extensibility/gem-builder",permalink:"/low-code-spark/extensibility/gem-builder",draft:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,title:"Gem builder",id:"gem-builder",description:"Gem-Builder",tags:[]},sidebar:"defaultSidebar",previous:{title:"User-defined functions",permalink:"/low-code-spark/extensibility/udfs"},next:{title:"Low-code Jobs",permalink:"/low-code-jobs/"}},c={},u=[{value:"Getting Started",id:"getting-started",level:2},{value:"Tutorial",id:"tutorial",level:2},{value:"Defining a Gem",id:"defining-a-gem",level:2},{value:"Example",id:"example",level:3},{value:"Parent Class",id:"parent-class",level:3},{value:"Properties Classes",id:"properties-classes",level:3},{value:"Dialog",id:"dialog",level:3},{value:"Validation",id:"validation",level:3},{value:"State Changes",id:"state-changes",level:3},{value:"Component Code",id:"component-code",level:3},{value:"Source/Target Gems",id:"sourcetarget-gems",level:2}],h={toc:u};function f(e){var n=e.components,p=(0,a.Z)(e,s);return(0,r.kt)("wrapper",(0,o.Z)({},h,p,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("div",{className:"admonition admonition-caution alert alert--warning"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 16 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"}))),"Enterprise Only")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"Please ",(0,r.kt)("a",{parentName:"p",href:"https://www.prophecy.io/request-a-demo"},"contact us")," to learn more about the Enterprise offering."))),(0,r.kt)("p",null,"Each Prophecy Pipeline is composed of individual operations, or ",(0,r.kt)("a",{parentName:"p",href:"/concepts/gems"},"Gems"),", that perform actions on data. While Prophecy offers dozens of Gems out-of-the-box, some data practitioners want to extend this idea and create their own Gems. Gem Builder allows enterprise users to add custom Gems. Create custom source, target, and transformation Gems, publish, and your team can utilize your custom Gem."),(0,r.kt)("div",{class:"video-container"},(0,r.kt)("iframe",{src:"https://www.youtube.com/embed/K23pOatAeVE",title:"YouTube video player",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:!0})),(0,r.kt)("br",null),(0,r.kt)("h2",{id:"getting-started"},"Getting Started"),(0,r.kt)("p",null,"Custom Gem logic can be shared with other users within the Team and Organization. Navigate to the Gem listing to review Prophecy-defined and User-defined Gems. Add a new Gem or modify an existing Gem. Specify Gem name, preferred language, and Gem category. Paste/Write your code specification at the prompt. Click ",(0,r.kt)("inlineCode",{parentName:"p"},"Preview")," to review the UX. Fill in some values and click ",(0,r.kt)("inlineCode",{parentName:"p"},"save")," to check the Python or Scala code generated. When the Gem is ready, ",(0,r.kt)("inlineCode",{parentName:"p"},"Publish"),"! The new Custom Gem is available to use in Pipelines!"),(0,r.kt)("p",null,"Please refer below video for a step-by-step example:"),(0,r.kt)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,r.kt)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,r.kt)("iframe",{src:"https://user-images.githubusercontent.com/121796483/215807557-c64d2e96-9f2b-47d8-b5ed-7b449dba3246.mp4",title:"Gem builder",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,r.kt)("h2",{id:"tutorial"},"Tutorial"),(0,r.kt)("p",null,"The Gem builder is a tool that enables users to create any custom Gems or modify existing ones. There are two types of Gems:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"DataSource Gems"),": These Gems enable the reading and writing of data from or to various data sources"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Transform Gems"),": These Gems apply transformations/joins/any other custom logic onto any DataFrame(s) that are passed into them.")),(0,r.kt)("p",null,"Programmatically, a Gem is a component with the following parts:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"The ",(0,r.kt)("strong",{parentName:"li"},"Gem UI Component")," to get user information from the screen (This code is rendered on the Prophecy UI)"),(0,r.kt)("li",{parentName:"ul"},"The ",(0,r.kt)("strong",{parentName:"li"},"Gem Code Logic")," which is how the Gem acts within the context of a Pipeline.")),(0,r.kt)("p",null,"Gem code can be written using either Python or Scala."),(0,r.kt)("h2",{id:"defining-a-gem"},"Defining a Gem"),(0,r.kt)("h3",{id:"example"},"Example"),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},'from prophecy.cb.server.base.ComponentBuilderBase import *\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import *\n\nfrom prophecy.cb.ui.UISpecUtil import getColumnsToHighlight2, validateSColumn\nfrom prophecy.cb.ui.uispec import *\nfrom prophecy.cb.util.StringUtils import isBlank\n\n\nclass Filter(ComponentSpec):\n    name: str = "Filter"\n    category: str = "Transform"\n\n    def optimizeCode(self) -> bool:\n        return True\n\n    @dataclass(frozen=True)\n    class FilterProperties(ComponentProperties):\n        columnsSelector: List[str] = field(default_factory=list)\n        condition: SColumn = SColumn("lit(True)")\n\n    def dialog(self) -> Dialog:\n        return Dialog("Filter").addElement(\n            ColumnsLayout(height="100%")\n                .addColumn(PortSchemaTabs(selectedFieldsProperty=("columnsSelector")).importSchema(), "2fr")\n                .addColumn(StackLayout(height=("100%"))\n                .addElement(TitleElement("Filter Condition"))\n                .addElement(\n                Editor(height=("100%")).withSchemaSuggestions().bindProperty("condition.expression")\n            ), "5fr"))\n\n    def validate(self, component: Component[FilterProperties]) -> List[Diagnostic]:\n        return validateSColumn(component.properties.condition, "condition", component)\n\n    def onChange(self, oldState: Component[FilterProperties], newState: Component[FilterProperties]) -> Component[\n        FilterProperties]:\n        newProps = newState.properties\n        usedColExps = getColumnsToHighlight2([newProps.condition], newState)\n        return newState.bindProperties(replace(newProps, columnsSelector=usedColExps))\n\n    class FilterCode(ComponentCode):\n        def __init__(self, newProps):\n            self.props: Filter.FilterProperties = newProps\n\n        def apply(self, spark: SparkSession, in0: DataFrame) -> DataFrame:\n            return in0.filter(self.props.condition.column())\n'))),(0,r.kt)(l.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'package io.prophecy.core.instructions.all\nimport io.prophecy.core.instructions.spec._\nimport io.prophecy.core.program.WorkflowContext\nimport org.apache.spark.sql.{DataFrame, SparkSession}\n\nobject Filter extends ComponentSpec {\n  val name: String = "Filter"\n  val category: String = "Transform"\n  override def optimizeCode: Boolean = true\n\n    type PropertiesType = FilterProperties\n  case class FilterProperties(\n    @Property("Columns selector")\n    columnsSelector: List[String] = Nil,\n    @Property("Filter", "Predicate expression to filter rows of incoming dataframe")\n    condition: SColumn = SColumn("lit(true)")\n  ) extends ComponentProperties\n\n  def dialog: Dialog = Dialog("Filter")\n    .addElement(\n      ColumnsLayout(height = Some("100%"))\n        .addColumn(\n          PortSchemaTabs(selectedFieldsProperty = Some("columnsSelector")).importSchema(),\n          "2fr"\n        )\n        .addColumn(\n          StackLayout(height = Some("100%"))\n            .addElement(TitleElement("Filter Condition"))\n            .addElement(\n              Editor(height = Some("100%"))\n                .withSchemaSuggestions()\n                .bindProperty("condition.expression")\n            ),\n          "5fr"\n        )\n    )\n\n  def validate(component: Component)(implicit context: WorkflowContext): List[Diagnostic] = {\n    val diagnostics =\n      validateSColumn(component.properties.condition, "condition", component)\n    diagnostics.toList\n  }\n\n  def onChange(oldState: Component, newState: Component)(implicit context: WorkflowContext): Component = {\n    val newProps = newState.properties\n    val portId = newState.ports.inputs.head.id\n\n    val expressions = getColumnsToHighlight(List(newProps.condition), newState)\n\n    newState.copy(properties = newProps.copy(columnsSelector = expressions))\n  }\n\n  class FilterCode(props: PropertiesType)(implicit context: WorkflowContext) extends ComponentCode {\n\n    def apply(spark: SparkSession, in: DataFrame): DataFrame = {\n      val out = in.filter(props.condition.column)\n      out\n    }\n\n  }\n\n}\n\n')))),(0,r.kt)("h3",{id:"parent-class"},"Parent Class"),(0,r.kt)("p",null,"Every Gem class needs to extend a parent class from which it inherits the representation of the overall Gem. This includes the UI and the logic.\nFor transform Gems, you need to extend ",(0,r.kt)("inlineCode",{parentName:"p"},"ComponentSpec")," (like in the example above), and for Source/Target Gems you need to extend ",(0,r.kt)("inlineCode",{parentName:"p"},"DatasetSpec"),". We will see the difference between the two at the end."),(0,r.kt)("p",null,"First thing you give after this is the name and category of your Gem, ",(0,r.kt)("inlineCode",{parentName:"p"},'"Filter"')," and ",(0,r.kt)("inlineCode",{parentName:"p"},'"Transform"')," in this example."),(0,r.kt)("p",null,"Another thing to note here is ",(0,r.kt)("inlineCode",{parentName:"p"},"optimizeCode"),". This flag can be set to ",(0,r.kt)("inlineCode",{parentName:"p"},"True")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"False")," value depending on whether we want the Prophecy Optimizer to run on this code to simplify it.\nIn most cases, it's best to leave this value as ",(0,r.kt)("inlineCode",{parentName:"p"},"True"),"."),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},'class Filter(ComponentSpec):\nname: str = "Filter"\n    category: str = "Transform"\n    def optimizeCode(self) -> bool:\n        return True\n'))),(0,r.kt)(l.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'object Filter extends ComponentSpec {\nval name: String = "Filter"\nval category: String = "Transform"\noverride def optimizeCode: Boolean = true\n')))),(0,r.kt)("h3",{id:"properties-classes"},"Properties Classes"),(0,r.kt)("p",null,"There is one class (seen here as ",(0,r.kt)("inlineCode",{parentName:"p"},"FilterProperties"),") that contains a list of the properties to be made available to the user for this particular Gem. Think of these as all the values a user fills out within the template of this Gem, or any other UI state that you need to maintain (seen here as ",(0,r.kt)("inlineCode",{parentName:"p"},"columnsSelector")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"condition"),")."),(0,r.kt)("div",{className:"admonition admonition-caution alert alert--warning"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 16 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"}))),"caution")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"The content of these ",(0,r.kt)("inlineCode",{parentName:"p"},"Properties")," classes is persisted in JSON and stored in Git."))),(0,r.kt)("p",null,"These properties are available in ",(0,r.kt)("inlineCode",{parentName:"p"},"validate"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"onChange")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"apply")," and can be set from ",(0,r.kt)("inlineCode",{parentName:"p"},"dialog"),", functions."),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},'@dataclass(frozen=True)\n    class FilterProperties(ComponentProperties):\n        columnsSelector: List[str] = field(default_factory=list)\n        condition: SColumn = SColumn("lit(True)")\n'))),(0,r.kt)(l.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'    case class FilterProperties(\n    @Property("Columns selector")\n    columnsSelector: List[String] = Nil,\n    @Property("Filter", "Predicate expression to filter rows of incoming dataframe")\n    condition: SColumn = SColumn("lit(true)")\n  ) extends ComponentProperties\n\n')))),(0,r.kt)("p",null,"Let's look at each of these functions."),(0,r.kt)("h3",{id:"dialog"},"Dialog"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"dialog")," function contains code specific to how the Gem UI should look to the user."),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},'def dialog(self) -> Dialog:\n        return Dialog("Filter").addElement(\n            ColumnsLayout(height="100%")\n                .addColumn(PortSchemaTabs(selectedFieldsProperty=("columnsSelector")).importSchema(), "2fr")\n                .addColumn(StackLayout(height=("100%"))\n                .addElement(TitleElement("Filter Condition"))\n                .addElement(\n                Editor(height=("100%")).withSchemaSuggestions().bindProperty("condition.expression")\n            ), "5fr"))\n'))),(0,r.kt)(l.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'def dialog: Dialog = Dialog("Filter")\n    .addElement(\n      ColumnsLayout(height = Some("100%"))\n        .addColumn(\n          PortSchemaTabs(selectedFieldsProperty = Some("columnsSelector")).importSchema(),\n          "2fr"\n        )\n        .addColumn(\n          StackLayout(height = Some("100%"))\n            .addElement(TitleElement("Filter Condition"))\n            .addElement(\n              Editor(height = Some("100%"))\n                .withSchemaSuggestions()\n                .bindProperty("condition.expression")\n            ),\n          "5fr"\n        )\n    )\n')))),(0,r.kt)("p",null,"The above Dialog code in the filter is rendered on UI like this:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Dialog",src:t(18968).Z,width:"1636",height:"846"})),(0,r.kt)("p",null,"There are various UI components that can be defined for custom Gems such as scroll boxes, tabs, buttons, and more! These UI components can be grouped together in various types of panels to create a custom user experience when using the Gem."),(0,r.kt)("p",null,"After the Dialog object is defined, it's serialized as JSON, sent to the UI, and rendered there."),(0,r.kt)("p",null,"Depending on what kind of Gem is being created, either a ",(0,r.kt)("inlineCode",{parentName:"p"},"Dialog")," or a ",(0,r.kt)("inlineCode",{parentName:"p"},"DatasetDialog")," needs to be defined."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"The ",(0,r.kt)("strong",{parentName:"p"},"Transformation Dialog"),": The Dialog for Transformation Gems (any Gem that is not a Dataset Gem) is created using the ",(0,r.kt)("inlineCode",{parentName:"p"},"dialog")," method, which must return a Dialog object.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"The ",(0,r.kt)("strong",{parentName:"p"},"Dataset Dialog"),": The Dialog for a ",(0,r.kt)("a",{parentName:"p",href:"../gems/source-target/"},"Source/Target")," Gem is a ",(0,r.kt)("inlineCode",{parentName:"p"},"DatasetDialog")," object. You will need to have ",(0,r.kt)("inlineCode",{parentName:"p"},"source")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"target")," methods defined."))),(0,r.kt)("p",null,"Column Selector: This is a special property that you should add if you want to select the columns from UI and then highlight the used columns using the ",(0,r.kt)("inlineCode",{parentName:"p"},"onChange")," function.\nIt is recommended to try out this dialogue code in Gem builder UI and see how each of these elements looks in UI."),(0,r.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},(0,r.kt)("strong",{parentName:"p"},"Coming Soon"),"\nWe will soon launch a playground to try out the different UI components and test how the UI renders for each of them."))),(0,r.kt)("h3",{id:"validation"},"Validation"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"validate")," method performs validation checks so that in the case where there's any issue with any inputs provided for the user an Error can be displayed. In our example case if the Filter condition is empty. Similarly, you can add any validation on your properties."),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},'def validate(self, component: Component[FilterProperties]) -> List[Diagnostic]:\n        return validateSColumn(component.properties.condition, "condition", component)\n\n'))),(0,r.kt)(l.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'def validate(component: Component)(implicit context: WorkflowContext): List[Diagnostic] = {\n    val diagnostics =\n      validateSColumn(component.properties.condition, "condition", component)\n    diagnostics.toList\n  }\n')))),(0,r.kt)("h3",{id:"state-changes"},"State Changes"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"onChange")," method is given for the UI State transformations. You are given both the previous and the new incoming state and can merge or modify the state as needed. The properties of the Gem are also accessible to this function, so functions like selecting columns, etc. are possible to add from here."),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"def onChange(self, oldState: Component[FilterProperties], newState: Component[FilterProperties]) -> Component[\n        FilterProperties]:\n        newProps = newState.properties\n        usedColExps = getColumnsToHighlight2([newProps.condition], newState)\n        return newState.bindProperties(replace(newProps, columnsSelector=usedColExps))\n\n"))),(0,r.kt)(l.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},"def onChange(oldState: Component, newState: Component)(implicit context: WorkflowContext): Component = {\n    val newProps = newState.properties\n    val portId = newState.ports.inputs.head.id\n\n    val expressions = getColumnsToHighlight(List(newProps.condition), newState)\n\n    newState.copy(properties = newProps.copy(columnsSelector = expressions))\n  }\n")))),(0,r.kt)("h3",{id:"component-code"},"Component Code"),(0,r.kt)("p",null,"The last class used here is ",(0,r.kt)("inlineCode",{parentName:"p"},"FilterCode")," which is inherited from ",(0,r.kt)("inlineCode",{parentName:"p"},"ComponentCode")," class. This class contains the actual Spark code that needs to run on your Spark cluster. Here the above User Defined properties are accessible using ",(0,r.kt)("inlineCode",{parentName:"p"},"self.props.{property}"),". The Spark code for the Gem logic is defined in the apply function. Input/Output of apply method can only be DataFrame or list of DataFrames or empty.\nFor example, we are calling the ",(0,r.kt)("inlineCode",{parentName:"p"},".filter()")," method in this example in the apply function."),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"class FilterCode(ComponentCode):\ndef __init__(self, newProps):\nself.props: Filter.FilterProperties = newProps\n\n    def apply(self, spark: SparkSession, in0: DataFrame) -> DataFrame:\n            return in0.filter(self.props.condition.column())\n"))),(0,r.kt)(l.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},"class FilterCode(props: PropertiesType)(implicit context: WorkflowContext) extends ComponentCode {\n\n    def apply(spark: SparkSession, in: DataFrame): DataFrame = {\n      val out = in.filter(props.condition.column)\n      out\n    }\n\n  }\n")))),(0,r.kt)("p",null,"You can go ahead and preview the component in the Gem Builder now to see how it looks. You can modify the properties and then save it to preview the generated Spark code which will eventually run on your cluster."),(0,r.kt)("h2",{id:"sourcetarget-gems"},"Source/Target Gems"),(0,r.kt)("p",null,"Source/Target Gems are Gems that you use to read/write your Datasets into DataFrames. There are certain differences between how you define a Source/Target Gem and a Transformation Gem. For example, a Source/Target Gem will have two ",(0,r.kt)("inlineCode",{parentName:"p"},"dialog")," and two ",(0,r.kt)("inlineCode",{parentName:"p"},"apply")," functions each for Source and Target respectively. Let's look at them with an example"),(0,r.kt)(i.Z,{mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},'from pyspark.sql import SparkSession, DataFrame\nfrom pyspark.sql.types import StructType\n\nfrom prophecy.cb.server.base.ComponentBuilderBase import ComponentCode, Diagnostic, SeverityLevelEnum\nfrom prophecy.cb.server.base.DatasetBuilderBase import DatasetSpec, DatasetProperties, Component\nfrom prophecy.cb.ui.uispec import *\n\n\nclass ParquetFormat(DatasetSpec):\n    name: str = "parquet"\n    datasetType: str = "File"\n\n    def optimizeCode(self) -> bool:\n        return True\n\n    @dataclass(frozen=True)\n    class ParquetProperties(DatasetProperties):\n        schema: Optional[StructType] = None\n        description: Optional[str] = ""\n        useSchema: Optional[bool] = False\n        path: str = ""\n        mergeSchema: Optional[bool] = None\n        datetimeRebaseMode: Optional[str] = None\n        int96RebaseMode: Optional[str] = None\n        compression: Optional[str] = None\n        partitionColumns: Optional[List[str]] = None\n        writeMode: Optional[str] = None\n        pathGlobFilter: Optional[str] = None\n        modifiedBefore: Optional[str] = None\n        modifiedAfter: Optional[str] = None\n        recursiveFileLookup: Optional[bool] = None\n\n    def sourceDialog(self) -> DatasetDialog:\n        return DatasetDialog("parquet") \\\n            .addSection("LOCATION", TargetLocation("path")) \\\n            .addSection(\n            "PROPERTIES",\n            ColumnsLayout(gap=("1rem"), height=("100%"))\n                .addColumn(\n                ScrollBox().addElement(\n                    StackLayout(height=("100%"))\n                        .addElement(\n                        StackItem(grow=(1)).addElement(\n                            FieldPicker(height=("100%"))\n                                .addField(\n                                TextArea("Description", 2, placeholder="Dataset description..."),\n                                "description",\n                                True\n                            )\n                                .addField(Checkbox("Use user-defined schema"), "useSchema", True)\n                                .addField(Checkbox("Merge schema"), "mergeSchema")\n                                .addField(\n                                SelectBox("Datetime Rebase Mode")\n                                    .addOption("EXCEPTION", "EXCEPTION")\n                                    .addOption("CORRECTED", "CORRECTED")\n                                    .addOption("LEGACY", "LEGACY"),\n                                "datetimeRebaseMode"\n                            )\n                                .addField(\n                                SelectBox("Int96 Rebase Mode")\n                                    .addOption("EXCEPTION", "EXCEPTION")\n                                    .addOption("CORRECTED", "CORRECTED")\n                                    .addOption("LEGACY", "LEGACY"),\n                                "int96RebaseMode"\n                            )\n                                .addField(Checkbox("Recursive File Lookup"), "recursiveFileLookup")\n                                .addField(TextBox("Path Global Filter").bindPlaceholder(""), "pathGlobFilter")\n                                .addField(TextBox("Modified Before").bindPlaceholder(""), "modifiedBefore")\n                                .addField(TextBox("Modified After").bindPlaceholder(""), "modifiedAfter")\n                        )\n                    )\n                ),\n                "auto"\n            )\n                .addColumn(SchemaTable("").bindProperty("schema"), "5fr")\n        ) \\\n            .addSection(\n            "PREVIEW",\n            PreviewTable("").bindProperty("schema")\n        )\n\n    def targetDialog(self) -> DatasetDialog:\n        return DatasetDialog("parquet") \\\n            .addSection("LOCATION", TargetLocation("path")) \\\n            .addSection(\n            "PROPERTIES",\n            ColumnsLayout(gap=("1rem"), height=("100%"))\n                .addColumn(\n                ScrollBox().addElement(\n                    StackLayout(height=("100%")).addElement(\n                        StackItem(grow=(1)).addElement(\n                            FieldPicker(height=("100%"))\n                                .addField(\n                                TextArea("Description", 2, placeholder="Dataset description..."),\n                                "description",\n                                True\n                            )\n                                .addField(\n                                SelectBox("Write Mode")\n                                    .addOption("error", "error")\n                                    .addOption("overwrite", "overwrite")\n                                    .addOption("append", "append")\n                                    .addOption("ignore", "ignore"),\n                                "writeMode"\n                            )\n                                .addField(\n                                SchemaColumnsDropdown("Partition Columns")\n                                    .withMultipleSelection()\n                                    .bindSchema("schema")\n                                    .showErrorsFor("partitionColumns"),\n                                "partitionColumns"\n                            )\n                                .addField(\n                                SelectBox("Compression Codec")\n                                    .addOption("none", "none")\n                                    .addOption("uncompressed", "uncompressed")\n                                    .addOption("gzip", "gzip")\n                                    .addOption("lz4", "lz4")\n                                    .addOption("snappy", "snappy")\n                                    .addOption("lzo", "lzo")\n                                    .addOption("brotli", "brotli")\n                                    .addOption("zstd", "zstd"),\n                                "compression"\n                            )\n                        )\n                    )\n                ),\n                "auto"\n            )\n                .addColumn(SchemaTable("").isReadOnly().withoutInferSchema().bindProperty("schema"), "5fr")\n        )\n\n    def validate(self, component: Component) -> list:\n        diagnostics = super(ParquetFormat, self).validate(component)\n        if len(component.properties.path) == 0:\n            diagnostics.append(\n                Diagnostic("properties.path", "path variable cannot be empty [Location]", SeverityLevelEnum.Error))\n        return diagnostics\n\n    def onChange(self, oldState: Component, newState: Component) -> Component:\n        return newState\n\n    class ParquetFormatCode(ComponentCode):\n        def __init__(self, props):\n            self.props: ParquetFormat.ParquetProperties = props\n\n        def sourceApply(self, spark: SparkSession) -> DataFrame:\n            reader = spark.read.format("parquet")\n            if self.props.mergeSchema is not None:\n                reader = reader.option("mergeSchema", self.props.mergeSchema)\n            if self.props.datetimeRebaseMode is not None:\n                reader = reader.option("datetimeRebaseMode", self.props.datetimeRebaseMode)\n            if self.props.int96RebaseMode is not None:\n                reader = reader.option("int96RebaseMode", self.props.int96RebaseMode)\n            if self.props.modifiedBefore is not None:\n                reader = reader.option("modifiedBefore", self.props.modifiedBefore)\n            if self.props.modifiedAfter is not None:\n                reader = reader.option("modifiedAfter", self.props.modifiedAfter)\n            if self.props.recursiveFileLookup is not None:\n                reader = reader.option("recursiveFileLookup", self.props.recursiveFileLookup)\n            if self.props.pathGlobFilter is not None:\n                reader = reader.option("pathGlobFilter", self.props.pathGlobFilter)\n\n            if self.props.schema is not None and self.props.useSchema:\n                reader = reader.schema(self.props.schema)\n\n            return reader.load(self.props.path)\n\n        def targetApply(self, spark: SparkSession, in0: DataFrame):\n            writer = in0.write.format("parquet")\n            if self.props.compression is not None:\n                writer = writer.option("compression", self.props.compression)\n\n            if self.props.writeMode is not None:\n                writer = writer.mode(self.props.writeMode)\n            if self.props.partitionColumns is not None and len(self.props.partitionColumns) > 0:\n                writer = writer.partitionBy(*self.props.partitionColumns)\n\n            writer.save(self.props.path)\n\n'))),(0,r.kt)(l.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'package io.prophecy.core.instructions.all.datasets\n\nimport io.prophecy.core.instructions.all._\nimport io.prophecy.core.instructions.spec._\nimport io.prophecy.core.program.WorkflowContext\nimport org.apache.spark.sql.{DataFrame, SparkSession}\nimport org.apache.spark.sql.types.StructType\nimport io.prophecy.libs._\n\nobject ParquetFormat extends DatasetSpec {\n\n  val name: String = "parquet"\n  val datasetType: String = "File"\n\n  type PropertiesType = ParquetProperties\n  case class ParquetProperties(\n    @Property("Schema")\n    schema: Option[StructType] = None,\n    @Property("Description")\n    description: Option[String] = Some(""),\n    @Property("useSchema")\n    useSchema: Option[Boolean] = Some(false),\n    @Property("Path")\n    path: String = "",\n    @Property(\n      "",\n      "(default is the value specified in spark.sql.parquet.mergeSchema(false)): sets whether we should merge schemas collected from all Parquet part-files. This will override spark.sql.parquet.mergeSchema."\n    )\n    mergeSchema: Option[Boolean] = None,\n    @Property(\n      "datetimeRebaseMode",\n      "The datetimeRebaseMode option allows to specify the rebasing mode for the values of the DATE, TIMESTAMP_MILLIS, TIMESTAMP_MICROS logical types from the Julian to Proleptic Gregorian calendar."\n    )\n    datetimeRebaseMode: Option[String] = None,\n    @Property(\n      "int96RebaseMode",\n      "The int96RebaseMode option allows to specify the rebasing mode for INT96 timestamps from the Julian to Proleptic Gregorian calendar."\n    )\n    int96RebaseMode: Option[String] = None,\n    @Property("compression", "(default: none) compression codec to use when saving to file.")\n    compression: Option[String] = None,\n    @Property("partitionColumns", "Partitioning column.")\n    partitionColumns: Option[List[String]] = None,\n    @Property("Write Mode", """(default: "error") Specifies the behavior when data or table already exists.""")\n    writeMode: Option[String] = None,\n    @Property(\n      "",\n      "an optional glob pattern to only include files with paths matching the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter. It does not change the behavior of partition discovery."\n    )\n    pathGlobFilter: Option[String] = None,\n    @Property(\n      "",\n      "(batch only): an optional timestamp to only include files with modification times occurring before the specified Time. The provided timestamp must be in the following form: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)"\n    )\n    modifiedBefore: Option[String] = None,\n    @Property(\n      "",\n      "(batch only): an optional timestamp to only include files with modification times occurring after the specified Time. The provided timestamp must be in the following form: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)"\n    )\n    modifiedAfter: Option[String] = None,\n    @Property("", "recursively scan a directory for files. Using this option disables partition discovery")\n    recursiveFileLookup: Option[Boolean] = None\n  ) extends DatasetProperties\n\n  def sourceDialog: DatasetDialog = DatasetDialog("parquet")\n    .addSection("LOCATION", TargetLocation("path"))\n    .addSection(\n      "PROPERTIES",\n      ColumnsLayout(gap = Some("1rem"), height = Some("100%"))\n        .addColumn(\n          ScrollBox().addElement(\n            StackLayout(height = Some("100%"))\n              .addElement(\n                StackItem(grow = Some(1)).addElement(\n                  FieldPicker(height = Some("100%"))\n                    .addField(\n                      TextArea("Description", 2, placeholder = "Dataset description..."),\n                      "description",\n                      true\n                    )\n                    .addField(Checkbox("Use user-defined schema"), "useSchema", true)\n                    .addField(Checkbox("Merge schema"), "mergeSchema")\n                    .addField(\n                      SelectBox("Datetime Rebase Mode")\n                        .addOption("EXCEPTION", "EXCEPTION")\n                        .addOption("CORRECTED", "CORRECTED")\n                        .addOption("LEGACY", "LEGACY"),\n                      "datetimeRebaseMode"\n                    )\n                    .addField(\n                      SelectBox("Int96 Rebase Mode")\n                        .addOption("EXCEPTION", "EXCEPTION")\n                        .addOption("CORRECTED", "CORRECTED")\n                        .addOption("LEGACY", "LEGACY"),\n                      "int96RebaseMode"\n                    )\n                    .addField(Checkbox("Recursive File Lookup"), "recursiveFileLookup")\n                    .addField(TextBox("Path Global Filter").bindPlaceholder(""), "pathGlobFilter")\n                    .addField(TextBox("Modified Before").bindPlaceholder(""), "modifiedBefore")\n                    .addField(TextBox("Modified After").bindPlaceholder(""), "modifiedAfter")\n                )\n              )\n          ),\n          "auto"\n        )\n        .addColumn(SchemaTable("").bindProperty("schema"), "5fr")\n    )\n    .addSection(\n      "PREVIEW",\n      PreviewTable("").bindProperty("schema")\n    )\n\n  def targetDialog: DatasetDialog = DatasetDialog("parquet")\n    .addSection("LOCATION", TargetLocation("path"))\n    .addSection(\n      "PROPERTIES",\n      ColumnsLayout(gap = Some("1rem"), height = Some("100%"))\n        .addColumn(\n          ScrollBox().addElement(\n            StackLayout(height = Some("100%")).addElement(\n              StackItem(grow = Some(1)).addElement(\n                FieldPicker(height = Some("100%"))\n                  .addField(\n                    TextArea("Description", 2, placeholder = "Dataset description..."),\n                    "description",\n                    true\n                  )\n                  .addField(\n                    SelectBox("Write Mode")\n                      .addOption("error", "error")\n                      .addOption("overwrite", "overwrite")\n                      .addOption("append", "append")\n                      .addOption("ignore", "ignore"),\n                    "writeMode"\n                  )\n                  .addField(\n                    SchemaColumnsDropdown("Partition Columns")\n                      .withMultipleSelection()\n                      .bindSchema("schema")\n                      .showErrorsFor("partitionColumns"),\n                    "partitionColumns"\n                  )\n                  .addField(\n                    SelectBox("Compression Codec")\n                      .addOption("none", "none")\n                      .addOption("uncompressed", "uncompressed")\n                      .addOption("gzip", "gzip")\n                      .addOption("lz4", "lz4")\n                      .addOption("snappy", "snappy")\n                      .addOption("lzo", "lzo")\n                      .addOption("brotli", "brotli")\n                      .addOption("zstd", "zstd"),\n                    "compression"\n                  )\n              )\n            )\n          ),\n          "auto"\n        )\n        .addColumn(SchemaTable("").isReadOnly().withoutInferSchema().bindProperty("schema"), "5fr")\n    )\n\n  override def validate(component: Component)(implicit context: WorkflowContext): List[Diagnostic] = {\n    import scala.collection.mutable.ListBuffer\n    val diagnostics = ListBuffer[Diagnostic]()\n    diagnostics ++= super.validate(component)\n\n    if (component.properties.path.isEmpty) {\n      diagnostics += Diagnostic("properties.path", "path variable cannot be empty [Location]", SeverityLevel.Error)\n    }\n    if (component.properties.schema.isEmpty) {\n      // diagnostics += Diagnostic("properties.schema", "Schema cannot be empty [Properties]", SeverityLevel.Error)\n    }\n\n    diagnostics.toList\n  }\n\n  def onChange(oldState: Component, newState: Component)(implicit context: WorkflowContext): Component = newState\n\n  class ParquetFormatCode(props: ParquetProperties) extends ComponentCode {\n\n    def sourceApply(spark: SparkSession): DataFrame = {\n      var reader = spark.read\n        .format("parquet")\n        .option("mergeSchema", props.mergeSchema)\n        .option("datetimeRebaseMode", props.datetimeRebaseMode)\n        .option("int96RebaseMode", props.int96RebaseMode)\n        .option("modifiedBefore", props.modifiedBefore)\n        .option("modifiedAfter", props.modifiedAfter)\n        .option("recursiveFileLookup", props.recursiveFileLookup)\n        .option("pathGlobFilter", props.pathGlobFilter)\n\n      if (props.useSchema.isDefined && props.useSchema.get)\n        props.schema.foreach(schema \u21d2 reader = reader.schema(schema))\n\n      reader.load(props.path)\n    }\n\n    def targetApply(spark: SparkSession, in: DataFrame): Unit = {\n      var writer = in.write\n        .format("parquet")\n        .option("compression", props.compression)\n\n      props.writeMode.foreach { mode \u21d2\n        writer = writer.mode(mode)\n      }\n      props.partitionColumns.foreach(pcols \u21d2\n        writer = pcols match {\n          case Nil \u21d2 writer\n          case _ \u21d2 writer.partitionBy(pcols: _*)\n        }\n      )\n      writer.save(props.path)\n    }\n\n  }\n\n}\n\n\n')))),(0,r.kt)("p",null,"Here you can see that the differences between a Transform Gem and a DataSource Gem is"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"The Source/Target Gem extends ",(0,r.kt)("inlineCode",{parentName:"li"},"DatasetSpec")),(0,r.kt)("li",{parentName:"ol"},"It has two Dialog functions: ",(0,r.kt)("inlineCode",{parentName:"li"},"sourceDialog")," and ",(0,r.kt)("inlineCode",{parentName:"li"},"targetDialog"),". They return both a ",(0,r.kt)("inlineCode",{parentName:"li"},"DatasetDialog")," object, whereas for any Transform Gem, the dialog function returns a ",(0,r.kt)("inlineCode",{parentName:"li"},"Dialog")," object."),(0,r.kt)("li",{parentName:"ol"},"The ",(0,r.kt)("inlineCode",{parentName:"li"},"ComponentCode")," class has two apply functions: ",(0,r.kt)("inlineCode",{parentName:"li"},"sourceApply")," and ",(0,r.kt)("inlineCode",{parentName:"li"},"targetApply")," for Source and Target modes respectively.")),(0,r.kt)("p",null,"There is no change in ",(0,r.kt)("inlineCode",{parentName:"p"},"onChange")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"validate")," functions."))}f.isMDXComponent=!0},18968:function(e,n,t){n.Z=t.p+"assets/images/gem-builder-ui-b0bd1d841891aeb22ddef5398815b8f0.png"}}]);