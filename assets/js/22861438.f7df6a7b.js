"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[9407],{15680:(e,t,a)=>{a.d(t,{xA:()=>p,yg:()=>m});var r=a(96540);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},l=Object.keys(e);for(r=0;r<l.length;r++)a=l[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(r=0;r<l.length;r++)a=l[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var s=r.createContext({}),u=function(e){var t=r.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=u(e.components);return r.createElement(s.Provider,{value:t},e.children)},d="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},g=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,l=e.originalType,s=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),d=u(a),g=n,m=d["".concat(s,".").concat(g)]||d[g]||c[g]||l;return a?r.createElement(m,o(o({ref:t},p),{},{components:a})):r.createElement(m,o({ref:t},p))}));function m(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var l=a.length,o=new Array(l);o[0]=g;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i[d]="string"==typeof e?e:n,o[1]=i;for(var u=2;u<l;u++)o[u]=a[u];return r.createElement.apply(null,o)}return r.createElement.apply(null,a)}g.displayName="MDXCreateElement"},19365:(e,t,a)=>{a.d(t,{A:()=>o});var r=a(96540),n=a(20053);const l={tabItem:"tabItem_Ymn6"};function o(e){let{children:t,hidden:a,className:o}=e;return r.createElement("div",{role:"tabpanel",className:(0,n.A)(l.tabItem,o),hidden:a},t)}},11470:(e,t,a)=>{a.d(t,{A:()=>S});var r=a(58168),n=a(96540),l=a(20053),o=a(23104),i=a(56347),s=a(57485),u=a(31682),p=a(89466);function d(e){return function(e){return n.Children.map(e,(e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:a,attributes:r,default:n}}=e;return{value:t,label:a,attributes:r,default:n}}))}function c(e){const{values:t,children:a}=e;return(0,n.useMemo)((()=>{const e=t??d(a);return function(e){const t=(0,u.X)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function g(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function m(e){let{queryString:t=!1,groupId:a}=e;const r=(0,i.W6)(),l=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,s.aZ)(l),(0,n.useCallback)((e=>{if(!l)return;const t=new URLSearchParams(r.location.search);t.set(l,e),r.replace({...r.location,search:t.toString()})}),[l,r])]}function y(e){const{defaultValue:t,queryString:a=!1,groupId:r}=e,l=c(e),[o,i]=(0,n.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!g({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const r=a.find((e=>e.default))??a[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:t,tabValues:l}))),[s,u]=m({queryString:a,groupId:r}),[d,y]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[r,l]=(0,p.Dv)(a);return[r,(0,n.useCallback)((e=>{a&&l.set(e)}),[a,l])]}({groupId:r}),f=(()=>{const e=s??d;return g({value:e,tabValues:l})?e:null})();(0,n.useLayoutEffect)((()=>{f&&i(f)}),[f]);return{selectedValue:o,selectValue:(0,n.useCallback)((e=>{if(!g({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);i(e),u(e),y(e)}),[u,y,l]),tabValues:l}}var f=a(92303);const h={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function b(e){let{className:t,block:a,selectedValue:i,selectValue:s,tabValues:u}=e;const p=[],{blockElementScrollPositionUntilNextRender:d}=(0,o.a_)(),c=e=>{const t=e.currentTarget,a=p.indexOf(t),r=u[a].value;r!==i&&(d(t),s(r))},g=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const a=p.indexOf(e.currentTarget)+1;t=p[a]??p[0];break}case"ArrowLeft":{const a=p.indexOf(e.currentTarget)-1;t=p[a]??p[p.length-1];break}}t?.focus()};return n.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,l.A)("tabs",{"tabs--block":a},t)},u.map((e=>{let{value:t,label:a,attributes:o}=e;return n.createElement("li",(0,r.A)({role:"tab",tabIndex:i===t?0:-1,"aria-selected":i===t,key:t,ref:e=>p.push(e),onKeyDown:g,onClick:c},o,{className:(0,l.A)("tabs__item",h.tabItem,o?.className,{"tabs__item--active":i===t})}),a??t)})))}function v(e){let{lazy:t,children:a,selectedValue:r}=e;const l=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=l.find((e=>e.props.value===r));return e?(0,n.cloneElement)(e,{className:"margin-top--md"}):null}return n.createElement("div",{className:"margin-top--md"},l.map(((e,t)=>(0,n.cloneElement)(e,{key:t,hidden:e.props.value!==r}))))}function N(e){const t=y(e);return n.createElement("div",{className:(0,l.A)("tabs-container",h.tabList)},n.createElement(b,(0,r.A)({},e,t)),n.createElement(v,(0,r.A)({},e,t)))}function S(e){const t=(0,f.A)();return n.createElement(N,(0,r.A)({key:String(t)},e))}},15479:(e,t,a)=>{a.d(t,{A:()=>s});var r=a(96540),n=a(86025);const l=e=>{let{children:t}=e;return r.createElement("div",{style:{position:"relative",display:"flex","justify-content":"center","align-items":"center"}},t)},o=e=>{let{source:t,children:a}=e;return r.createElement("img",{src:(0,n.A)(t),style:{"object-fit":"cover"}})},i=e=>{let{slides:t}=e;const[a,n]=(0,r.useState)(0);return r.createElement(l,null,r.createElement("i",{class:"fa fa-chevron-left",onClick:()=>{n(0===a?t.length-1:a-1)},style:{position:"absolute",top:"50%",left:"0px","font-size":"2rem"}}),r.createElement("i",{class:"fa fa-chevron-right",onClick:()=>{n(a===t.length-1?0:a+1)},style:{position:"absolute",top:"50%",right:"0px","font-size":"2rem"}}),r.createElement("div",{style:{padding:"30px"}},r.createElement(o,{source:t[a].image}),t[a].description))};function s(e){let{ImageData:t}=e;return r.createElement(i,{slides:t,style:{"font-family":" sans-serif","text-align":"center"}})}},88490:(e,t,a)=>{a.r(t),a.d(t,{ImageData:()=>c,ImageData2:()=>m,assets:()=>d,contentTitle:()=>u,default:()=>h,frontMatter:()=>s,metadata:()=>p,toc:()=>g});var r=a(58168),n=(a(96540),a(15680)),l=a(15479),o=a(11470),i=a(19365);const s={title:"CSV",id:"csv",description:"CSV",sidebar_position:1,tags:["gems","file","csv"]},u=void 0,p={unversionedId:"Spark/gems/source-target/file/csv",id:"Spark/gems/source-target/file/csv",title:"CSV",description:"CSV",source:"@site/docs/Spark/gems/source-target/file/csv.md",sourceDirName:"Spark/gems/source-target/file",slug:"/Spark/gems/source-target/file/csv",permalink:"/Spark/gems/source-target/file/csv",draft:!1,tags:[{label:"gems",permalink:"/tags/gems"},{label:"file",permalink:"/tags/file"},{label:"csv",permalink:"/tags/csv"}],version:"current",sidebarPosition:1,frontMatter:{title:"CSV",id:"csv",description:"CSV",sidebar_position:1,tags:["gems","file","csv"]},sidebar:"defaultSidebar",previous:{title:"Source & Target",permalink:"/Spark/gems/source-target/"},next:{title:"FTP",permalink:"/Spark/gems/source-target/file/ftp"}},d={},c=[{image:"/img/csv/load/1.png",description:(0,n.yg)("h3",{style:{padding:"10px"}},"Step 1 - Create Source Component")},{image:"/img/csv/load/2.png",description:(0,n.yg)("h3",{style:{padding:"10px"}},"Step 2 - Click 'Create Dataset'")},{image:"/img/csv/load/3.png",description:(0,n.yg)("h3",{style:{padding:"10px"}}," Step 3 - Enter 'Dataset Name' and select the CSV format")},{image:"/img/csv/load/4.png",description:(0,n.yg)("h3",{style:{padding:"10px"}},"Step 4 - Navigate to the desired CSV source file")},{image:"/img/csv/load/5.png",description:(0,n.yg)("h3",{style:{padding:"10px"}},"Step 5 - Customize the properties and update schema as per your needs")},{image:"/img/csv/load/6.png",description:(0,n.yg)("h3",{style:{padding:"10px"}},"Step 6 - Hit 'Refresh' to preview data ")}],g=[{value:"Source",id:"source",level:2},{value:"Source Parameters",id:"source-parameters",level:3},{value:"Example",id:"source-example",level:3},{value:"Generated Code",id:"source-code",level:3},{value:"Target",id:"target",level:2},{value:"Target Parameters",id:"target-parameters",level:3},{value:"Supported Write Modes",id:"supported-write-modes",level:3},{value:"Example",id:"target-example",level:3},{value:"Generated Code",id:"target-code",level:3},{value:"Producing a single output file",id:"producing-a-single-output-file",level:3}],m=[{image:"/img/csv/write/1.png",description:(0,n.yg)("h3",{style:{padding:"10px"}},"Step 1 - Create Target Component")},{image:"/img/csv/write/2.png",description:(0,n.yg)("h3",{style:{padding:"10px"}},"Step 2 - Click 'Create Dataset'")},{image:"/img/csv/write/3.png",description:(0,n.yg)("h3",{style:{padding:"10px"}}," Step 3 - Enter 'Dataset Name' and select the CSV format")},{image:"/img/csv/write/4.png",description:(0,n.yg)("h3",{style:{padding:"10px"}},"Step 4 - Navigate to the desired CSV target location")},{image:"/img/csv/write/5.png",description:(0,n.yg)("h3",{style:{padding:"10px"}},"Step 5 - Customize the properties as per your needs")}],y={ImageData:c,toc:g,ImageData2:m},f="wrapper";function h(e){let{components:t,...s}=e;return(0,n.yg)(f,(0,r.A)({},y,s,{components:t,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"Allows you to read or write delimited files such as CSV (Comma-separated Values) or TSV (Tab-separated Values)"),(0,n.yg)("h2",{id:"source"},"Source"),(0,n.yg)("h3",{id:"source-parameters"},"Source Parameters"),(0,n.yg)("p",null,"CSV ",(0,n.yg)("strong",{parentName:"p"},(0,n.yg)("em",{parentName:"strong"},"Source"))," supports all the available ",(0,n.yg)("a",{parentName:"p",href:"https://spark.apache.org/docs/latest/sql-data-sources-csv.html"},"Spark read options for CSV"),"."),(0,n.yg)("p",null,"The below list contains the additional parameters to read a CSV file:"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Parameter"),(0,n.yg)("th",{parentName:"tr",align:null}),(0,n.yg)("th",{parentName:"tr",align:null},"Description"),(0,n.yg)("th",{parentName:"tr",align:null},"Required"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Dataset Name"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"Name of the Dataset"),(0,n.yg)("td",{parentName:"tr",align:null},"True")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Location"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"Location of the file(s) to be loaded ",(0,n.yg)("br",null)," Eg: ",(0,n.yg)("inlineCode",{parentName:"td"},"dbfs:/data/test.csv")),(0,n.yg)("td",{parentName:"tr",align:null},"True")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Schema"),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"Schema to applied on the loaded data. Can be defined/edited as JSON or inferred using ",(0,n.yg)("inlineCode",{parentName:"td"},"Infer Schema")," button"),(0,n.yg)("td",{parentName:"tr",align:null},"True")))),(0,n.yg)("h3",{id:"source-example"},"Example"),(0,n.yg)(l.A,{ImageData:c,mdxType:"App"}),(0,n.yg)("h3",{id:"source-code"},"Generated Code"),(0,n.yg)(o.A,{mdxType:"Tabs"},(0,n.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-py"},'def load_csv(spark: SparkSession) -> DataFrame:\n    return spark.read\\\n        .schema(\n          StructType([\n            StructField("order_id", IntegerType(), True),\n            StructField("customer_id", IntegerType(), True),\n            StructField("order_status", StringType(), True),\n            StructField("order_category", StringType(), True),\n            StructField("order_date", DateType(), True),\n            StructField("amount", DoubleType(), True)\n        ])\n        )\\\n        .option("header", True)\\\n        .option("quote", "\\"")\\\n        .option("sep", ",")\\\n        .csv("dbfs:/Prophecy/anshuman@simpledatalabs.com/OrdersDatasetInput.csv")\n\n'))),(0,n.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'object load_csv {\n\n  def apply(spark: SparkSession): DataFrame =\n    spark.read\n      .format("csv")\n      .option("header", true)\n      .option("quote",  "\\"")\n      .option("sep",    ",")\n      .schema(\n        StructType(\n          Array(\n            StructField("order_id",       IntegerType, true),\n            StructField("customer_id",    IntegerType, true),\n            StructField("order_status",   StringType,  true),\n            StructField("order_category", StringType,  true),\n            StructField("order_date",     DateType,    true),\n            StructField("amount",         DoubleType,  true)\n          )\n        )\n      )\n      .load("dbfs:/Prophecy/anshuman@simpledatalabs.com/OrdersDatasetInput.csv")\n\n}\n')))),(0,n.yg)("hr",null),(0,n.yg)("h2",{id:"target"},"Target"),(0,n.yg)("h3",{id:"target-parameters"},"Target Parameters"),(0,n.yg)("p",null,"CSV ",(0,n.yg)("strong",{parentName:"p"},(0,n.yg)("em",{parentName:"strong"},"Target"))," supports all the available ",(0,n.yg)("a",{parentName:"p",href:"https://spark.apache.org/docs/latest/sql-data-sources-csv.html"},"Spark write options for CSV"),"."),(0,n.yg)("p",null,"The below list contains the additional parameters to write a CSV file:"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Parameter"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"),(0,n.yg)("th",{parentName:"tr",align:null},"Required"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Dataset Name"),(0,n.yg)("td",{parentName:"tr",align:null},"Name of the Dataset"),(0,n.yg)("td",{parentName:"tr",align:null},"True")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Location"),(0,n.yg)("td",{parentName:"tr",align:null},"Location of the file(s) to be loaded ",(0,n.yg)("br",null)," Eg: ",(0,n.yg)("inlineCode",{parentName:"td"},"dbfs:/data/output.csv")),(0,n.yg)("td",{parentName:"tr",align:null},"True")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Write Mode"),(0,n.yg)("td",{parentName:"tr",align:null},"How to handle existing data. See ",(0,n.yg)("a",{parentName:"td",href:"#supported-write-modes"},"this table")," for a list of available options."),(0,n.yg)("td",{parentName:"tr",align:null},"False")))),(0,n.yg)("h3",{id:"supported-write-modes"},"Supported Write Modes"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Write Mode"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"overwrite"),(0,n.yg)("td",{parentName:"tr",align:null},"If data already exists, overwrite with the contents of the DataFrame")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"append"),(0,n.yg)("td",{parentName:"tr",align:null},"If data already exists, append the contents of the DataFrame")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"ignore"),(0,n.yg)("td",{parentName:"tr",align:null},"If data already exists, do nothing with the contents of the DataFrame. This is similar to a ",(0,n.yg)("inlineCode",{parentName:"td"},"CREATE TABLE IF NOT EXISTS")," in SQL.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"error"),(0,n.yg)("td",{parentName:"tr",align:null},"If data already exists, throw an exception.")))),(0,n.yg)("h3",{id:"target-example"},"Example"),(0,n.yg)(l.A,{ImageData:m,mdxType:"App"}),(0,n.yg)("h3",{id:"target-code"},"Generated Code"),(0,n.yg)(o.A,{mdxType:"Tabs"},(0,n.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-py"},'def write_as_csv(spark: SparkSession, in0: DataFrame):\n    in0.write\\\n        .option("header", True)\\\n        .option("sep", ",")\\\n        .mode("error")\\\n        .option("separator", ",")\\\n        .option("header", True)\\\n        .csv("dbfs:/Prophecy/anshuman@simpledatalabs.com/output.csv")\n'))),(0,n.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'object write_as_csv {\n  def apply(spark: SparkSession, in: DataFrame): Unit =\n    in.write\n      .format("csv")\n      .option("header", true)\n      .option("sep",    ",")\n      .mode("error")\n      .save("dbfs:/Prophecy/anshuman@simpledatalabs.com/output.csv")\n}\n')))),(0,n.yg)("h3",{id:"producing-a-single-output-file"},"Producing a single output file"),(0,n.yg)("p",null,"Because of Spark's distributed nature, output files are written as multiple separate partition files. If you need a single output file for some reason (such as reporting or exporting to an external system), use a ",(0,n.yg)("inlineCode",{parentName:"p"},"Repartition")," Gem in ",(0,n.yg)("inlineCode",{parentName:"p"},"Coalesce")," mode with 1 output partition:"),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Coalesce example",src:a(64978).A,width:"1172",height:"582"})),(0,n.yg)("admonition",{type:"caution"},(0,n.yg)("p",{parentName:"admonition"},"Note: This is not recommended for extremely large data sets as it may overwhelm the worker node writing the file.")))}h.isMDXComponent=!0},64978:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/coalesce-d33a29eb3a5d7288134b79aea66bf549.gif"}}]);