"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[42685],{15680:(e,t,a)=>{a.d(t,{xA:()=>d,yg:()=>y});var r=a(96540);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},l=Object.keys(e);for(r=0;r<l.length;r++)a=l[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(r=0;r<l.length;r++)a=l[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var s=r.createContext({}),p=function(e){var t=r.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},d=function(e){var t=p(e.components);return r.createElement(s.Provider,{value:t},e.children)},u="mdxType",g={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,l=e.originalType,s=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),u=p(a),m=n,y=u["".concat(s,".").concat(m)]||u[m]||g[m]||l;return a?r.createElement(y,i(i({ref:t},d),{},{components:a})):r.createElement(y,i({ref:t},d))}));function y(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var l=a.length,i=new Array(l);i[0]=m;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[u]="string"==typeof e?e:n,i[1]=o;for(var p=2;p<l;p++)i[p]=a[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,a)}m.displayName="MDXCreateElement"},19365:(e,t,a)=>{a.d(t,{A:()=>i});var r=a(96540),n=a(20053);const l={tabItem:"tabItem_Ymn6"};function i(e){let{children:t,hidden:a,className:i}=e;return r.createElement("div",{role:"tabpanel",className:(0,n.A)(l.tabItem,i),hidden:a},t)}},11470:(e,t,a)=>{a.d(t,{A:()=>w});var r=a(58168),n=a(96540),l=a(20053),i=a(23104),o=a(56347),s=a(57485),p=a(31682),d=a(89466);function u(e){return function(e){return n.Children.map(e,(e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:a,attributes:r,default:n}}=e;return{value:t,label:a,attributes:r,default:n}}))}function g(e){const{values:t,children:a}=e;return(0,n.useMemo)((()=>{const e=t??u(a);return function(e){const t=(0,p.X)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function m(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function y(e){let{queryString:t=!1,groupId:a}=e;const r=(0,o.W6)(),l=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,s.aZ)(l),(0,n.useCallback)((e=>{if(!l)return;const t=new URLSearchParams(r.location.search);t.set(l,e),r.replace({...r.location,search:t.toString()})}),[l,r])]}function c(e){const{defaultValue:t,queryString:a=!1,groupId:r}=e,l=g(e),[i,o]=(0,n.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!m({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const r=a.find((e=>e.default))??a[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:t,tabValues:l}))),[s,p]=y({queryString:a,groupId:r}),[u,c]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[r,l]=(0,d.Dv)(a);return[r,(0,n.useCallback)((e=>{a&&l.set(e)}),[a,l])]}({groupId:r}),h=(()=>{const e=s??u;return m({value:e,tabValues:l})?e:null})();(0,n.useLayoutEffect)((()=>{h&&o(h)}),[h]);return{selectedValue:i,selectValue:(0,n.useCallback)((e=>{if(!m({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);o(e),p(e),c(e)}),[p,c,l]),tabValues:l}}var h=a(92303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function b(e){let{className:t,block:a,selectedValue:o,selectValue:s,tabValues:p}=e;const d=[],{blockElementScrollPositionUntilNextRender:u}=(0,i.a_)(),g=e=>{const t=e.currentTarget,a=d.indexOf(t),r=p[a].value;r!==o&&(u(t),s(r))},m=e=>{let t=null;switch(e.key){case"Enter":g(e);break;case"ArrowRight":{const a=d.indexOf(e.currentTarget)+1;t=d[a]??d[0];break}case"ArrowLeft":{const a=d.indexOf(e.currentTarget)-1;t=d[a]??d[d.length-1];break}}t?.focus()};return n.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,l.A)("tabs",{"tabs--block":a},t)},p.map((e=>{let{value:t,label:a,attributes:i}=e;return n.createElement("li",(0,r.A)({role:"tab",tabIndex:o===t?0:-1,"aria-selected":o===t,key:t,ref:e=>d.push(e),onKeyDown:m,onClick:g},i,{className:(0,l.A)("tabs__item",f.tabItem,i?.className,{"tabs__item--active":o===t})}),a??t)})))}function N(e){let{lazy:t,children:a,selectedValue:r}=e;const l=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=l.find((e=>e.props.value===r));return e?(0,n.cloneElement)(e,{className:"margin-top--md"}):null}return n.createElement("div",{className:"margin-top--md"},l.map(((e,t)=>(0,n.cloneElement)(e,{key:t,hidden:e.props.value!==r}))))}function v(e){const t=c(e);return n.createElement("div",{className:(0,l.A)("tabs-container",f.tabList)},n.createElement(b,(0,r.A)({},e,t)),n.createElement(N,(0,r.A)({},e,t)))}function w(e){const t=(0,h.A)();return n.createElement(v,(0,r.A)({key:String(t)},e))}},37102:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>s,default:()=>y,frontMatter:()=>o,metadata:()=>p,toc:()=>u});var r=a(58168),n=(a(96540),a(15680)),l=a(11470),i=a(19365);const o={title:"Parquet",id:"parquet",description:"Parameters and properties to read from and write to Parquet files",tags:["gems","file","parquet"]},s=void 0,p={unversionedId:"Spark/gems/source-target/file/parquet",id:"Spark/gems/source-target/file/parquet",title:"Parquet",description:"Parameters and properties to read from and write to Parquet files",source:"@site/docs/Spark/gems/source-target/file/parquet.md",sourceDirName:"Spark/gems/source-target/file",slug:"/Spark/gems/source-target/file/parquet",permalink:"/Spark/gems/source-target/file/parquet",draft:!1,tags:[{label:"gems",permalink:"/tags/gems"},{label:"file",permalink:"/tags/file"},{label:"parquet",permalink:"/tags/parquet"}],version:"current",frontMatter:{title:"Parquet",id:"parquet",description:"Parameters and properties to read from and write to Parquet files",tags:["gems","file","parquet"]},sidebar:"mySidebar",previous:{title:"ORC",permalink:"/Spark/gems/source-target/file/orc"},next:{title:"Seed",permalink:"/Spark/gems/source-target/file/seed"}},d={},u=[{value:"Parameters",id:"parameters",level:2},{value:"Source",id:"source",level:2},{value:"Source properties",id:"source-properties",level:3},{value:"Supported Int96 rebase modes",id:"supported-int96-rebase-modes",level:3},{value:"Supported Datetime rebase modes",id:"supported-datetime-rebase-modes",level:3},{value:"Example",id:"source-example",level:3},{value:"Generated Code",id:"source-code",level:3},{value:"Target",id:"target",level:2},{value:"Target properties",id:"target-properties",level:3},{value:"Supported write modes",id:"supported-write-modes",level:3},{value:"Example",id:"target",level:3},{value:"Generated Code",id:"target-code",level:3}],g={toc:u},m="wrapper";function y(e){let{components:t,...a}=e;return(0,n.yg)(m,(0,r.A)({},g,a,{components:t,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"The Parquet file type:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Is an open-source columnar file format designed for efficient data storage and retrieval."),(0,n.yg)("li",{parentName:"ul"},"Handles large volumes of data by supporting complex predicate pushdown, nested schemas, and a wide variety of column encoding types.")),(0,n.yg)("h2",{id:"parameters"},"Parameters"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Parameter"),(0,n.yg)("th",{parentName:"tr",align:null},"Tab"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Location"),(0,n.yg)("td",{parentName:"tr",align:null},"Location"),(0,n.yg)("td",{parentName:"tr",align:null},"File path to read from or write to the Parquet file.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Schema"),(0,n.yg)("td",{parentName:"tr",align:null},"Properties"),(0,n.yg)("td",{parentName:"tr",align:null},"Schema to apply on the loaded data.",(0,n.yg)("br",null),"In the Source gem, you can define or edit the schema visually or in JSON code.",(0,n.yg)("br",null),"In the Target gem, you can view the schema visually or as JSON code.")))),(0,n.yg)("h2",{id:"source"},"Source"),(0,n.yg)("p",null,"The Source gem reads data from Parquet files and allows you to optionally specify the following additional properties."),(0,n.yg)("h3",{id:"source-properties"},"Source properties"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Property name"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"),(0,n.yg)("th",{parentName:"tr",align:null},"Default"),(0,n.yg)("th",{parentName:"tr",align:null}))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Description"),(0,n.yg)("td",{parentName:"tr",align:null},"Description of your dataset."),(0,n.yg)("td",{parentName:"tr",align:null},"None"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Use user-defined schema"),(0,n.yg)("td",{parentName:"tr",align:null},"Whether to use the schema you define."),(0,n.yg)("td",{parentName:"tr",align:null},"false"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Merge schema"),(0,n.yg)("td",{parentName:"tr",align:null},"Whether the Target gem should merge schemas from all the Parquet part-files collected. This overrides ",(0,n.yg)("inlineCode",{parentName:"td"},"spark.sql.parquet.mergeSchema"),"."),(0,n.yg)("td",{parentName:"tr",align:null},"(value of ",(0,n.yg)("inlineCode",{parentName:"td"},"spark.sql."),(0,n.yg)("br",null),(0,n.yg)("inlineCode",{parentName:"td"},"parquet."),(0,n.yg)("br",null),(0,n.yg)("inlineCode",{parentName:"td"},"mergeSchema"),")"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Datetime Rebase Mode"),(0,n.yg)("td",{parentName:"tr",align:null},"Specify the rebasing mode for the values of the DATE, TIMESTAMP_MILLIS, TIMESTAMP_MICROS logical types from the Julian to Proleptic Gregorian calendar. ",(0,n.yg)("br",null),"For a list of the possible values, see ",(0,n.yg)("a",{parentName:"td",href:"#supported-datetime-rebase-modes"},"Supported Datetime rebase modes"),"."),(0,n.yg)("td",{parentName:"tr",align:null},"(value of ",(0,n.yg)("inlineCode",{parentName:"td"},"spark.sql."),(0,n.yg)("br",null),(0,n.yg)("inlineCode",{parentName:"td"},"parquet"),(0,n.yg)("br",null),(0,n.yg)("inlineCode",{parentName:"td"},".datetimeRebaseModeInRead"),")"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Int96 Rebase Mode"),(0,n.yg)("td",{parentName:"tr",align:null},"Specify the rebasing mode for INT96 timestamps from the Julian to Proleptic Gregorian calendar. For a list of the possible values, see ",(0,n.yg)("a",{parentName:"td",href:"#supported-int96-rebase-modes"},"Supported Int96 rebase modes"),"."),(0,n.yg)("td",{parentName:"tr",align:null},"(value of ",(0,n.yg)("inlineCode",{parentName:"td"},"spark.sql."),(0,n.yg)("br",null),(0,n.yg)("inlineCode",{parentName:"td"},"parquet"),(0,n.yg)("br",null),(0,n.yg)("inlineCode",{parentName:"td"},".int96RebaseModeInRead"),")"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Recursive File Lookup"),(0,n.yg)("td",{parentName:"tr",align:null},"Whether to recursively load files and disable partition inferring. If the data source explicitly specifies the ",(0,n.yg)("inlineCode",{parentName:"td"},"partitionSpec")," when the",(0,n.yg)("inlineCode",{parentName:"td"},"recursiveFileLookup")," is ",(0,n.yg)("inlineCode",{parentName:"td"},"true"),", the Source gem throws an exception."),(0,n.yg)("td",{parentName:"tr",align:null},"false"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Path Global Filter"),(0,n.yg)("td",{parentName:"tr",align:null},"Glob pattern to only include files with paths matching the pattern. The syntax follows ",(0,n.yg)("a",{parentName:"td",href:"https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/GlobFilter.html"},"GlobFilter")," and does not change the behavior of partition discovery."),(0,n.yg)("td",{parentName:"tr",align:null},"None"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Modified Before"),(0,n.yg)("td",{parentName:"tr",align:null},"Timestamp to only include files with modification times occurring before the time you specify. The timestamp must be in the following form: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)."),(0,n.yg)("td",{parentName:"tr",align:null},"None"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Modified After"),(0,n.yg)("td",{parentName:"tr",align:null},"Timestamp to only include files with modification times occurring after the time you specify. The timestamp must be in the following form: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)."),(0,n.yg)("td",{parentName:"tr",align:null},"None"),(0,n.yg)("td",{parentName:"tr",align:null})))),(0,n.yg)("h3",{id:"supported-int96-rebase-modes"},"Supported Int96 rebase modes"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Int96 rebase mode"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"EXCEPTION"),(0,n.yg)("td",{parentName:"tr",align:null},"Fails in reads of ancient INT96 timestamps that are ambiguous between two calendars.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"CORRECTED"),(0,n.yg)("td",{parentName:"tr",align:null},"Loads INT96 timestamps without rebasing.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"LEGACY"),(0,n.yg)("td",{parentName:"tr",align:null},"Rebases ancient INT96 timestamps from the Julian to Proleptic Gregorian.")))),(0,n.yg)("h3",{id:"supported-datetime-rebase-modes"},"Supported Datetime rebase modes"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Datetime rebase mode"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"EXCEPTION"),(0,n.yg)("td",{parentName:"tr",align:null},"Fails in reads of ancient dates and timestamps that are ambiguous between two calendars.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"CORRECTED"),(0,n.yg)("td",{parentName:"tr",align:null},"Loads dates and timestamps without rebasing.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"LEGACY"),(0,n.yg)("td",{parentName:"tr",align:null},"Rebases ancient dates and timestamps from the Julian to Proleptic Gregorian.")))),(0,n.yg)("h3",{id:"source-example"},"Example"),(0,n.yg)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,n.yg)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,n.yg)("iframe",{src:"https://user-images.githubusercontent.com/103921419/175030738-4c53b5c9-73e7-46c7-9fdc-c49048f78572.mp4",title:"Parquet Source",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,n.yg)("h3",{id:"source-code"},"Generated Code"),(0,n.yg)("admonition",{type:"tip"},(0,n.yg)("p",{parentName:"admonition"},"To see the generated source code, ",(0,n.yg)("a",{parentName:"p",href:"/getting-started/tutorials/spark-with-databricks#review-the-code"},"switch to the Code view")," at the top of the page.")),(0,n.yg)(l.A,{mdxType:"Tabs"},(0,n.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-py"},'def read_parquet(spark: SparkSession) -> DataFrame:\n    return spark.read\\\n        .format("parquet")\\\n        .option("mergeSchema", True)\\\n        .load("dbfs:/FileStore/Users/parquet/test.parquet")\n'))),(0,n.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'object read_parquet {\n\n  def apply(spark: SparkSession): DataFrame =\n    spark.read\n        .format("parquet")\n        .option("mergeSchema", true)\n        .load("dbfs:/FileStore/Users/parquet/test.parquet")\n\n}\n')))),(0,n.yg)("hr",null),(0,n.yg)("h2",{id:"target"},"Target"),(0,n.yg)("p",null,"The Target gem writes data to Parquet files and allows you to optionally specify the following additional properties."),(0,n.yg)("h3",{id:"target-properties"},"Target properties"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Property name"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"),(0,n.yg)("th",{parentName:"tr",align:null},"Default"),(0,n.yg)("th",{parentName:"tr",align:null}))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Description"),(0,n.yg)("td",{parentName:"tr",align:null},"Description of your dataset."),(0,n.yg)("td",{parentName:"tr",align:null},"None"),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Partition Columns"),(0,n.yg)("td",{parentName:"tr",align:null},"List of columns to partition the Parquet file by."),(0,n.yg)("td",{parentName:"tr",align:null}),(0,n.yg)("td",{parentName:"tr",align:null},"None")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Compression Codec"),(0,n.yg)("td",{parentName:"tr",align:null},"Compression codec when writing to the Parquet file. ",(0,n.yg)("br",null),"The Parquet file supports the following codecs: ",(0,n.yg)("inlineCode",{parentName:"td"},"none"),", ",(0,n.yg)("inlineCode",{parentName:"td"},"uncompressed"),", ",(0,n.yg)("inlineCode",{parentName:"td"},"gzip"),", ",(0,n.yg)("inlineCode",{parentName:"td"},"lz4"),", ",(0,n.yg)("inlineCode",{parentName:"td"},"snappy"),", ",(0,n.yg)("inlineCode",{parentName:"td"},"lzo"),", ",(0,n.yg)("inlineCode",{parentName:"td"},"brotli"),", and ",(0,n.yg)("inlineCode",{parentName:"td"},"zstd"),". This overrides the ",(0,n.yg)("inlineCode",{parentName:"td"},"spark.sql.parquet.compression.codec")," parameter."),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"snappy")),(0,n.yg)("td",{parentName:"tr",align:null})),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Write Mode"),(0,n.yg)("td",{parentName:"tr",align:null},"How to handle existing data. For a list of the possible values, see ",(0,n.yg)("a",{parentName:"td",href:"#supported-write-modes"},"Supported write modes"),"."),(0,n.yg)("td",{parentName:"tr",align:null},(0,n.yg)("inlineCode",{parentName:"td"},"error")),(0,n.yg)("td",{parentName:"tr",align:null})))),(0,n.yg)("h3",{id:"supported-write-modes"},"Supported write modes"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Write mode"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"error"),(0,n.yg)("td",{parentName:"tr",align:null},"If the data already exists, throw an exception.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"overwrite"),(0,n.yg)("td",{parentName:"tr",align:null},"If the data already exists, overwrite the data with the contents of the ",(0,n.yg)("inlineCode",{parentName:"td"},"DataFrame"),".")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"append"),(0,n.yg)("td",{parentName:"tr",align:null},"If the data already exists, append the contents of the ",(0,n.yg)("inlineCode",{parentName:"td"},"DataFrame"),".")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"ignore"),(0,n.yg)("td",{parentName:"tr",align:null},"If the data already exists, do nothing with the contents of the ",(0,n.yg)("inlineCode",{parentName:"td"},"DataFrame"),". ",(0,n.yg)("br",null),"This is similar to the ",(0,n.yg)("inlineCode",{parentName:"td"},"CREATE TABLE IF NOT EXISTS")," clause in SQL.")))),(0,n.yg)("h3",{id:"target"},"Example"),(0,n.yg)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,n.yg)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,n.yg)("iframe",{src:"https://user-images.githubusercontent.com/103921419/175030713-9de9d38a-c145-42e9-8411-baa44a70d0d0.mp4",title:"Parquet Target",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,n.yg)("h3",{id:"target-code"},"Generated Code"),(0,n.yg)("admonition",{type:"tip"},(0,n.yg)("p",{parentName:"admonition"},"To see the generated source code, ",(0,n.yg)("a",{parentName:"p",href:"/getting-started/tutorials/spark-with-databricks#review-the-code"},"switch to the Code view")," at the top of the page.")),(0,n.yg)(l.A,{mdxType:"Tabs"},(0,n.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-py"},'def write_parquet(spark: SparkSession, in0: DataFrame):\n    in0.write\\\n        .format("parquet")\\\n        .mode("overwrite")\\\n        .save("dbfs:/data/test_output.parquet")\n'))),(0,n.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-scala"},'object write_parquet {\n  def apply(spark: SparkSession, in: DataFrame): Unit =\n    in.write\n        .format("parquet")\n        .mode("overwrite")\n        .save("dbfs:/data/test_output.parquet")\n}\n')))))}y.isMDXComponent=!0}}]);