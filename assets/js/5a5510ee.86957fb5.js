"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[8806],{15680:(e,t,a)=>{a.d(t,{xA:()=>g,yg:()=>m});var r=a(96540);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},o=Object.keys(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var s=r.createContext({}),p=function(e){var t=r.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},g=function(e){var t=p(e.components);return r.createElement(s.Provider,{value:t},e.children)},c="mdxType",y={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,o=e.originalType,s=e.parentName,g=l(e,["components","mdxType","originalType","parentName"]),c=p(a),u=n,m=c["".concat(s,".").concat(u)]||c[u]||y[u]||o;return a?r.createElement(m,i(i({ref:t},g),{},{components:a})):r.createElement(m,i({ref:t},g))}));function m(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=a.length,i=new Array(o);i[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[c]="string"==typeof e?e:n,i[1]=l;for(var p=2;p<o;p++)i[p]=a[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,a)}u.displayName="MDXCreateElement"},29784:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>y,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var r=a(58168),n=(a(96540),a(15680));const o={title:"Spark with Databricks",id:"spark-with-databricks",description:"Getting started with Spark on Databricks",tags:["Spark","Databricks","tutorial"]},i=void 0,l={unversionedId:"getting-started/tutorials/spark-with-databricks",id:"getting-started/tutorials/spark-with-databricks",title:"Spark with Databricks",description:"Getting started with Spark on Databricks",source:"@site/docs/getting-started/tutorials/spark-with-databricks.md",sourceDirName:"getting-started/tutorials",slug:"/getting-started/tutorials/spark-with-databricks",permalink:"/getting-started/tutorials/spark-with-databricks",draft:!1,tags:[{label:"Spark",permalink:"/tags/spark"},{label:"Databricks",permalink:"/tags/databricks"},{label:"tutorial",permalink:"/tags/tutorial"}],version:"current",frontMatter:{title:"Spark with Databricks",id:"spark-with-databricks",description:"Getting started with Spark on Databricks",tags:["Spark","Databricks","tutorial"]},sidebar:"mySidebar",previous:{title:"Tutorials",permalink:"/getting-started/tutorials/"},next:{title:"SQL with Snowflake",permalink:"/getting-started/tutorials/sql-with-snowflake"}},s={},p=[{value:"Requirements",id:"requirements",level:2},{value:"Create a Databricks Spark fabric",id:"create-a-databricks-spark-fabric",level:2},{value:"Create a Prophecy project",id:"create-a-prophecy-project",level:2},{value:"Set up Git repository",id:"set-up-git-repository",level:3},{value:"Select project dependencies",id:"select-project-dependencies",level:3},{value:"Project metadata and editor",id:"project-metadata-and-editor",level:3},{value:"Develop a pipeline",id:"develop-a-pipeline",level:2},{value:"Source gem",id:"source-gem",level:3},{value:"Reformat gem",id:"reformat-gem",level:3},{value:"Aggregate gem",id:"aggregate-gem",level:3},{value:"Target gem",id:"target-gem",level:3},{value:"Review the code",id:"review-the-code",level:2},{value:"Release and deploy the project",id:"release-and-deploy-the-project",level:2},{value:"Create a Databricks job",id:"create-a-databricks-job",level:2},{value:"Enable the job",id:"enable-the-job",level:3},{value:"What&#39;s next",id:"whats-next",level:2}],g={toc:p},c="wrapper";function y(e){let{components:t,...o}=e;return(0,n.yg)(c,(0,r.A)({},g,o,{components:t,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"When you build pipelines in Prophecy, you can connect to a variety of execution engines to run those pipelines. Complete the tutorial below to learn about using Databricks to compute with Spark and try it yourself!"),(0,n.yg)("h2",{id:"requirements"},"Requirements"),(0,n.yg)("p",null,"For this tutorial, you will need:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"A Prophecy account."),(0,n.yg)("li",{parentName:"ul"},"A Databricks account."),(0,n.yg)("li",{parentName:"ul"},"Permission to create Spark clusters in Databricks.")),(0,n.yg)("h2",{id:"create-a-databricks-spark-fabric"},"Create a Databricks Spark fabric"),(0,n.yg)("p",null,"A ",(0,n.yg)("a",{parentName:"p",href:"/concepts/fabrics/"},"fabric")," in Prophecy is an execution environment. In this tutorial, you'll see how to use Databricks as your execution environment in Prophecy. Let's begin!"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Open Prophecy."),(0,n.yg)("li",{parentName:"ol"},"Click on the ",(0,n.yg)("strong",{parentName:"li"},"Create Entity")," button in the left navigation bar."),(0,n.yg)("li",{parentName:"ol"},"Select the ",(0,n.yg)("strong",{parentName:"li"},"Fabric")," tile."),(0,n.yg)("li",{parentName:"ol"},"Give the fabric any name, like ",(0,n.yg)("inlineCode",{parentName:"li"},"DatabricksTutorial"),"."),(0,n.yg)("li",{parentName:"ol"},"Select the team that will be able to use the fabric. For this tutorial, you might want to select your personal team. (It will match your individual user email.)"),(0,n.yg)("li",{parentName:"ol"},"Click ",(0,n.yg)("strong",{parentName:"li"},"Continue"),".")),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Databricks fabric",src:a(94387).A,width:"2620",height:"1488"})),(0,n.yg)("p",null,"Now, you will enter your Databricks credentials."),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Choose ",(0,n.yg)("strong",{parentName:"li"},"Spark")," as the Provider Type."),(0,n.yg)("li",{parentName:"ol"},"Select ",(0,n.yg)("strong",{parentName:"li"},"Databricks")," under Provider."),(0,n.yg)("li",{parentName:"ol"},"Enter your Databricks Workspace URL. This will be the URL that you use to open your Databricks Workspace."),(0,n.yg)("li",{parentName:"ol"},"For this tutorial, we'll use a ",(0,n.yg)("strong",{parentName:"li"},"Personal Access Token")," for authentication. Visit the Databricks documentation to learn how to ",(0,n.yg)("a",{parentName:"li",href:"https://docs.databricks.com/en/dev-tools/auth/pat.html"},"generate a personal access token"),"."),(0,n.yg)("li",{parentName:"ol"},"Paste your Databricks Personal Access Token."),(0,n.yg)("li",{parentName:"ol"},"Now, click ",(0,n.yg)("strong",{parentName:"li"},"Test Connection"),". This test must be successful to continue with fabric creation."),(0,n.yg)("li",{parentName:"ol"},"After the test succeeds, click ",(0,n.yg)("strong",{parentName:"li"},"Continue"),".")),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Databricks credential",src:a(43236).A,width:"2620",height:"1644"})),(0,n.yg)("p",null,"You are almost done setting up your Databricks fabric!"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"We'll skip the Connection section for now. Click ",(0,n.yg)("strong",{parentName:"li"},"Continue"),"."),(0,n.yg)("li",{parentName:"ol"},"We don't need to add any secrets at this point. Click ",(0,n.yg)("strong",{parentName:"li"},"Complete"),".")),(0,n.yg)("h2",{id:"create-a-prophecy-project"},"Create a Prophecy project"),(0,n.yg)("p",null,"After you create your first Spark fabric, you'll see a ",(0,n.yg)("a",{parentName:"p",href:"/concepts/project/"},"project")," in Prophecy called HelloWorld. If you just want to play around with Prophecy, you can start there. However, for the purpose of this tutorial, let's build a new project from scratch."),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Once again, click on the ",(0,n.yg)("strong",{parentName:"li"},"Create Entity")," button in the left navigation bar."),(0,n.yg)("li",{parentName:"ol"},"Hover over the ",(0,n.yg)("strong",{parentName:"li"},"Project")," tile and select ",(0,n.yg)("strong",{parentName:"li"},"Create"),"."),(0,n.yg)("li",{parentName:"ol"},"Give your project a name."),(0,n.yg)("li",{parentName:"ol"},"For the Project Type, choose ",(0,n.yg)("strong",{parentName:"li"},"Spark/Python (PySpark)"),"."),(0,n.yg)("li",{parentName:"ol"},"Select the team that will have access to your project. Again, you might want to use your personal team.")),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Prophecy project creation",src:a(76061).A,width:"2620",height:"1488"})),(0,n.yg)("h3",{id:"set-up-git-repository"},"Set up Git repository"),(0,n.yg)("p",null,"Prophecy uses ",(0,n.yg)("a",{parentName:"p",href:"/concepts/git/"},"Git")," for version control. Each project in Prophecy is stored as code in its own repository or folder in a repository. For this project, we suggest that you use ",(0,n.yg)("strong",{parentName:"p"},"Prophecy-managed Git"),". When you are ready to use Prophecy for a production use case, ",(0,n.yg)("a",{parentName:"p",href:"/concepts/git/#Git-credentials"},"connect to an external Git provider"),". This provides extra functionality for you to work with."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Prophecy project Git connection",src:a(28118).A,width:"2620",height:"1488"})),(0,n.yg)("p",null,"Once you have set up your Git repository, click ",(0,n.yg)("strong",{parentName:"p"},"Continue"),"."),(0,n.yg)("h3",{id:"select-project-dependencies"},"Select project dependencies"),(0,n.yg)("p",null,"In the Project Dependencies section, you have the option to include ",(0,n.yg)("a",{parentName:"p",href:"/extensibility/dependencies/spark-dependencies"},"additional functionality")," in your project. A few dependencies are added by default, and these are all we will need to get started. Click ",(0,n.yg)("strong",{parentName:"p"},"Complete"),", and then ",(0,n.yg)("strong",{parentName:"p"},"View Project"),"."),(0,n.yg)("h3",{id:"project-metadata-and-editor"},"Project metadata and editor"),(0,n.yg)("p",null,"You should see the ",(0,n.yg)("a",{parentName:"p",href:"/concepts/metadata"},"project metadata")," page. Project metadata includes information about the contents, dependencies, settings, and more related to your project. We want to start editing the project, so click ",(0,n.yg)("strong",{parentName:"p"},"Open in Editor"),"."),(0,n.yg)("h2",{id:"develop-a-pipeline"},"Develop a pipeline"),(0,n.yg)("p",null,"You should now see your empty project. Let's add a pipeline!"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"To get started, click the ",(0,n.yg)("strong",{parentName:"li"},"Create Pipeline")," button."),(0,n.yg)("li",{parentName:"ol"},"Create a new Git branch where you will begin developing this pipeline."),(0,n.yg)("li",{parentName:"ol"},"Give the pipeline a name, like ",(0,n.yg)("inlineCode",{parentName:"li"},"dev"),"."),(0,n.yg)("li",{parentName:"ol"},"Choose ",(0,n.yg)("strong",{parentName:"li"},"Batch")," mode."),(0,n.yg)("li",{parentName:"ol"},"Click ",(0,n.yg)("strong",{parentName:"li"},"Create New"),".")),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Create a pipeline",src:a(6525).A,width:"2620",height:"1488"})),(0,n.yg)("p",null,"Next, we can start building the pipeline. If you want to run the pipeline as you develop it, you need to attach your Databricks fabric now."),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"In the upper right corner of the pipeline editor, click on the ",(0,n.yg)("strong",{parentName:"li"},"Attach a cluster")," dropdown."),(0,n.yg)("li",{parentName:"ol"},"Select the fabric you created earlier."),(0,n.yg)("li",{parentName:"ol"},"Choose a cluster to attach to. If you don't have any clusters, or if your clusters aren't running, you can choose to create a new cluster from the Prophecy UI.")),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Attach cluster",src:a(61616).A,width:"2620",height:"1488"})),(0,n.yg)("p",null,"The cluster may take some time to start up. Once the cluster is up and running, you can move on to the next section."),(0,n.yg)("h3",{id:"source-gem"},"Source gem"),(0,n.yg)("p",null,"The first thing we need in the pipeline is a data source."),(0,n.yg)("p",null,"When you are connected to a Databricks fabric, you can browse the Databricks catalogs in the ",(0,n.yg)("strong",{parentName:"p"},"Environment")," tab of the left sidebar. From here, you can drag and drop tables onto the pipeline canvas. To make things easy, we'll use one of Databricks' sample tables."),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Open the ",(0,n.yg)("strong",{parentName:"li"},"Environment")," tab of the left sidebar."),(0,n.yg)("li",{parentName:"ol"},"Select the ",(0,n.yg)("strong",{parentName:"li"},"samples")," catalog."),(0,n.yg)("li",{parentName:"ol"},"Under ",(0,n.yg)("strong",{parentName:"li"},"tpch > Tables"),", click the ",(0,n.yg)("strong",{parentName:"li"},"plus")," sign next to the ",(0,n.yg)("strong",{parentName:"li"},"orders")," table."),(0,n.yg)("li",{parentName:"ol"},"In the dialog, choose ",(0,n.yg)("strong",{parentName:"li"},"Source")," gem and name the gem. Then, click ",(0,n.yg)("strong",{parentName:"li"},"Add"),"."),(0,n.yg)("li",{parentName:"ol"},"A ",(0,n.yg)("a",{parentName:"li",href:"/Spark/gems/source-target/"},"Source")," gem should be added to the pipeline canvas.")),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Attach cluster",src:a(69519).A,width:"2620",height:"1488"})),(0,n.yg)("h3",{id:"reformat-gem"},"Reformat gem"),(0,n.yg)("p",null,"Next, let's add a new column to the dataset using the ",(0,n.yg)("a",{parentName:"p",href:"/Spark/gems/transform/reformat"},"Reformat")," gem to extract the month and year from the order date."),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Click on ",(0,n.yg)("strong",{parentName:"li"},"Transformations")," to see the transformation gems."),(0,n.yg)("li",{parentName:"ol"},"Select ",(0,n.yg)("strong",{parentName:"li"},"Reformat"),". This adds the gem to the canvas."),(0,n.yg)("li",{parentName:"ol"},"Connect the ",(0,n.yg)("strong",{parentName:"li"},"Source")," gem output to the ",(0,n.yg)("strong",{parentName:"li"},"Reformat")," gem input."),(0,n.yg)("li",{parentName:"ol"},"Hover the ",(0,n.yg)("strong",{parentName:"li"},"Reformat")," gem and click ",(0,n.yg)("strong",{parentName:"li"},"Open"),".")),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Reformat gem",src:a(31044).A,width:"2620",height:"1488"})),(0,n.yg)("p",null,"Add the following configurations:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Hover ",(0,n.yg)("strong",{parentName:"li"},"in0:tpch_orders")," and click ",(0,n.yg)("strong",{parentName:"li"},"Add 9 columns"),"."),(0,n.yg)("li",{parentName:"ol"},"In the next available row, type ",(0,n.yg)("inlineCode",{parentName:"li"},"ordermonth")," in the ",(0,n.yg)("strong",{parentName:"li"},"Target Column"),"."),(0,n.yg)("li",{parentName:"ol"},"In the ",(0,n.yg)("strong",{parentName:"li"},"Expression")," cell, paste ",(0,n.yg)("inlineCode",{parentName:"li"},"month(o_orderdate)"),"."),(0,n.yg)("li",{parentName:"ol"},"In the next available row, type ",(0,n.yg)("inlineCode",{parentName:"li"},"orderyear")," as the target column and ",(0,n.yg)("inlineCode",{parentName:"li"},"year(o_orderdate)")," as the expression."),(0,n.yg)("li",{parentName:"ol"},"Review the new output schema in the ",(0,n.yg)("strong",{parentName:"li"},"Output")," tab of the gem.")),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Reformat configuration",src:a(97454).A,width:"2620",height:"1488"})),(0,n.yg)("p",null,"To preview the output of the gem:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Click ",(0,n.yg)("strong",{parentName:"li"},"Run")," in the top right corner. This will execute the pipeline up to and including the gem."),(0,n.yg)("li",{parentName:"ol"},"You'll be able to see a sample output dataset under ",(0,n.yg)("strong",{parentName:"li"},"Data")," in the gem footer."),(0,n.yg)("li",{parentName:"ol"},"If you ",(0,n.yg)("strong",{parentName:"li"},"Save")," and close the gem, you will also be able to see a new small icon in the output of the Reformat gem. You can click on this to open the ",(0,n.yg)("a",{parentName:"li",href:"/Spark/execution/interactive-execution"},"interim data sample"),".")),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Reformat preview",src:a(6729).A,width:"2620",height:"1488"})),(0,n.yg)("admonition",{type:"note"},(0,n.yg)("ul",{parentName:"admonition"},(0,n.yg)("li",{parentName:"ul"},"To learn more about running pipelines, visit ",(0,n.yg)("a",{parentName:"li",href:"/Spark/execution/interactive-execution"},"Interactive Execution"),"."),(0,n.yg)("li",{parentName:"ul"},"To learn more about Spark SQL expressions in gems, visit ",(0,n.yg)("a",{parentName:"li",href:"/Spark/expression-builder"},"Expression Builder"),"."))),(0,n.yg)("h3",{id:"aggregate-gem"},"Aggregate gem"),(0,n.yg)("p",null,"Another common gem to use is the ",(0,n.yg)("a",{parentName:"p",href:"/Spark/gems/transform/aggregate"},"Aggregate")," gem. Let's use it to find the total sales amount per month each year."),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Select ",(0,n.yg)("strong",{parentName:"li"},"Transformations > Aggregate")," to add the Aggregate gem to the canvas."),(0,n.yg)("li",{parentName:"ol"},"Connect the output of ",(0,n.yg)("strong",{parentName:"li"},"Reformat")," to the input of ",(0,n.yg)("strong",{parentName:"li"},"Aggregate"),". Then, open the Aggregate gem."),(0,n.yg)("li",{parentName:"ol"},"Add the ",(0,n.yg)("strong",{parentName:"li"},"o_totalprice")," column to the Aggregate table."),(0,n.yg)("li",{parentName:"ol"},"Next to amount, write ",(0,n.yg)("inlineCode",{parentName:"li"},"sum(o_totalprice)")," as the expression."),(0,n.yg)("li",{parentName:"ol"},"Switch to the ",(0,n.yg)("strong",{parentName:"li"},"Group By")," tab, and add the ",(0,n.yg)("strong",{parentName:"li"},"orderyear")," and ",(0,n.yg)("strong",{parentName:"li"},"ordermonth")," columns."),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Save")," the gem.")),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Aggregate gem configuration",src:a(4927).A,width:"2812",height:"900"})),(0,n.yg)("h3",{id:"target-gem"},"Target gem"),(0,n.yg)("p",null,"Once you know how to use a couple of gems, you can experiment with others in the pipeline. When you are finished transforming your data, you should write the final table to a target location:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Select ",(0,n.yg)("strong",{parentName:"li"},"Source/Target > Target")," to add the Target gem to the canvas."),(0,n.yg)("li",{parentName:"ol"},"Connect the Target gem after the ",(0,n.yg)("strong",{parentName:"li"},"Aggregate")," gem and open it."),(0,n.yg)("li",{parentName:"ol"},"Click ",(0,n.yg)("strong",{parentName:"li"},"+ New Dataset")," and give the new dataset a name."),(0,n.yg)("li",{parentName:"ol"},"Choose the format to save the dataset, then click ",(0,n.yg)("strong",{parentName:"li"},"Next"),"."),(0,n.yg)("li",{parentName:"ol"},"Choose the location to save the dataset, then click ",(0,n.yg)("strong",{parentName:"li"},"Next"),"."),(0,n.yg)("li",{parentName:"ol"},"In the Properties tab, under ",(0,n.yg)("strong",{parentName:"li"},"Write Mode"),", select ",(0,n.yg)("a",{parentName:"li",href:"/Spark/gems/source-target/file/delta#supported-write-modes"},"overwrite"),"."),(0,n.yg)("li",{parentName:"ol"},"Confirm the schema, and then click ",(0,n.yg)("strong",{parentName:"li"},"Create Dataset"),".")),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Aggregate gem configuration",src:a(63695).A,width:"2620",height:"1488"})),(0,n.yg)("p",null,"To write the data, we should run the whole pipeline. Click on the big play button in the bottom right corner to do so."),(0,n.yg)("h2",{id:"review-the-code"},"Review the code"),(0,n.yg)("p",null,"This far, we've only made changes in the visual editor of the project. If you want to see how your pipeline looks in code, switch to the ",(0,n.yg)("strong",{parentName:"p"},"Code")," view. Since we made a Python project in this tutorial, the code is in Python."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Code view",src:a(9019).A,width:"2620",height:"1488"})),(0,n.yg)("h2",{id:"release-and-deploy-the-project"},"Release and deploy the project"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"In the project footer, click ",(0,n.yg)("strong",{parentName:"li"},"Commit Changes"),". This will open the Git dialog."),(0,n.yg)("li",{parentName:"ol"},"Review your changes and the auto-generated commit message. Then, select ",(0,n.yg)("strong",{parentName:"li"},"Commit"),"."),(0,n.yg)("li",{parentName:"ol"},"Because there were no changes made to remote or source branches, we can skill the ",(0,n.yg)("strong",{parentName:"li"},"Pull")," step."),(0,n.yg)("li",{parentName:"ol"},"Merge your current branch into the main branch by clicking ",(0,n.yg)("strong",{parentName:"li"},"Merge"),"."),(0,n.yg)("li",{parentName:"ol"},"Lastly, you can ",(0,n.yg)("strong",{parentName:"li"},"Release and Deploy")," your project. This makes a certain version of your project available for use (in a CI/CD workflow, for example).")),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Release and deploy",src:a(66317).A,width:"2620",height:"1488"})),(0,n.yg)("h2",{id:"create-a-databricks-job"},"Create a Databricks job"),(0,n.yg)("p",null,"If you want to schedule your pipeline to run periodically, you can create a job to do so."),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"In the left sidebar, hover over ",(0,n.yg)("strong",{parentName:"li"},"Jobs")," and click the ",(0,n.yg)("strong",{parentName:"li"},"plus")," sign."),(0,n.yg)("li",{parentName:"ol"},"Fill in the required fields. For this project, use the ",(0,n.yg)("strong",{parentName:"li"},"Databricks")," scheduler."),(0,n.yg)("li",{parentName:"ol"},"Leave the schedule interval blank for now. This is where you define the job run frequency."),(0,n.yg)("li",{parentName:"ol"},"Click ",(0,n.yg)("strong",{parentName:"li"},"Create New"),".")),(0,n.yg)("p",null,"Now, you can configure the flow of your job."),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Click on ",(0,n.yg)("strong",{parentName:"li"},"Pipeline")," to add it to the canvas."),(0,n.yg)("li",{parentName:"ol"},"Under ",(0,n.yg)("strong",{parentName:"li"},"Pipeline to schedule"),", choose the pipeline you created in this tutorial."),(0,n.yg)("li",{parentName:"ol"},"Click ",(0,n.yg)("strong",{parentName:"li"},"Save"),".")),(0,n.yg)("h3",{id:"enable-the-job"},"Enable the job"),(0,n.yg)("p",null,"To enable this job, you ",(0,n.yg)("strong",{parentName:"p"},"must")," complete the following two steps:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Turn on the ",(0,n.yg)("strong",{parentName:"li"},"Enabled")," toggle."),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Release your project"),".")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"If you do not release your project, the Databricks scheduler will not see that you have enabled this job.")),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Enable job",src:a(21363).A,width:"2620",height:"1488"})),(0,n.yg)("h2",{id:"whats-next"},"What's next"),(0,n.yg)("p",null,"Great work! You've successfully set up, developed, run, and deployed your first Spark project using a Databricks execution environment. Take a moment to appreciate your accomplishment!"),(0,n.yg)("p",null,"To continue learning and expanding your skills with Prophecy, keep exploring our documentation and apply your newfound knowledge to address real-world business challenges!"),(0,n.yg)("p",null,"If you ever encounter any difficulties, don't hesitate to reach out to us (",(0,n.yg)("a",{parentName:"p",href:"mailto:Contact.us@Prophecy.io"},"Contact.us@Prophecy.io"),") or join our ",(0,n.yg)("a",{parentName:"p",href:"https://prophecy-io-support.slack.com/join/shared_invite/zt-moq3xzoj-~5MSJ6WPnZfz7bwsqWi8tQ#/shared-invite/email"},"Slack community")," for assistance. We're here to help!"))}y.isMDXComponent=!0},4927:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/spark-databricks-aggregate-a0d659eabfa5b045462170b805ac10d2.png"},61616:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/spark-databricks-cluster-1dc4fff38be9ddaca31e3bdfce5d20f5.png"},9019:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/spark-databricks-code-b8694b2a9e910b0552ce2e6a289c59e7.png"},6525:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/spark-databricks-create-pipeline-4e4b5de59df1a7db63d0c545fd21a072.png"},43236:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/spark-databricks-credentials-96dc280e488afac55811f442b6266604.png"},94387:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/spark-databricks-fabric-87b1232a4e89fa765e05a0f54be7fd95.png"},28118:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/spark-databricks-git-f6d24a45765026e8f10702c14a9ccfa0.png"},21363:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/spark-databricks-job-0813c26e42e467fd096063f63e7f4361.png"},76061:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/spark-databricks-project-8c637dcb301888023b9ba4cafe6af2c3.png"},6729:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/spark-databricks-reformat-data-5503f7b39d667a3ad5f7063663eaac38.png"},31044:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/spark-databricks-reformat-gem-e61c927d7aad5670a882fd7c93ede01c.png"},97454:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/spark-databricks-reformat-48bcaeccd4a4f3e96258404e2ad4d458.png"},66317:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/spark-databricks-release-3caa5537720e702967c649d93a7ab4cd.png"},69519:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/spark-databricks-source-523673691779c31322c090db38cb14bb.png"},63695:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/spark-databricks-target-37cbd72ac6307e14ca22a77b5ca18df8.png"}}]);