"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[4400],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return h}});var i=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,i,o=function(e,t){if(null==e)return{};var n,i,o={},a=Object.keys(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var p=i.createContext({}),s=function(e){var t=i.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},u=function(e){var t=s(e.components);return i.createElement(p.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},c=i.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,p=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),c=s(n),h=o,m=c["".concat(p,".").concat(h)]||c[h]||d[h]||a;return n?i.createElement(m,r(r({ref:t},u),{},{components:n})):i.createElement(m,r({ref:t},u))}));function h(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,r=new Array(a);r[0]=c;var l={};for(var p in t)hasOwnProperty.call(t,p)&&(l[p]=t[p]);l.originalType=e,l.mdxType="string"==typeof e?e:o,r[1]=l;for(var s=2;s<a;s++)r[s]=n[s];return i.createElement.apply(null,r)}return i.createElement.apply(null,n)}c.displayName="MDXCreateElement"},69289:function(e,t,n){n.r(t),n.d(t,{assets:function(){return u},contentTitle:function(){return p},default:function(){return h},frontMatter:function(){return l},metadata:function(){return s},toc:function(){return d}});var i=n(87462),o=n(63366),a=(n(67294),n(3905)),r=["components"],l={title:"Prophecy Build Tool (pbt)",id:"prophecy-build-tool",description:"Prophecy Build tool",sidebar_position:4,tags:["metadata","build","deploy","test","cli","continuous integration","continuous deployment"]},p=void 0,s={unversionedId:"metadata/prophecy-build-tool",id:"metadata/prophecy-build-tool",title:"Prophecy Build Tool (pbt)",description:"Prophecy Build tool",source:"@site/docs/metadata/prophecy-build-tool.md",sourceDirName:"metadata",slug:"/metadata/prophecy-build-tool",permalink:"/metadata/prophecy-build-tool",draft:!1,tags:[{label:"metadata",permalink:"/tags/metadata"},{label:"build",permalink:"/tags/build"},{label:"deploy",permalink:"/tags/deploy"},{label:"test",permalink:"/tags/test"},{label:"cli",permalink:"/tags/cli"},{label:"continuous integration",permalink:"/tags/continuous-integration"},{label:"continuous deployment",permalink:"/tags/continuous-deployment"}],version:"current",sidebarPosition:4,frontMatter:{title:"Prophecy Build Tool (pbt)",id:"prophecy-build-tool",description:"Prophecy Build tool",sidebar_position:4,tags:["metadata","build","deploy","test","cli","continuous integration","continuous deployment"]},sidebar:"defaultSidebar",previous:{title:"Lineage",permalink:"/metadata/lineage"},next:{title:"Pull request templates",permalink:"/metadata/pr-templates"}},u={},d=[{value:"Features (v1.0.4.1)",id:"features-v1041",level:2},{value:"Requirements",id:"requirements",level:2},{value:"Installation",id:"installation",level:2},{value:"Quickstart",id:"quickstart",level:2},{value:"Usage",id:"usage",level:3},{value:"Running locally",id:"running-locally",level:3},{value:"Building Pipelines and deploying Jobs",id:"building-pipelines-and-deploying-jobs",level:4},{value:"Running all unit tests in project",id:"running-all-unit-tests-in-project",level:4},{value:"Integrating with GitHub Actions",id:"integrating-with-github-actions",level:2},{value:"Pre-requisite",id:"pre-requisite",level:3},{value:"Setting up environment variables and secrets",id:"setting-up-environment-variables-and-secrets",level:3},{value:"Setting up a GitHub Actions Workflow on every push to main branch",id:"setting-up-a-github-actions-workflow-on-every-push-to-main-branch",level:3}],c={toc:d};function h(e){var t=e.components,l=(0,o.Z)(e,r);return(0,a.kt)("wrapper",(0,i.Z)({},c,l,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Prophecy-built-tool")," (PBT) allows you to quickly build, test and deploy projects generated by Prophecy (your standard Spark Scala and\nPySpark Pipelines) to integrate with your own CI / CD (e.g. Github Actions), build system (e.g. Jenkins), and\norchestration (e.g. Databricks Workflows)."),(0,a.kt)("h2",{id:"features-v1041"},"Features (v1.0.4.1)"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Build and unit test all Pipelines in Prophecy projects (Scala and Python)"),(0,a.kt)("li",{parentName:"ul"},"Deploy Jobs with built Pipelines on Databricks"),(0,a.kt)("li",{parentName:"ul"},"Deploying Jobs filtered with Fabric ids on Databricks"),(0,a.kt)("li",{parentName:"ul"},"Integrate with CI/CD tools like GitHub Actions"),(0,a.kt)("li",{parentName:"ul"},"Verify the project structure of Prophecy projects"),(0,a.kt)("li",{parentName:"ul"},"Support for Project Configurations")),(0,a.kt)("h2",{id:"requirements"},"Requirements"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Python >=3.6 (Recommended 3.9.13)"),(0,a.kt)("li",{parentName:"ul"},"pip"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"pyspark")," (Recommended 3.3.0)")),(0,a.kt)("h2",{id:"installation"},"Installation"),(0,a.kt)("p",null,"To install PBT, simply run:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"pip3 install prophecy-build-tool\n")),(0,a.kt)("h2",{id:"quickstart"},"Quickstart"),(0,a.kt)("h3",{id:"usage"},"Usage"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"Usage: pbt [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  build\n  deploy\n  test\n")),(0,a.kt)("h3",{id:"running-locally"},"Running locally"),(0,a.kt)("p",null,"The PBT cli can be used to build, test and deploy projects created by Prophecy that are present in your local filesystem."),(0,a.kt)("p",null,"Please make sure the ",(0,a.kt)("strong",{parentName:"p"},"DATABRICKS_URL")," and ",(0,a.kt)("strong",{parentName:"p"},"DATABRICKS_TOKEN")," environment variables are set appropriately pointing to your Databricks workspace before running any PBT commands.\nExample:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},'export DATABRICKS_HOST="https://example_databricks_host.cloud.databricks.com"\nexport DATABRICKS_TOKEN="exampledatabrickstoken"\n')),(0,a.kt)("h4",{id:"building-pipelines-and-deploying-jobs"},"Building Pipelines and deploying Jobs"),(0,a.kt)("p",null,"PBT can build and deploy Jobs inside your Prophecy project to the Databricks environment defined by the ",(0,a.kt)("inlineCode",{parentName:"p"},"DATABRICKS_HOST")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"DATABRICKS_TOKEN"),"\nenvironment variables."),(0,a.kt)("p",null,"Since v1.0.3 PBT supports new input parameters that are used to determine the DBFS path where your project's artifacts would\nbe uploaded. These are the ",(0,a.kt)("inlineCode",{parentName:"p"},"--release-version")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"--project-id")," parameters which would be used to replace the ",(0,a.kt)("inlineCode",{parentName:"p"},"__PROJECT_RELEASE_VERSION_ PLACEHOLDER__")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"__PROJECT_ID_PLACEHOLDER__")," placeholders that would already be present in your Job's definition file\n(",(0,a.kt)("inlineCode",{parentName:"p"},"databricks-job.json"),"). Using a unique release version of your choice and the project's Prophecy ID\n(as seen in the project's URL on the Prophecy UI) is recommended."),(0,a.kt)("p",null,"Example deploy command:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"pbt deploy --path /path/to/your/prophecy_project/ --release-version 1.0 --project-id 10\n")),(0,a.kt)("p",null,"Sample output:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"Prophecy-build-tool v1.0.4.1\n\nFound 1 jobs: daily\nFound 1 pipelines: customers_orders (python)\n\nBuilding 1 pipelines \ud83d\udeb0\n\n  Building pipeline pipelines/customers_orders [1/1]\n\n\u2705 Build complete!\n\nDeploying 1 jobs \u23f1\n\n  Deploying job jobs/daily [1/1]\n    Uploading customers_orders-1.0-py3-none-any.whl to\ndbfs:/FileStore/prophecy/artifacts/...\nQuerying existing jobs to find current job: Offset: 0, Pagesize: 25\n    Updating an existing job: daily\n\n\u2705 Deployment completed successfully!\n")),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"deploy")," command also supports an advanced option ",(0,a.kt)("inlineCode",{parentName:"p"},"--dependent-projects-path")," if there is a need to build projects other than the main project that has to be deployed.\nThis would be useful if there are dependent Pipelines whose source code can be cloned into a different directory accessible to PBT\nwhile running ",(0,a.kt)("inlineCode",{parentName:"p"},"deploy")," for the main project. This option supports only one path as argument but the path itself can contain multiple Prophecy projects within it in different\nsubdirectories."),(0,a.kt)("p",null,"Example deploy command:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"pbt deploy --path /path/to/your/prophecy_project/ --release-version 1.0 --project-id 10 --dependent-projects-path /path/to/dependent/prophecy/projects\n")),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"deploy")," command also supports an advanced option ",(0,a.kt)("inlineCode",{parentName:"p"},"--fabric-ids")," ( comma separated if more than one ), if there is a need to only deploy Jobs associated with certain Fabric ids,\nyou can find the Fabric id in your Prophecy metadata page.\nfollowing command will filter out and only deploy the jobs associated with given Fabric ids.\nExample deploy:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"pbt deploy --fabric-ids 647,1527 --path /path/to/your/prophecy_project/\n")),(0,a.kt)("p",null,"Sample output:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"Project name: HelloWorld\nFound 2 jobs: ashish-TestJob2, ashish-TestJob\nFound 4 pipelines: customers_orders (python), report_top_customers (python), join_agg_sort (python),\nfarmers-markets-irs (python)\n[SKIP]: Skipping builds for all pipelines as '--skip-builds' flag is passed.\n\n Deploying 2 jobs\nDeploying jobs only for given Fabric IDs: ['647', '1527']\n\n[START]:  Deploying job jobs/TestJob2 [1/2]\n[DEPLOY]: Job being deployed for fabric id: 1527\n    Pipeline pipelines/farmers-markets-irs might be shared, checking if it exists in DBFS\n    Dependent package exists on DBFS already, continuing with next pipeline\n    Pipeline pipelines/report_top_customers might be shared, checking if it exists in DBFS\n    Dependent package exists on DBFS already, continuing with next pipeline\n    Querying existing jobs to find current job: Offset: 0, Pagesize: 25\n    Updating an existing job: ashish-TestJob2\n\n[START]:  Deploying job jobs/TestJob [2/2]\n[DEPLOY]: Job being deployed for fabric id: 647\n    Pipeline pipelines/customers_orders might be shared, checking if it exists in DBFS\n    Dependent package exists on DBFS already, continuing with next pipeline\n    Pipeline pipelines/join_agg_sort might be shared, checking if it exists in DBFS\n    Dependent package exists on DBFS already, continuing with next pipeline\n    Pipeline pipelines/report_top_customers might be shared, checking if it exists in DBFS\n    Dependent package exists on DBFS already, continuing with next pipeline\n    Querying existing jobs to find current job: Offset: 0, Pagesize: 25\n    Updating an existing job: ashish-TestJob\n\n\u2705 Deployment completed successfully!\n")),(0,a.kt)("p",null,"By default, ",(0,a.kt)("inlineCode",{parentName:"p"},"deploy")," command builds all pipelines and then deploys them, if you want to skip building all pipelines\n( this could be useful, if you are running a ",(0,a.kt)("inlineCode",{parentName:"p"},"deploy")," command after running ",(0,a.kt)("inlineCode",{parentName:"p"},"deploy")," or ",(0,a.kt)("inlineCode",{parentName:"p"},"build")," previously.)"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"pbt deploy --skip-builds --path /path/to/your/prophecy_project/\n")),(0,a.kt)("p",null,"Complete list of options for PBT ",(0,a.kt)("inlineCode",{parentName:"p"},"deploy"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"pbt deploy --help\nProphecy-build-tool v1.0.4.1\n\nUsage: pbt deploy [OPTIONS]\n\nOptions:\n  --path TEXT                     Path to the directory containing the\n                                  pbt_project.yml file  [required]\n  --dependent-projects-path TEXT  Dependent projects path\n  --release-version TEXT          Release version to be used during\n                                  deployments\n  --project-id TEXT               Project Id placeholder to be used during\n                                  deployments\n  --prophecy-url TEXT             Prophecy URL placeholder to be used during\n                                  deployments\n  --fabric-ids TEXT               Fabric IDs(comma separated) which can be\n                                  used to filter jobs for deployments\n  --skip-builds                   Flag to skip building Pipelines\n  --help                          Show this message and exit.\n")),(0,a.kt)("h4",{id:"running-all-unit-tests-in-project"},"Running all unit tests in project"),(0,a.kt)("p",null,"PBT supports running unit tests inside the Prophecy project. Unit tests run with the ",(0,a.kt)("inlineCode",{parentName:"p"},"default")," configuration present in the\nPipeline's ",(0,a.kt)("inlineCode",{parentName:"p"},"configs/resources/config")," directory."),(0,a.kt)("p",null,"To run all unit tests present in the project, use the ",(0,a.kt)("inlineCode",{parentName:"p"},"test")," command as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"pbt test --path /path/to/your/prophecy_project/\n")),(0,a.kt)("p",null,"Sample output:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"Prophecy-build-tool v1.0.1\n\nFound 1 jobs: daily\nFound 1 pipelines: customers_orders (python)\n\n  Unit Testing pipeline pipelines/customers_orders [1/1]\n\n    ============================= test session starts ==============================\n    platform darwin -- Python 3.8.9, pytest-7.1.2, pluggy-1.0.0 -- /Library/Developer/CommandLineTools/usr/bin/python\n    cachedir: .pytest_cache\n    metadata: None\n    rootdir: /path/to/your/prophecy_project/pipelines/customers_orders/code\n    plugins: html-3.1.1, metadata-2.0.2\n    collecting ... collected 1 item\n\n    test/TestSuite.py::CleanupTest::test_unit_test_0 PASSED                  [100%]\n\n    ============================== 1 passed in 17.42s ==============================\n\n\u2705 Unit test for pipeline: pipelines/customers_orders succeeded.\n")),(0,a.kt)("h2",{id:"integrating-with-github-actions"},"Integrating with GitHub Actions"),(0,a.kt)("p",null,"PBT can be integrated with your own CI/CD solution to build, test and deploy Prophecy code. The steps for setting up PBT with Github Actions on your repository containing a Prophecy project is mentioned below."),(0,a.kt)("h3",{id:"pre-requisite"},"Pre-requisite"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"A Prophecy project that is currently hosted in a Github repository")),(0,a.kt)("h3",{id:"setting-up-environment-variables-and-secrets"},"Setting up environment variables and secrets"),(0,a.kt)("p",null,"PBT requires environment variables ",(0,a.kt)("strong",{parentName:"p"},"DATABRICKS_URL")," and ",(0,a.kt)("strong",{parentName:"p"},"DATABRICKS_TOKEN")," to be set for complete functionality."),(0,a.kt)("p",null,"The ",(0,a.kt)("strong",{parentName:"p"},"DATABRICKS_TOKEN")," that needs to be used can be set as a secret inside the Github repository of the project.\nSteps:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Go to Settings > Secrets > Actions from the repository menu"),(0,a.kt)("li",{parentName:"ul"},"Click \u2018New Repository secret\u2019"),(0,a.kt)("li",{parentName:"ul"},"Add the secret with name DATABRICKS_TOKEN and value of the Databricks token to be used by PBT.")),(0,a.kt)("p",null,"Screenshot after setting DATABRICKS_TOKEN secret:\n",(0,a.kt)("img",{alt:"Github Actions Secret addition",src:n(2942).Z,width:"2230",height:"1288"})),(0,a.kt)("p",null,"The environment variables can now be all set within the Github actions YML file as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'env:\n  DATABRICKS_HOST: "https://sample_databricks_url.cloud.databricks.com"\n  DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}\n')),(0,a.kt)("p",null,"The complete YML file definition is discussed in the next section."),(0,a.kt)("h3",{id:"setting-up-a-github-actions-workflow-on-every-push-to-main-branch"},"Setting up a GitHub Actions Workflow on every push to main branch"),(0,a.kt)("p",null,"We\u2019re now ready to setup CI/CD on the Prophecy project.\nTo setup a workflow to build, run all unit tests and then deploy the built jar (Scala)/ whl (Python) on Databricks on every push to the main automatically:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Create a .YML file in the project repository at the below location (relative to root)")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},".github/workflows/exampleWorkflow.yml\n")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Add the below contents to ",(0,a.kt)("strong",{parentName:"li"},"exampleWorkflow.yml"))),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'name: Example CI/CD with Github actions\n\non:\n  push:\n    branches:\n      - "main"\n\nenv:\n  DATABRICKS_HOST: "https://sample_databricks_url.cloud.databricks.com"\n  DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up JDK 11\n        uses: actions/setup-java@v3\n        with:\n          java-version: "11"\n          distribution: "adopt"\n      - name: Set up Python 3.9.13\n        uses: actions/setup-python@v4\n        with:\n          python-version: "3.9.13"\n      # Install all python dependencies\n      # prophecy-libs not included here because prophecy-build-tool takes care of it by reading each pipeline\'s setup.py\n      - name: Install dependencies\n        run: |\n          python3 -m pip install --upgrade pip\n          pip3 install build pytest wheel pytest-html pyspark==3.3.0  prophecy-build-tool\n      - name: Run PBT build\n        run: pbt build --path .\n      - name: Run PBT test\n        run: pbt test --path .\n      - name: Run PBT deploy\n        run: pbt deploy --path . --release-version 1.0 --project-id example_project_id\n')),(0,a.kt)("p",null,"The above workflow does the following in order:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Triggers on every change that is pushed to the branch \u2018main\u2019."),(0,a.kt)("li",{parentName:"ol"},"Sets the environment variables required for PBT to run: DATABRICKS_HOST and DATABRICKS_TOKEN."),(0,a.kt)("li",{parentName:"ol"},"Sets up JDK 11, Python 3 and other dependencies required for PBT to run."),(0,a.kt)("li",{parentName:"ol"},"Builds all the Pipelines present in the project and generates a .jar/.whl file. If the build fails at any point a non-zero exit code is returned which stops the workflow from proceeding further and the workflow run is marked as a failure."),(0,a.kt)("li",{parentName:"ol"},"Runs all the unit tests present in the project using FABRIC_NAME(optional) as the configuration. If any of the unit test fails a non-zero exit code is returned which stops the workflow from proceeding further and the workflow run is marked as a failure."),(0,a.kt)("li",{parentName:"ol"},"Deploys the built .jar/.whl to the Databricks location mentioned in ",(0,a.kt)("inlineCode",{parentName:"li"},"databricks-job.json")," mentioned in the ",(0,a.kt)("inlineCode",{parentName:"li"},"jobs")," directory of the project. If the Job already exists in Databricks it is updated with the new .jar/.whl."),(0,a.kt)("li",{parentName:"ol"},"Deploys Pipeline configurations, if present, to the DBFS path mentioned in ",(0,a.kt)("inlineCode",{parentName:"li"},"databricks-job.json"),"."),(0,a.kt)("li",{parentName:"ol"},"If this process fails at any step, a non-zero exit code is returned which stops the workflow from proceeding further and the workflow run is marked as a failure.")))}h.isMDXComponent=!0},2942:function(e,t,n){t.Z=n.p+"assets/images/pbt-github-secret-b7e9a81b0279316b77fc4a01e9e20bcf.png"}}]);