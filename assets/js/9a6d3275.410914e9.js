"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[9571],{3905:function(e,t,o){o.d(t,{Zo:function(){return u},kt:function(){return f}});var r=o(67294);function n(e,t,o){return t in e?Object.defineProperty(e,t,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[t]=o,e}function a(e,t){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),o.push.apply(o,r)}return o}function i(e){for(var t=1;t<arguments.length;t++){var o=null!=arguments[t]?arguments[t]:{};t%2?a(Object(o),!0).forEach((function(t){n(e,t,o[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):a(Object(o)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(o,t))}))}return e}function c(e,t){if(null==e)return{};var o,r,n=function(e,t){if(null==e)return{};var o,r,n={},a=Object.keys(e);for(r=0;r<a.length;r++)o=a[r],t.indexOf(o)>=0||(n[o]=e[o]);return n}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)o=a[r],t.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(n[o]=e[o])}return n}var s=r.createContext({}),l=function(e){var t=r.useContext(s),o=t;return e&&(o="function"==typeof e?e(t):i(i({},t),e)),o},u=function(e){var t=l(e.components);return r.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var o=e.components,n=e.mdxType,a=e.originalType,s=e.parentName,u=c(e,["components","mdxType","originalType","parentName"]),d=l(o),f=n,b=d["".concat(s,".").concat(f)]||d[f]||p[f]||a;return o?r.createElement(b,i(i({ref:t},u),{},{components:o})):r.createElement(b,i({ref:t},u))}));function f(e,t){var o=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var a=o.length,i=new Array(a);i[0]=d;var c={};for(var s in t)hasOwnProperty.call(t,s)&&(c[s]=t[s]);c.originalType=e,c.mdxType="string"==typeof e?e:n,i[1]=c;for(var l=2;l<a;l++)i[l]=o[l];return r.createElement.apply(null,i)}return r.createElement.apply(null,o)}d.displayName="MDXCreateElement"},61105:function(e,t,o){o.r(t),o.d(t,{assets:function(){return u},contentTitle:function(){return s},default:function(){return f},frontMatter:function(){return c},metadata:function(){return l},toc:function(){return p}});var r=o(87462),n=o(63366),a=(o(67294),o(3905)),i=["components"],c={title:"Low-code Jobs",id:"low-code-jobs",description:"Low-code Jobs",tags:["jobs","deployment","scheduling"]},s=void 0,l={unversionedId:"low-code-jobs/low-code-jobs",id:"low-code-jobs/low-code-jobs",title:"Low-code Jobs",description:"Low-code Jobs",source:"@site/docs/low-code-jobs/low-code-jobs.md",sourceDirName:"low-code-jobs",slug:"/low-code-jobs/",permalink:"/low-code-jobs/",draft:!1,tags:[{label:"jobs",permalink:"/tags/jobs"},{label:"deployment",permalink:"/tags/deployment"},{label:"scheduling",permalink:"/tags/scheduling"}],version:"current",frontMatter:{title:"Low-code Jobs",id:"low-code-jobs",description:"Low-code Jobs",tags:["jobs","deployment","scheduling"]},sidebar:"defaultSidebar",previous:{title:"Interactive Execution",permalink:"/low-code-spark/execution/interactive-execution"},next:{title:"Databricks Jobs",permalink:"/low-code-jobs/databricks-jobs"}},u={},p=[],d={toc:p};function f(e){var t=e.components,o=(0,n.Z)(e,i);return(0,a.kt)("wrapper",(0,r.Z)({},d,o,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,"Once you have developed a Spark data Pipeline using Prophecy, you will want to schedule it to run at some frequency. To\nsupport this, Prophecy provides you with an easy to use low-code interface to develop Jobs, using two different\nschedulers:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},(0,a.kt)("a",{parentName:"strong",href:"/low-code-jobs/databricks-jobs"},"Databricks Jobs"))," - for simpler data-Pipeline use-cases, where you just\norchestrate multiple data-Pipelines to run together. Databricks Jobs is a ",(0,a.kt)("strong",{parentName:"p"},"recommended")," scheduler, if you're\nDatabricks Native.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},(0,a.kt)("a",{parentName:"strong",href:"/low-code-jobs/airflow"},"Airflow"))," - for more complex use-cases, where you have to use various operators, or need\nany additional data pre-and-post-processing, you can design your Jobs using Prophecy's low-code Airflow environment."))),(0,a.kt)("p",null,"Alternatively, since Prophecy provides you native Spark code on Git, you can easily integrate with any other scheduler."))}f.isMDXComponent=!0}}]);