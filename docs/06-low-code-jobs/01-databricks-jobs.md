---
sidebar_position: 1
title: Databricks Jobs
---

Once you have developed a spark workflow using prophecy, you will want to schedule it to run at some frequency. To
support this, prophecy integrates with Airflow and provides a low-code interface to develop Airflow DAGs.

## Development

## Configuration 

## Deployment

## Monitoring 

## Guides

1. [How to trigger a job from another job?](/tutorials/low-code-jobs/multi-jobs-trigger)
2. [How to work with multiple execution environments?](/tutorials/low-code-jobs/deploying-jobs-across-fabrics)
