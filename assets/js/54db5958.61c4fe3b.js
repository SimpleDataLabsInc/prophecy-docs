"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[53479],{28453:(e,s,r)=>{r.d(s,{R:()=>a,x:()=>c});var i=r(96540);const t={},n=i.createContext(t);function a(e){const s=i.useContext(n);return i.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function c(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(n.Provider,{value:s},e.children)}},63004:(e,s,r)=>{r.r(s),r.d(s,{assets:()=>l,contentTitle:()=>c,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>o});const i=JSON.parse('{"id":"administration/fabrics/Spark-fabrics/databricks/databricks-serverless","title":"Databricks serverless compute for PySpark","description":"Use Databricks serverless compute to execute PySpark pipelines","source":"@site/docs/administration/fabrics/Spark-fabrics/databricks/databricks-serverless.md","sourceDirName":"administration/fabrics/Spark-fabrics/databricks","slug":"/administration/fabrics/Spark-fabrics/databricks/databricks-serverless","permalink":"/administration/fabrics/Spark-fabrics/databricks/databricks-serverless","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"fabric","permalink":"/tags/fabric"},{"inline":true,"label":"databricks","permalink":"/tags/databricks"}],"version":"current","frontMatter":{"title":"Databricks serverless compute for PySpark","sidebar_label":"Databricks serverless","id":"databricks-serverless","description":"Use Databricks serverless compute to execute PySpark pipelines","tags":["fabric","databricks"]},"sidebar":"adminSidebar","previous":{"title":"Databricks","permalink":"/administration/fabrics/Spark-fabrics/databricks/"},"next":{"title":"UC cluster compatibility","permalink":"/administration/fabrics/Spark-fabrics/databricks/ucshared"}}');var t=r(74848),n=r(28453);const a={title:"Databricks serverless compute for PySpark",sidebar_label:"Databricks serverless",id:"databricks-serverless",description:"Use Databricks serverless compute to execute PySpark pipelines",tags:["fabric","databricks"]},c=void 0,l={},o=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Supported data sources",id:"supported-data-sources",level:2},{value:"Supported data sampling modes",id:"supported-data-sampling-modes",level:2},{value:"Limitations",id:"limitations",level:2}];function d(e){const s={a:"a",admonition:"admonition",h2:"h2",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,n.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.a,{href:"https://docs.databricks.com/aws/en/compute/serverless/",children:"Databricks serverless compute"})," allows you to run workloads without manually provisioning a Spark cluster. With serverless compute, Databricks takes care of the infrastructure in the background, so your jobs start up quickly and scale as needed. Prophecy supports serverless compute for running pipelines in PySpark projects on Databricks."]}),"\n",(0,t.jsx)(s.p,{children:"This page explains how to use serverless compute with Prophecy, including supported data sources, data sampling modes, and current limitations."}),"\n",(0,t.jsx)(s.admonition,{type:"info",children:(0,t.jsxs)(s.p,{children:["Databricks serverless compute differs from ",(0,t.jsx)(s.a,{href:"https://docs.databricks.com/aws/en/compute/sql-warehouse/#what-is-serverless-sql",children:"serverless SQL warehouses"}),". Prophecy uses serverless compute to run Spark pipelines on Spark fabrics. In contrast, serverless SQL warehouses are connected to Prophecy via JDBC and are used to run SQL queries generated from pipelines in SQL projects."]})}),"\n",(0,t.jsx)(s.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(s.p,{children:"To use serverless compute in Prophecy, you need:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.a,{href:"https://docs.databricks.com/aws/en/compute/serverless/#enable-serverless-compute",children:"Access to serverless compute"})," in Databricks"]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.a,{href:"/projects#project-types",children:"PySpark projects"})," in Prophecy (Scala not supported)"]}),"\n"]}),"\n",(0,t.jsx)(s.h2,{id:"supported-data-sources",children:"Supported data sources"}),"\n",(0,t.jsx)(s.p,{children:"You can run the following sources on Databricks serverless compute:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"/engineers/avro",children:"Avro"})}),"\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"/engineers/csv",children:"CSV"})}),"\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"/engineers/data-generator",children:"Data Generator"})}),"\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"/engineers/delta",children:"Delta file"})}),"\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"/engineers/json",children:"JSON"})}),"\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"/engineers/kafka",children:"Kafka"})}),"\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"/engineers/orc",children:"ORC"})}),"\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"/engineers/parquet",children:"Parquet"})}),"\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"/engineers/seed",children:"Seed files"})}),"\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"https://docs.databricks.com/aws/en/tables/",children:"Unity Catalog tables"})}),"\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-volumes",children:"Unity Catalog volumes"})}),"\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"/engineers/xml",children:"XML"})}),"\n"]}),"\n",(0,t.jsx)(s.h2,{id:"supported-data-sampling-modes",children:"Supported data sampling modes"}),"\n",(0,t.jsx)(s.p,{children:"You can use the following data sampling modes when using Databricks serverless compute:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.a,{href:"/engineers/data-sampling#selective-sampling",children:"Selective"})," mode"]}),"\n",(0,t.jsx)(s.li,{children:"Vanilla mode (deprecated)"}),"\n"]}),"\n",(0,t.jsx)(s.h2,{id:"limitations",children:"Limitations"}),"\n",(0,t.jsx)(s.p,{children:"Below are the current limitations of Databricks Serverless and how they impact Prophecy project development."}),"\n",(0,t.jsxs)(s.table,{children:[(0,t.jsx)(s.thead,{children:(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.th,{children:(0,t.jsx)(s.strong,{children:"Feature"})}),(0,t.jsx)(s.th,{children:(0,t.jsx)(s.strong,{children:"Limitation"})})]})}),(0,t.jsxs)(s.tbody,{children:[(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:"Scala support"}),(0,t.jsxs)(s.td,{children:["Databricks serverless only supports Python and SQL. ",(0,t.jsx)("br",{}),(0,t.jsx)(s.a,{href:"/projects#project-types",children:"Scala projects"})," cannot run on Databricks Serverless."]})]}),(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:"Dependencies"}),(0,t.jsxs)(s.td,{children:["Only Python dependencies are supported. ",(0,t.jsx)("br",{}),(0,t.jsx)(s.a,{href:"/engineers/dependencies",children:"Dependencies"})," must be added through the Prophecy UI. ",(0,t.jsx)("br",{}),"You cannot install dependencies to serverless compute directly in Databricks."]})]}),(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:"Row size"}),(0,t.jsx)(s.td,{children:"Maximum row size is 128MB."})]}),(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:"Driver size"}),(0,t.jsx)(s.td,{children:"Databricks serverless driver size is unknown and cannot be changed."})]}),(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:"Supported data formats"}),(0,t.jsx)(s.td,{children:"XLSX, fixed format, and custom formats are not supported."})]}),(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:"UDF network access"}),(0,t.jsxs)(s.td,{children:[(0,t.jsx)(s.a,{href:"/engineers/user-defined-functions",children:"UDFs"})," cannot access the internet."]})]}),(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:"Spark configuration"}),(0,t.jsxs)(s.td,{children:["Databricks Serverless only supports a limited number of ",(0,t.jsx)(s.a,{href:"https://docs.databricks.com/aws/en/spark/conf#configure-spark-properties-for-serverless-notebooks-and-jobs",children:"Spark configuration properties"}),"."]})]}),(0,t.jsxs)(s.tr,{children:[(0,t.jsxs)(s.td,{children:["APIs in ",(0,t.jsx)(s.a,{href:"/engineers/script",children:"Script gems"})]}),(0,t.jsxs)(s.td,{children:["Spark Connect APIs are supported. ",(0,t.jsx)("br",{}),"Spark RDD APIs are not supported. ",(0,t.jsx)("br",{}),"DataFrame and SQL cache APIs are not supported."]})]})]})]}),"\n",(0,t.jsx)(s.admonition,{type:"note",children:(0,t.jsxs)(s.p,{children:["For the complete list of limitations, visit ",(0,t.jsx)(s.a,{href:"https://docs.databricks.com/aws/en/compute/serverless/limitations",children:"Serverless compute limitations"})," in the Databricks documentation."]})})]})}function p(e={}){const{wrapper:s}={...(0,n.R)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);