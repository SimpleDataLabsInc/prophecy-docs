"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[9818],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>f});var r=a(67294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},l=Object.keys(e);for(r=0;r<l.length;r++)a=l[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(r=0;r<l.length;r++)a=l[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var i=r.createContext({}),p=function(e){var t=r.useContext(i),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},c=function(e){var t=p(e.components);return r.createElement(i.Provider,{value:t},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,l=e.originalType,i=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=p(a),d=n,f=m["".concat(i,".").concat(d)]||m[d]||u[d]||l;return a?r.createElement(f,o(o({ref:t},c),{},{components:a})):r.createElement(f,o({ref:t},c))}));function f(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var l=a.length,o=new Array(l);o[0]=d;var s={};for(var i in t)hasOwnProperty.call(t,i)&&(s[i]=t[i]);s.originalType=e,s[m]="string"==typeof e?e:n,o[1]=s;for(var p=2;p<l;p++)o[p]=a[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,a)}d.displayName="MDXCreateElement"},85162:(e,t,a)=>{a.d(t,{Z:()=>o});var r=a(67294),n=a(86010);const l={tabItem:"tabItem_Ymn6"};function o(e){let{children:t,hidden:a,className:o}=e;return r.createElement("div",{role:"tabpanel",className:(0,n.Z)(l.tabItem,o),hidden:a},t)}},74866:(e,t,a)=>{a.d(t,{Z:()=>v});var r=a(87462),n=a(67294),l=a(86010),o=a(12466),s=a(16550),i=a(91980),p=a(67392),c=a(50012);function m(e){return function(e){return n.Children.map(e,(e=>{if((0,n.isValidElement)(e)&&"value"in e.props)return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))}(e).map((e=>{let{props:{value:t,label:a,attributes:r,default:n}}=e;return{value:t,label:a,attributes:r,default:n}}))}function u(e){const{values:t,children:a}=e;return(0,n.useMemo)((()=>{const e=t??m(a);return function(e){const t=(0,p.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function d(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function f(e){let{queryString:t=!1,groupId:a}=e;const r=(0,s.k6)(),l=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,i._X)(l),(0,n.useCallback)((e=>{if(!l)return;const t=new URLSearchParams(r.location.search);t.set(l,e),r.replace({...r.location,search:t.toString()})}),[l,r])]}function k(e){const{defaultValue:t,queryString:a=!1,groupId:r}=e,l=u(e),[o,s]=(0,n.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!d({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const r=a.find((e=>e.default))??a[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:t,tabValues:l}))),[i,p]=f({queryString:a,groupId:r}),[m,k]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[r,l]=(0,c.Nk)(a);return[r,(0,n.useCallback)((e=>{a&&l.set(e)}),[a,l])]}({groupId:r}),g=(()=>{const e=i??m;return d({value:e,tabValues:l})?e:null})();(0,n.useLayoutEffect)((()=>{g&&s(g)}),[g]);return{selectedValue:o,selectValue:(0,n.useCallback)((e=>{if(!d({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);s(e),p(e),k(e)}),[p,k,l]),tabValues:l}}var g=a(72389);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function h(e){let{className:t,block:a,selectedValue:s,selectValue:i,tabValues:p}=e;const c=[],{blockElementScrollPositionUntilNextRender:m}=(0,o.o5)(),u=e=>{const t=e.currentTarget,a=c.indexOf(t),r=p[a].value;r!==s&&(m(t),i(r))},d=e=>{let t=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{const a=c.indexOf(e.currentTarget)+1;t=c[a]??c[0];break}case"ArrowLeft":{const a=c.indexOf(e.currentTarget)-1;t=c[a]??c[c.length-1];break}}t?.focus()};return n.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,l.Z)("tabs",{"tabs--block":a},t)},p.map((e=>{let{value:t,label:a,attributes:o}=e;return n.createElement("li",(0,r.Z)({role:"tab",tabIndex:s===t?0:-1,"aria-selected":s===t,key:t,ref:e=>c.push(e),onKeyDown:d,onClick:u},o,{className:(0,l.Z)("tabs__item",b.tabItem,o?.className,{"tabs__item--active":s===t})}),a??t)})))}function y(e){let{lazy:t,children:a,selectedValue:r}=e;if(a=Array.isArray(a)?a:[a],t){const e=a.find((e=>e.props.value===r));return e?(0,n.cloneElement)(e,{className:"margin-top--md"}):null}return n.createElement("div",{className:"margin-top--md"},a.map(((e,t)=>(0,n.cloneElement)(e,{key:t,hidden:e.props.value!==r}))))}function N(e){const t=k(e);return n.createElement("div",{className:(0,l.Z)("tabs-container",b.tabList)},n.createElement(h,(0,r.Z)({},e,t)),n.createElement(y,(0,r.Z)({},e,t)))}function v(e){const t=(0,g.Z)();return n.createElement(N,(0,r.Z)({key:String(t)},e))}},24311:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>f,frontMatter:()=>s,metadata:()=>p,toc:()=>m});var r=a(87462),n=(a(67294),a(3905)),l=a(74866),o=a(85162);const s={title:"Kafka",id:"kafka",description:"Reading and writing data from Apache Kafka in batch mode",sidebar_position:9,tags:["gems","file","kafka"]},i=void 0,p={unversionedId:"low-code-spark/gems/source-target/file/kafka",id:"low-code-spark/gems/source-target/file/kafka",title:"Kafka",description:"Reading and writing data from Apache Kafka in batch mode",source:"@site/docs/low-code-spark/gems/source-target/file/kafka-stream.md",sourceDirName:"low-code-spark/gems/source-target/file",slug:"/low-code-spark/gems/source-target/file/kafka",permalink:"/low-code-spark/gems/source-target/file/kafka",draft:!1,tags:[{label:"gems",permalink:"/tags/gems"},{label:"file",permalink:"/tags/file"},{label:"kafka",permalink:"/tags/kafka"}],version:"current",sidebarPosition:9,frontMatter:{title:"Kafka",id:"kafka",description:"Reading and writing data from Apache Kafka in batch mode",sidebar_position:9,tags:["gems","file","kafka"]},sidebar:"defaultSidebar",previous:{title:"Fixed Format",permalink:"/low-code-spark/gems/source-target/file/fixed-format"},next:{title:"XLSX (Excel)",permalink:"/low-code-spark/gems/source-target/file/xlsx"}},c={},m=[{value:"Source",id:"source",level:2},{value:"Source Parameters",id:"source-parameters",level:3},{value:"Example",id:"source-example",level:3},{value:"Generated Code",id:"source-code",level:3},{value:"Target",id:"target",level:2},{value:"Target Parameters",id:"target-parameters",level:3},{value:"Example",id:"target-example",level:3},{value:"Generated Code",id:"target-code",level:3},{value:"Example Pipelines",id:"example-pipelines",level:2},{value:"Source Pipeline Example",id:"source-pipeline-example",level:3},{value:"Metadata Table",id:"metadata-table",level:4},{value:"Spark Code used for script component",id:"spark-code-used-for-script-component",level:4}],u={toc:m},d="wrapper";function f(e){let{components:t,...s}=e;return(0,n.kt)(d,(0,r.Z)({},u,s,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://kafka.apache.org/"},"Apache Kafka")," is an open-source distributed event streaming platform. Supporting a number of streaming paradigms it's used by thousands of companies and organizations in scenarios including Data Ingestion, Analytics and more."),(0,n.kt)("p",null,"This source currently connects with Kafka Brokers in ",(0,n.kt)("strong",{parentName:"p"},"Batch")," mode."),(0,n.kt)("h2",{id:"source"},"Source"),(0,n.kt)("p",null,"Reads data from Kafka stream in batch mode. Data is read only incrementally from the last offset stored in the specified Metadata table. If the Metadata table is not present, then data will be read from the ",(0,n.kt)("inlineCode",{parentName:"p"},"earliest")," offset."),(0,n.kt)("h3",{id:"source-parameters"},"Source Parameters"),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",{parentName:"tr",align:"left"},"Parameter"),(0,n.kt)("th",{parentName:"tr",align:"left"},"Description"),(0,n.kt)("th",{parentName:"tr",align:"left"},"Required"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"Broker List"),(0,n.kt)("td",{parentName:"tr",align:"left"},"Comma separated list of Kafka brokers"),(0,n.kt)("td",{parentName:"tr",align:"left"},"True")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"Group Id"),(0,n.kt)("td",{parentName:"tr",align:"left"},"Kafka consumer group ID"),(0,n.kt)("td",{parentName:"tr",align:"left"},"True")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"Session Timeout"),(0,n.kt)("td",{parentName:"tr",align:"left"},"Session timeout for Kafka. (Default value set to 6000s)"),(0,n.kt)("td",{parentName:"tr",align:"left"},"False")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"Security Protocol"),(0,n.kt)("td",{parentName:"tr",align:"left"},"Security protocol for Kafka (Default value set to SASL_SSL)"),(0,n.kt)("td",{parentName:"tr",align:"left"},"True")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"SASL Mechanism"),(0,n.kt)("td",{parentName:"tr",align:"left"},"Default SASL Mechanism for SASL_SSL (Default value set to SCRAM-SHA-256)"),(0,n.kt)("td",{parentName:"tr",align:"left"},"True")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"Credential Type"),(0,n.kt)("td",{parentName:"tr",align:"left"},"Credential Type provider (Databricks Secrets or Username/Password)"),(0,n.kt)("td",{parentName:"tr",align:"left"},"True")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"Credential Scope"),(0,n.kt)("td",{parentName:"tr",align:"left"},"Scope to use for Databricks secrets"),(0,n.kt)("td",{parentName:"tr",align:"left"},"True")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"Kafka Topic"),(0,n.kt)("td",{parentName:"tr",align:"left"},"Comma separated list of Kafka topics"),(0,n.kt)("td",{parentName:"tr",align:"left"},"True")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"Metadata Table"),(0,n.kt)("td",{parentName:"tr",align:"left"},"Table name which would be used to store offsets for each topic, partition"),(0,n.kt)("td",{parentName:"tr",align:"left"},"True")))),(0,n.kt)("h3",{id:"source-example"},"Example"),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Example usage of Filter",src:a(72673).Z,width:"3024",height:"1590"})),(0,n.kt)("h3",{id:"source-code"},"Generated Code"),(0,n.kt)(l.Z,{mdxType:"Tabs"},(0,n.kt)(o.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-py"},'def KafkaSource(spark: SparkSession) -> DataFrame:\n    from delta.tables import DeltaTable\n    import json\n    from pyspark.dbutils import DBUtils\n\n    if spark.catalog._jcatalog.tableExists(f"metadata.kafka_offsets"):\n        offset_dict = {}\n\n        for row in DeltaTable.forName(spark, f"metadata.kafka_offsets").toDF().collect():\n            if row["topic"] in offset_dict.keys():\n                offset_dict[row["topic"]].update({row["partition"] : row["max_offset"] + 1})\n            else:\n                offset_dict[row["topic"]] = {row["partition"] : row["max_offset"] + 1}\n\n        return (spark.read\\\n            .format("kafka")\\\n            .options(\n              **{\n                "kafka.sasl.jaas.config": (\n                  f"kafkashaded.org.apache.kafka.common.security.scram.ScramLoginModule"\n                  + f\' required username="{DBUtils(spark).secrets.get(scope = "test", key = "username")}" password="{DBUtils(spark).secrets.get(scope = "test", key = "password")}";\'\n                ),\n                "kafka.sasl.mechanism": "SCRAM-SHA-256",\n                "kafka.security.protocol": "SASL_SSL",\n                "kafka.bootstrap.servers": "broker1.aws.com:9094,broker2.aws.com:9094",\n                "kafka.session.timeout.ms": "6000",\n                "group.id": "group_id_1",\n                "subscribe": "my_first_topic,my_second_topic",\n                "startingOffsets": json.dumps(offset_dict),\n              }\n            )\\\n            .load()\\\n            .withColumn("value", col("value").cast("string"))\\\n            .withColumn("key", col("key").cast("string")))\n    else:\n        return (spark.read\\\n            .format("kafka")\\\n            .options(\n              **{\n                "kafka.sasl.jaas.config": (\n                  f"kafkashaded.org.apache.kafka.common.security.scram.ScramLoginModule"\n                  + f\' required username="{DBUtils(spark).secrets.get(scope = "test", key = "username")}" password="{DBUtils(spark).secrets.get(scope = "test", key = "password")}";\'\n                ),\n                "kafka.sasl.mechanism": "SCRAM-SHA-256",\n                "kafka.security.protocol": "SASL_SSL",\n                "kafka.bootstrap.servers": "broker1.aws.com:9094,broker2.aws.com:9094",\n                "kafka.session.timeout.ms": "6000",\n                "group.id": "group_id_1",\n                "subscribe": "my_first_topic,my_second_topic"\n              }\n            )\\\n            .load()\\\n            .withColumn("value", col("value").cast("string"))\\\n            .withColumn("key", col("key").cast("string")))\n\n'))),(0,n.kt)(o.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},"Coming Soon\n")))),(0,n.kt)("hr",null),(0,n.kt)("h2",{id:"target"},"Target"),(0,n.kt)("p",null,"Writes each row from the Dataframe to Kafka topic(s) as JSON messages."),(0,n.kt)("h3",{id:"target-parameters"},"Target Parameters"),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",{parentName:"tr",align:"left"},"Parameter"),(0,n.kt)("th",{parentName:"tr",align:"left"},"Description"),(0,n.kt)("th",{parentName:"tr",align:"left"},"Required"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"Broker List"),(0,n.kt)("td",{parentName:"tr",align:"left"},"Comma separated list of Kafka brokers"),(0,n.kt)("td",{parentName:"tr",align:"left"},"True")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"Security Protocol"),(0,n.kt)("td",{parentName:"tr",align:"left"},"Security protocol for Kafka (Default value set to SASL_SSL)"),(0,n.kt)("td",{parentName:"tr",align:"left"},"True")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"SASL Mechanism"),(0,n.kt)("td",{parentName:"tr",align:"left"},"Default SASL Mechanism for SASL_SSL (Default value set to SCRAM-SHA-256)"),(0,n.kt)("td",{parentName:"tr",align:"left"},"True")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"Credential Type"),(0,n.kt)("td",{parentName:"tr",align:"left"},"Credential Type provider (Databricks Secrets or Username/Password)"),(0,n.kt)("td",{parentName:"tr",align:"left"},"True")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"Credential Scope"),(0,n.kt)("td",{parentName:"tr",align:"left"},"Scope to use for Databricks secrets"),(0,n.kt)("td",{parentName:"tr",align:"left"},"True")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"Kafka Topic"),(0,n.kt)("td",{parentName:"tr",align:"left"},"Comma separated list of Kafka topics"),(0,n.kt)("td",{parentName:"tr",align:"left"},"True")))),(0,n.kt)("h3",{id:"target-example"},"Example"),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Example usage of Filter",src:a(24200).Z,width:"3042",height:"1452"})),(0,n.kt)("h3",{id:"target-code"},"Generated Code"),(0,n.kt)(l.Z,{mdxType:"Tabs"},(0,n.kt)(o.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-py"},'def KafkaTarget(spark: SparkSession, in0: DataFrame):\n    df1 = in0.select(to_json(struct("*")).alias("value"))\n    df2 = df1.selectExpr("CAST(value AS STRING)")\n    df2.write\\\n        .format("kafka")\\\n        .options(\n          **{\n            "kafka.sasl.jaas.config": (\n              f"kafkashaded.org.apache.kafka.common.security.scram.ScramLoginModule"\n              + f\' required username="{DBUtils(spark).secrets.get(scope = "test", key = "username")}" password="{DBUtils(spark).secrets.get(scope = "test", key = "password")}";\'\n            ),\n            "kafka.sasl.mechanism": "SCRAM-SHA-256",\n            "kafka.security.protocol": "SASL_SSL",\n            "kafka.bootstrap.servers": "broker1.aws.com:9094,broker2.aws.com:9094",\n            "topic": "my_first_topic,my_second_topic",\n          }\n        )\\\n        .save()\n\n'))),(0,n.kt)(o.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},"Coming Soon\n\n")))),(0,n.kt)("h2",{id:"example-pipelines"},"Example Pipelines"),(0,n.kt)("h3",{id:"source-pipeline-example"},"Source Pipeline Example"),(0,n.kt)("p",null,"In this example we'll read JSON messages from Kafka, parse them, remove any null messagesand then finally persist it to a Delta table."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Example usage of Filter",src:a(44379).Z,width:"1139",height:"584"})),(0,n.kt)("h4",{id:"metadata-table"},"Metadata Table"),(0,n.kt)("p",null,"In order to avoid reprocessing messages on subsequent Pipeline runs, we're going to update a certain table with the last processed offsets for each Kafka partition and topic. The next time the Pipeline runs this table will be used to only get a batch of messages that have arrived since the previously-processed offset."),(0,n.kt)("p",null,"For this example, we're going to update ",(0,n.kt)("inlineCode",{parentName:"p"},"metadata.kafka_offsets"),", which has the following structure:"),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",{parentName:"tr",align:"left"},"topic"),(0,n.kt)("th",{parentName:"tr",align:"left"},"partition"),(0,n.kt)("th",{parentName:"tr",align:"left"},"max_offset"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"my_first_topic"),(0,n.kt)("td",{parentName:"tr",align:"left"},"0"),(0,n.kt)("td",{parentName:"tr",align:"left"},"10")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"my_first_topic"),(0,n.kt)("td",{parentName:"tr",align:"left"},"1"),(0,n.kt)("td",{parentName:"tr",align:"left"},"5")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"my_second_topic"),(0,n.kt)("td",{parentName:"tr",align:"left"},"0"),(0,n.kt)("td",{parentName:"tr",align:"left"},"10")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:"left"},"my_second_topic"),(0,n.kt)("td",{parentName:"tr",align:"left"},"1"),(0,n.kt)("td",{parentName:"tr",align:"left"},"5")))),(0,n.kt)("p",null,"Taking this approach gives us the following benefits:"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"Build the Pipeline interactively without committing any offsets"),(0,n.kt)("li",{parentName:"ol"},"Production workflows will only consume messages that have arrived since the previously-processed offset"),(0,n.kt)("li",{parentName:"ol"},"We can replay old messages by modifying the Metadata table")),(0,n.kt)("admonition",{type:"note"},(0,n.kt)("p",{parentName:"admonition"},"For production workflows the ",(0,n.kt)("a",{parentName:"p",href:"/concepts/project/gems#phase"},"Phase")," for the ",(0,n.kt)("inlineCode",{parentName:"p"},"Script")," Gem that updates the offsets should be greater than the Phase of the Target Gem.\nThis is to ensure that offsets are only updated in the table after data is safely persisted to the Target.")),(0,n.kt)("h4",{id:"spark-code-used-for-script-component"},"Spark Code used for script component"),(0,n.kt)(l.Z,{mdxType:"Tabs"},(0,n.kt)(o.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-py"},'def UpdateOffsets(spark: SparkSession, in0: DataFrame):\n\n    if not ("SColumnExpression" in locals()):\n        from delta.tables import DeltaTable\n        import pyspark.sql.functions as f\n        metadataTable = "metadata.kafka_offsets"\n        metaDataDf = in0.groupBy("partition", "topic").agg(f.max(f.col("`offset`").cast("int")).alias("max_offset"))\n\n        if not spark.catalog._jcatalog.tableExists(metadataTable):\n            metaDataDf.write.format("delta").mode("overwrite").saveAsTable(metadataTable)\n        else:\n            DeltaTable\\\n                .forName(spark, metadataTable)\\\n                .alias("target")\\\n                .merge(\n                  metaDataDf.alias("source"),\n                  (\n                    (col("source.`partition`") == col("target.`partition`"))\n                    & (col("source.`topic`") == col("target.`topic`"))\n                  )\n                )\\\n                .whenMatchedUpdateAll()\\\n                .whenNotMatchedInsertAll()\\\n                .execute()\n\n'))),(0,n.kt)(o.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-scala"},"Coming Soon\n\n")))))}f.isMDXComponent=!0},44379:(e,t,a)=>{a.d(t,{Z:()=>r});const r=a.p+"assets/images/kafka_pipeline_eg-f97290ea76491916f47acf312a5ea95b.gif"},72673:(e,t,a)=>{a.d(t,{Z:()=>r});const r=a.p+"assets/images/kafka_source_eg_1-5d9b36695526379b9a62cf152b6170bf.png"},24200:(e,t,a)=>{a.d(t,{Z:()=>r});const r=a.p+"assets/images/kafka_target_eg_1-498e2012164f661686168fbd52aebaaf.png"}}]);