"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[42437],{15680:(e,t,n)=>{n.d(t,{xA:()=>d,yg:()=>g});var a=n(96540);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},d=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},m="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),m=p(n),u=r,g=m["".concat(s,".").concat(u)]||m[u]||c[u]||o;return n?a.createElement(g,i(i({ref:t},d),{},{components:n})):a.createElement(g,i({ref:t},d))}));function g(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[m]="string"==typeof e?e:r,i[1]=l;for(var p=2;p<o;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},19365:(e,t,n)=>{n.d(t,{A:()=>i});var a=n(96540),r=n(20053);const o={tabItem:"tabItem_Ymn6"};function i(e){let{children:t,hidden:n,className:i}=e;return a.createElement("div",{role:"tabpanel",className:(0,r.A)(o.tabItem,i),hidden:n},t)}},11470:(e,t,n)=>{n.d(t,{A:()=>C});var a=n(58168),r=n(96540),o=n(20053),i=n(23104),l=n(56347),s=n(57485),p=n(31682),d=n(89466);function m(e){return function(e){return r.Children.map(e,(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:n,attributes:a,default:r}}=e;return{value:t,label:n,attributes:a,default:r}}))}function c(e){const{values:t,children:n}=e;return(0,r.useMemo)((()=>{const e=t??m(n);return function(e){const t=(0,p.X)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function u(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function g(e){let{queryString:t=!1,groupId:n}=e;const a=(0,l.W6)(),o=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,s.aZ)(o),(0,r.useCallback)((e=>{if(!o)return;const t=new URLSearchParams(a.location.search);t.set(o,e),a.replace({...a.location,search:t.toString()})}),[o,a])]}function y(e){const{defaultValue:t,queryString:n=!1,groupId:a}=e,o=c(e),[i,l]=(0,r.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!u({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:t,tabValues:o}))),[s,p]=g({queryString:n,groupId:a}),[m,y]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[a,o]=(0,d.Dv)(n);return[a,(0,r.useCallback)((e=>{n&&o.set(e)}),[n,o])]}({groupId:a}),h=(()=>{const e=s??m;return u({value:e,tabValues:o})?e:null})();(0,r.useLayoutEffect)((()=>{h&&l(h)}),[h]);return{selectedValue:i,selectValue:(0,r.useCallback)((e=>{if(!u({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);l(e),p(e),y(e)}),[p,y,o]),tabValues:o}}var h=n(92303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function b(e){let{className:t,block:n,selectedValue:l,selectValue:s,tabValues:p}=e;const d=[],{blockElementScrollPositionUntilNextRender:m}=(0,i.a_)(),c=e=>{const t=e.currentTarget,n=d.indexOf(t),a=p[n].value;a!==l&&(m(t),s(a))},u=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const n=d.indexOf(e.currentTarget)+1;t=d[n]??d[0];break}case"ArrowLeft":{const n=d.indexOf(e.currentTarget)-1;t=d[n]??d[d.length-1];break}}t?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.A)("tabs",{"tabs--block":n},t)},p.map((e=>{let{value:t,label:n,attributes:i}=e;return r.createElement("li",(0,a.A)({role:"tab",tabIndex:l===t?0:-1,"aria-selected":l===t,key:t,ref:e=>d.push(e),onKeyDown:u,onClick:c},i,{className:(0,o.A)("tabs__item",f.tabItem,i?.className,{"tabs__item--active":l===t})}),n??t)})))}function N(e){let{lazy:t,children:n,selectedValue:a}=e;const o=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=o.find((e=>e.props.value===a));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},o.map(((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==a}))))}function S(e){const t=y(e);return r.createElement("div",{className:(0,o.A)("tabs-container",f.tabList)},r.createElement(b,(0,a.A)({},e,t)),r.createElement(N,(0,a.A)({},e,t)))}function C(e){const t=(0,h.A)();return r.createElement(S,(0,a.A)({key:String(t)},e))}},5987:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>s,default:()=>g,frontMatter:()=>l,metadata:()=>p,toc:()=>m});var a=n(58168),r=(n(96540),n(15680)),o=n(11470),i=n(19365);const l={title:"Gem Builder reference for Spark",id:"gem-builder-reference",description:"Detailed explanation of custom gem code structure",sidebar_label:"Reference for Spark",tags:["gem builder"]},s=void 0,p={unversionedId:"extensibility/gem-builder/gem-builder-reference",id:"extensibility/gem-builder/gem-builder-reference",title:"Gem Builder reference for Spark",description:"Detailed explanation of custom gem code structure",source:"@site/docs/extensibility/gem-builder/gem-builder-reference.md",sourceDirName:"extensibility/gem-builder",slug:"/extensibility/gem-builder/gem-builder-reference",permalink:"/extensibility/gem-builder/gem-builder-reference",draft:!1,tags:[{label:"gem builder",permalink:"/tags/gem-builder"}],version:"current",frontMatter:{title:"Gem Builder reference for Spark",id:"gem-builder-reference",description:"Detailed explanation of custom gem code structure",sidebar_label:"Reference for Spark",tags:["gem builder"]},sidebar:"mySidebar",previous:{title:"Gem Builder for Spark",permalink:"/extensibility/gem-builder/spark-gem-builder"},next:{title:"Optimization functions",permalink:"/extensibility/gem-builder/optimization-functions"}},d={},m=[{value:"Requirements",id:"requirements",level:2},{value:"Mode",id:"mode",level:2},{value:"Classes",id:"classes",level:2},{value:"Functions",id:"functions",level:2},{value:"Examples",id:"examples",level:2},{value:"Parent Class",id:"parent-class",level:3},{value:"Properties Classes",id:"properties-classes",level:3},{value:"Dialog (UI)",id:"dialog-ui",level:3},{value:"Validation",id:"validation",level:3},{value:"State Changes",id:"state-changes",level:3},{value:"Component Code",id:"component-code",level:3},{value:"Dataset Format example",id:"dataset-format-example",level:3}],c={toc:m},u="wrapper";function g(e){let{components:t,...l}=e;return(0,r.yg)(u,(0,a.A)({},c,l,{components:t,mdxType:"MDXLayout"}),(0,r.yg)("p",null,"This page provides information about how gems are written in code. Reference this page when you are building or editing custom gems."),(0,r.yg)("h2",{id:"requirements"},"Requirements"),(0,r.yg)("p",null,"Some options require a specific ",(0,r.yg)("strong",{parentName:"p"},"gemLibsVersion"),". To update this, you must manually change the ",(0,r.yg)("inlineCode",{parentName:"p"},"gemLibsVersion")," value inside ",(0,r.yg)("strong",{parentName:"p"},"pbt_project.yml")," in your project Git repository."),(0,r.yg)("h2",{id:"mode"},"Mode"),(0,r.yg)("p",null,"There are a few different types of gems that you can create. The table below describes each mode you can choose."),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Mode"),(0,r.yg)("th",{parentName:"tr",align:null},"Description"),(0,r.yg)("th",{parentName:"tr",align:null},"Additional settings"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"Transformation"),(0,r.yg)("td",{parentName:"tr",align:null},"Edits intermediate data in the pipeline that is in-memory."),(0,r.yg)("td",{parentName:"tr",align:null},"Choose the ",(0,r.yg)("strong",{parentName:"td"},"category")," of the transformation gem")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"Dataset Format"),(0,r.yg)("td",{parentName:"tr",align:null},"Reads and writes data between storage and memory."),(0,r.yg)("td",{parentName:"tr",align:null},"Choose whether the type is ",(0,r.yg)("strong",{parentName:"td"},"batch")," or ",(0,r.yg)("strong",{parentName:"td"},"streaming"))),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"Custom Subgraph (",(0,r.yg)("strong",{parentName:"td"},"Python only"),")"),(0,r.yg)("td",{parentName:"tr",align:null},"Controls the flow of gems. Visit the ",(0,r.yg)("a",{parentName:"td",href:"/Spark/gems/subgraph/"},"Subgraph")," page for an example."),(0,r.yg)("td",{parentName:"tr",align:null},"None")))),(0,r.yg)("h2",{id:"classes"},"Classes"),(0,r.yg)("p",null,"The following classes must be included in all Spark gems. Each class extends a base class that Prophecy has defined."),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"A class where you inherit the representation of the overall gem."),(0,r.yg)("li",{parentName:"ul"},"A class that contains the properties to be made available to the user for this particular gem."),(0,r.yg)("li",{parentName:"ul"},"A class that defines the Spark code that needs to run on your Spark cluster.")),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Class"),(0,r.yg)("th",{parentName:"tr",align:null},"Base Class for Transformation"),(0,r.yg)("th",{parentName:"tr",align:null},"Base Class for Dataset Format"),(0,r.yg)("th",{parentName:"tr",align:null},"Base Class for Custom Subgraph"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"class CustomGem(BaseClass)"),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"ComponentSpec")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"DatasetSpec")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"MetaComponentSpec"))),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"class YourProperties(BaseClass)"),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"ComponentProperties")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"ComponentProperties")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"MetaComponentProperties"))),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"class YourCode(BaseClass)"),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"ComponentCode")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"ComponentCode")),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"MetaComponentCode"))))),(0,r.yg)("h2",{id:"functions"},"Functions"),(0,r.yg)("p",null,"The following functions can be used to customize Spark gems."),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Function"),(0,r.yg)("th",{parentName:"tr",align:null},"Purpose"),(0,r.yg)("th",{parentName:"tr",align:null},"Return"),(0,r.yg)("th",{parentName:"tr",align:null},"Gem Mode"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"optimizeCode")),(0,r.yg)("td",{parentName:"tr",align:null},"Enables the Prophecy optimizer to simplify the gem code when it runs."),(0,r.yg)("td",{parentName:"tr",align:null},"Boolean"),(0,r.yg)("td",{parentName:"tr",align:null},"All")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"customOutputSchemaEnabled")),(0,r.yg)("td",{parentName:"tr",align:null},"Enables the ",(0,r.yg)("a",{parentName:"td",href:"/concepts/project/gems#output-ports"},"custom schema")," option by default in the gem. Requires gemLibsVersion 1.1.47+ for Scala."),(0,r.yg)("td",{parentName:"tr",align:null},"Boolean"),(0,r.yg)("td",{parentName:"tr",align:null},"Transformation")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"dialog")),(0,r.yg)("td",{parentName:"tr",align:null},"Defines how you want the gem to look like in the visual interface."),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"Dialog")," object"),(0,r.yg)("td",{parentName:"tr",align:null},"Transformation and Subgraph")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"sourceDialog")),(0,r.yg)("td",{parentName:"tr",align:null},"Defines how you want the source gem to look like in the visual interface."),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"DatasetDialog")," object"),(0,r.yg)("td",{parentName:"tr",align:null},"Dataset and Subgraph")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"targetDialog")),(0,r.yg)("td",{parentName:"tr",align:null},"Defines how you want the target gem to look like in the visual interface."),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"DatasetDialog")," object"),(0,r.yg)("td",{parentName:"tr",align:null},"Dataset and Subgraph")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"validate")),(0,r.yg)("td",{parentName:"tr",align:null},"Defines how to detect user errors when using the gem."),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"Diagnostics")," array"),(0,r.yg)("td",{parentName:"tr",align:null},"All")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"onChange")),(0,r.yg)("td",{parentName:"tr",align:null},"Define UI state transformations."),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"Properties")," object"),(0,r.yg)("td",{parentName:"tr",align:null},"All")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"serializeProperty")),(0,r.yg)("td",{parentName:"tr",align:null},"(",(0,r.yg)("strong",{parentName:"td"},"Scala only"),") Takes a Properties object and converts it into JSON format."),(0,r.yg)("td",{parentName:"tr",align:null},"String"),(0,r.yg)("td",{parentName:"tr",align:null},"All")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"deserializeProperty")),(0,r.yg)("td",{parentName:"tr",align:null},"(",(0,r.yg)("strong",{parentName:"td"},"Scala only"),") Parses a JSON string and converts it into a Properties object."),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"Properties")," object"),(0,r.yg)("td",{parentName:"tr",align:null},"All")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"apply")),(0,r.yg)("td",{parentName:"tr",align:null},"Included in the class that extends ",(0,r.yg)("a",{parentName:"td",href:"#component-code"},"component code")," to define Spark logic."),(0,r.yg)("td",{parentName:"tr",align:null},"None, DataFrame, or list of DataFrames"),(0,r.yg)("td",{parentName:"tr",align:null},"Transformation and Subgraph")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"sourceApply")),(0,r.yg)("td",{parentName:"tr",align:null},"Included in the class that extends ",(0,r.yg)("a",{parentName:"td",href:"#component-code"},"component code")," to define Spark logic."),(0,r.yg)("td",{parentName:"tr",align:null},"DataFrame"),(0,r.yg)("td",{parentName:"tr",align:null},"Dataset")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"targetApply")),(0,r.yg)("td",{parentName:"tr",align:null},"Included in the class that extends ",(0,r.yg)("a",{parentName:"td",href:"#component-code"},"component code")," to define Spark logic."),(0,r.yg)("td",{parentName:"tr",align:null},"None"),(0,r.yg)("td",{parentName:"tr",align:null},"Dataset")))),(0,r.yg)("h2",{id:"examples"},"Examples"),(0,r.yg)("h3",{id:"parent-class"},"Parent Class"),(0,r.yg)(o.A,{mdxType:"Tabs"},(0,r.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'class Filter(ComponentSpec):\nname: str = "Filter"\n    category: str = "Transform"\n    def optimizeCode(self) -> bool:\n        return True\n'))),(0,r.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},'object Filter extends ComponentSpec {\nval name: String = "Filter"\nval category: String = "Transform"\noverride def optimizeCode: Boolean = true\n')))),(0,r.yg)("h3",{id:"properties-classes"},"Properties Classes"),(0,r.yg)(o.A,{mdxType:"Tabs"},(0,r.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'@dataclass(frozen=True)\n    class FilterProperties(ComponentProperties):\n        columnsSelector: List[str] = field(default_factory=list)\n        condition: SColumn = SColumn("lit(True)")\n'))),(0,r.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},'    case class FilterProperties(\n    @Property("Columns selector")\n    columnsSelector: List[String] = Nil,\n    @Property("Filter", "Predicate expression to filter rows of incoming dataframe")\n    condition: SColumn = SColumn("lit(true)")\n  ) extends ComponentProperties\n\n')))),(0,r.yg)("h3",{id:"dialog-ui"},"Dialog (UI)"),(0,r.yg)(o.A,{mdxType:"Tabs"},(0,r.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'def dialog(self) -> Dialog:\n        return Dialog("Filter").addElement(\n            ColumnsLayout(height="100%")\n                .addColumn(PortSchemaTabs(selectedFieldsProperty=("columnsSelector")).importSchema(), "2fr")\n                .addColumn(StackLayout(height=("100%"))\n                .addElement(TitleElement("Filter Condition"))\n                .addElement(\n                Editor(height=("100%")).withSchemaSuggestions().bindProperty("condition.expression")\n            ), "5fr"))\n'))),(0,r.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},'def dialog: Dialog = Dialog("Filter")\n    .addElement(\n      ColumnsLayout(height = Some("100%"))\n        .addColumn(\n          PortSchemaTabs(selectedFieldsProperty = Some("columnsSelector")).importSchema(),\n          "2fr"\n        )\n        .addColumn(\n          StackLayout(height = Some("100%"))\n            .addElement(TitleElement("Filter Condition"))\n            .addElement(\n              Editor(height = Some("100%"))\n                .withSchemaSuggestions()\n                .bindProperty("condition.expression")\n            ),\n          "5fr"\n        )\n    )\n')))),(0,r.yg)("p",null,"After the Dialog object is defined, it is serialized as JSON and rendered in the UI. When you preview this visual interface of the example code above, it appears like this:"),(0,r.yg)("p",null,(0,r.yg)("img",{alt:"Dialog",src:n(15354).A,width:"1636",height:"846"})),(0,r.yg)("p",null,"Various UI components can be added to this function such as scroll boxes, tabs, buttons, and more. You can also group these components into different panels."),(0,r.yg)("p",null,"Column Selector: This is a special property that you should add if you want to select the columns from UI and then highlight the used columns using the ",(0,r.yg)("inlineCode",{parentName:"p"},"onChange")," function."),(0,r.yg)("h3",{id:"validation"},"Validation"),(0,r.yg)(o.A,{mdxType:"Tabs"},(0,r.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'def validate(self, component: Component[FilterProperties]) -> List[Diagnostic]:\n        return validateSColumn(component.properties.condition, "condition", component)\n\n'))),(0,r.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},'def validate(component: Component)(implicit context: WorkflowContext): List[Diagnostic] = {\n    val diagnostics =\n      validateSColumn(component.properties.condition, "condition", component)\n    diagnostics.toList\n  }\n')))),(0,r.yg)("h3",{id:"state-changes"},"State Changes"),(0,r.yg)(o.A,{mdxType:"Tabs"},(0,r.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},"def onChange(self, oldState: Component[FilterProperties], newState: Component[FilterProperties]) -> Component[\n        FilterProperties]:\n        newProps = newState.properties\n        usedColExps = getColumnsToHighlight2([newProps.condition], newState)\n        return newState.bindProperties(replace(newProps, columnsSelector=usedColExps))\n\n"))),(0,r.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},"def onChange(oldState: Component, newState: Component)(implicit context: WorkflowContext): Component = {\n    val newProps = newState.properties\n    val portId = newState.ports.inputs.head.id\n\n    val expressions = getColumnsToHighlight(List(newProps.condition), newState)\n\n    newState.copy(properties = newProps.copy(columnsSelector = expressions))\n  }\n")))),(0,r.yg)("h3",{id:"component-code"},"Component Code"),(0,r.yg)(o.A,{mdxType:"Tabs"},(0,r.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},"class FilterCode(ComponentCode):\ndef __init__(self, newProps):\nself.props: Filter.FilterProperties = newProps\n\n    def apply(self, spark: SparkSession, in0: DataFrame) -> DataFrame:\n            return in0.filter(self.props.condition.column())\n"))),(0,r.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},"class FilterCode(props: PropertiesType)(implicit context: WorkflowContext) extends ComponentCode {\n\n    def apply(spark: SparkSession, in: DataFrame): DataFrame = {\n      val out = in.filter(props.condition.column)\n      out\n    }\n\n  }\n")))),(0,r.yg)("p",null,"If you want to test your Spark code, you can modify properties in the visual preview and save the changes. Then, you can see the generated Spark code which will eventually run on your cluster."),(0,r.yg)("admonition",{type:"info"},(0,r.yg)("p",{parentName:"admonition"},"To keep gems generally compatible with each other, they must conform to a common interface. Therefore, as defined in the ",(0,r.yg)("inlineCode",{parentName:"p"},"apply()")," method, gems must accept and produce ",(0,r.yg)("strong",{parentName:"p"},"DataFrame objects")," at the input and output ports.")),(0,r.yg)("admonition",{type:"note"},(0,r.yg)("p",{parentName:"admonition"},"To assist the Spark Catalyst Optimizer to build scalable code, Prophecy performs some minor optimizations to the code\ngenerated by the ",(0,r.yg)("inlineCode",{parentName:"p"},"apply()")," method.")),(0,r.yg)("h3",{id:"dataset-format-example"},"Dataset Format example"),(0,r.yg)("p",null,"The previous examples were for Transformation gems. The following example is the code for a Dataset Format gem."),(0,r.yg)(o.A,{mdxType:"Tabs"},(0,r.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-py"},'from pyspark.sql import SparkSession, DataFrame\nfrom pyspark.sql.types import StructType\n\nfrom prophecy.cb.server.base.ComponentBuilderBase import ComponentCode, Diagnostic, SeverityLevelEnum\nfrom prophecy.cb.server.base.DatasetBuilderBase import DatasetSpec, DatasetProperties, Component\nfrom prophecy.cb.ui.uispec import *\n\n\nclass ParquetFormat(DatasetSpec):\n    name: str = "parquet"\n    datasetType: str = "File"\n\n    def optimizeCode(self) -> bool:\n        return True\n\n    @dataclass(frozen=True)\n    class ParquetProperties(DatasetProperties):\n        schema: Optional[StructType] = None\n        description: Optional[str] = ""\n        useSchema: Optional[bool] = False\n        path: str = ""\n        mergeSchema: Optional[bool] = None\n        datetimeRebaseMode: Optional[str] = None\n        int96RebaseMode: Optional[str] = None\n        compression: Optional[str] = None\n        partitionColumns: Optional[List[str]] = None\n        writeMode: Optional[str] = None\n        pathGlobFilter: Optional[str] = None\n        modifiedBefore: Optional[str] = None\n        modifiedAfter: Optional[str] = None\n        recursiveFileLookup: Optional[bool] = None\n\n    def sourceDialog(self) -> DatasetDialog:\n        return DatasetDialog("parquet") \\\n            .addSection("LOCATION", TargetLocation("path")) \\\n            .addSection(\n            "PROPERTIES",\n            ColumnsLayout(gap=("1rem"), height=("100%"))\n                .addColumn(\n                ScrollBox().addElement(\n                    StackLayout(height=("100%"))\n                        .addElement(\n                        StackItem(grow=(1)).addElement(\n                            FieldPicker(height=("100%"))\n                                .addField(\n                                TextArea("Description", 2, placeholder="Dataset description..."),\n                                "description",\n                                True\n                            )\n                                .addField(Checkbox("Use user-defined schema"), "useSchema", True)\n                                .addField(Checkbox("Merge schema"), "mergeSchema")\n                                .addField(\n                                SelectBox("Datetime Rebase Mode")\n                                    .addOption("EXCEPTION", "EXCEPTION")\n                                    .addOption("CORRECTED", "CORRECTED")\n                                    .addOption("LEGACY", "LEGACY"),\n                                "datetimeRebaseMode"\n                            )\n                                .addField(\n                                SelectBox("Int96 Rebase Mode")\n                                    .addOption("EXCEPTION", "EXCEPTION")\n                                    .addOption("CORRECTED", "CORRECTED")\n                                    .addOption("LEGACY", "LEGACY"),\n                                "int96RebaseMode"\n                            )\n                                .addField(Checkbox("Recursive File Lookup"), "recursiveFileLookup")\n                                .addField(TextBox("Path Global Filter").bindPlaceholder(""), "pathGlobFilter")\n                                .addField(TextBox("Modified Before").bindPlaceholder(""), "modifiedBefore")\n                                .addField(TextBox("Modified After").bindPlaceholder(""), "modifiedAfter")\n                        )\n                    )\n                ),\n                "auto"\n            )\n                .addColumn(SchemaTable("").bindProperty("schema"), "5fr")\n        ) \\\n            .addSection(\n            "PREVIEW",\n            PreviewTable("").bindProperty("schema")\n        )\n\n    def targetDialog(self) -> DatasetDialog:\n        return DatasetDialog("parquet") \\\n            .addSection("LOCATION", TargetLocation("path")) \\\n            .addSection(\n            "PROPERTIES",\n            ColumnsLayout(gap=("1rem"), height=("100%"))\n                .addColumn(\n                ScrollBox().addElement(\n                    StackLayout(height=("100%")).addElement(\n                        StackItem(grow=(1)).addElement(\n                            FieldPicker(height=("100%"))\n                                .addField(\n                                TextArea("Description", 2, placeholder="Dataset description..."),\n                                "description",\n                                True\n                            )\n                                .addField(\n                                SelectBox("Write Mode")\n                                    .addOption("error", "error")\n                                    .addOption("overwrite", "overwrite")\n                                    .addOption("append", "append")\n                                    .addOption("ignore", "ignore"),\n                                "writeMode"\n                            )\n                                .addField(\n                                SchemaColumnsDropdown("Partition Columns")\n                                    .withMultipleSelection()\n                                    .bindSchema("schema")\n                                    .showErrorsFor("partitionColumns"),\n                                "partitionColumns"\n                            )\n                                .addField(\n                                SelectBox("Compression Codec")\n                                    .addOption("none", "none")\n                                    .addOption("uncompressed", "uncompressed")\n                                    .addOption("gzip", "gzip")\n                                    .addOption("lz4", "lz4")\n                                    .addOption("snappy", "snappy")\n                                    .addOption("lzo", "lzo")\n                                    .addOption("brotli", "brotli")\n                                    .addOption("zstd", "zstd"),\n                                "compression"\n                            )\n                        )\n                    )\n                ),\n                "auto"\n            )\n                .addColumn(SchemaTable("").isReadOnly().withoutInferSchema().bindProperty("schema"), "5fr")\n        )\n\n    def validate(self, component: Component) -> list:\n        diagnostics = super(ParquetFormat, self).validate(component)\n        if len(component.properties.path) == 0:\n            diagnostics.append(\n                Diagnostic("properties.path", "path variable cannot be empty [Location]", SeverityLevelEnum.Error))\n        return diagnostics\n\n    def onChange(self, oldState: Component, newState: Component) -> Component:\n        return newState\n\n    class ParquetFormatCode(ComponentCode):\n        def __init__(self, props):\n            self.props: ParquetFormat.ParquetProperties = props\n\n        def sourceApply(self, spark: SparkSession) -> DataFrame:\n            reader = spark.read.format("parquet")\n            if self.props.mergeSchema is not None:\n                reader = reader.option("mergeSchema", self.props.mergeSchema)\n            if self.props.datetimeRebaseMode is not None:\n                reader = reader.option("datetimeRebaseMode", self.props.datetimeRebaseMode)\n            if self.props.int96RebaseMode is not None:\n                reader = reader.option("int96RebaseMode", self.props.int96RebaseMode)\n            if self.props.modifiedBefore is not None:\n                reader = reader.option("modifiedBefore", self.props.modifiedBefore)\n            if self.props.modifiedAfter is not None:\n                reader = reader.option("modifiedAfter", self.props.modifiedAfter)\n            if self.props.recursiveFileLookup is not None:\n                reader = reader.option("recursiveFileLookup", self.props.recursiveFileLookup)\n            if self.props.pathGlobFilter is not None:\n                reader = reader.option("pathGlobFilter", self.props.pathGlobFilter)\n\n            if self.props.schema is not None and self.props.useSchema:\n                reader = reader.schema(self.props.schema)\n\n            return reader.load(self.props.path)\n\n        def targetApply(self, spark: SparkSession, in0: DataFrame):\n            writer = in0.write.format("parquet")\n            if self.props.compression is not None:\n                writer = writer.option("compression", self.props.compression)\n\n            if self.props.writeMode is not None:\n                writer = writer.mode(self.props.writeMode)\n            if self.props.partitionColumns is not None and len(self.props.partitionColumns) > 0:\n                writer = writer.partitionBy(*self.props.partitionColumns)\n\n            writer.save(self.props.path)\n\n'))),(0,r.yg)(i.A,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},'package io.prophecy.core.instructions.all.datasets\n\nimport io.prophecy.core.instructions.all._\nimport io.prophecy.core.instructions.spec._\nimport io.prophecy.core.program.WorkflowContext\nimport org.apache.spark.sql.{DataFrame, SparkSession}\nimport org.apache.spark.sql.types.StructType\nimport io.prophecy.libs._\n\nobject ParquetFormat extends DatasetSpec {\n\n  val name: String = "parquet"\n  val datasetType: String = "File"\n\n  type PropertiesType = ParquetProperties\n  case class ParquetProperties(\n    @Property("Schema")\n    schema: Option[StructType] = None,\n    @Property("Description")\n    description: Option[String] = Some(""),\n    @Property("useSchema")\n    useSchema: Option[Boolean] = Some(false),\n    @Property("Path")\n    path: String = "",\n    @Property(\n      "",\n      "(default is the value specified in spark.sql.parquet.mergeSchema(false)): sets whether we should merge schemas collected from all Parquet part-files. This will override spark.sql.parquet.mergeSchema."\n    )\n    mergeSchema: Option[Boolean] = None,\n    @Property(\n      "datetimeRebaseMode",\n      "The datetimeRebaseMode option allows to specify the rebasing mode for the values of the DATE, TIMESTAMP_MILLIS, TIMESTAMP_MICROS logical types from the Julian to Proleptic Gregorian calendar."\n    )\n    datetimeRebaseMode: Option[String] = None,\n    @Property(\n      "int96RebaseMode",\n      "The int96RebaseMode option allows to specify the rebasing mode for INT96 timestamps from the Julian to Proleptic Gregorian calendar."\n    )\n    int96RebaseMode: Option[String] = None,\n    @Property("compression", "(default: none) compression codec to use when saving to file.")\n    compression: Option[String] = None,\n    @Property("partitionColumns", "Partitioning column.")\n    partitionColumns: Option[List[String]] = None,\n    @Property("Write Mode", """(default: "error") Specifies the behavior when data or table already exists.""")\n    writeMode: Option[String] = None,\n    @Property(\n      "",\n      "an optional glob pattern to only include files with paths matching the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter. It does not change the behavior of partition discovery."\n    )\n    pathGlobFilter: Option[String] = None,\n    @Property(\n      "",\n      "(batch only): an optional timestamp to only include files with modification times occurring before the specified Time. The provided timestamp must be in the following form: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)"\n    )\n    modifiedBefore: Option[String] = None,\n    @Property(\n      "",\n      "(batch only): an optional timestamp to only include files with modification times occurring after the specified Time. The provided timestamp must be in the following form: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)"\n    )\n    modifiedAfter: Option[String] = None,\n    @Property("", "recursively scan a directory for files. Using this option disables partition discovery")\n    recursiveFileLookup: Option[Boolean] = None\n  ) extends DatasetProperties\n\n  def sourceDialog: DatasetDialog = DatasetDialog("parquet")\n    .addSection("LOCATION", TargetLocation("path"))\n    .addSection(\n      "PROPERTIES",\n      ColumnsLayout(gap = Some("1rem"), height = Some("100%"))\n        .addColumn(\n          ScrollBox().addElement(\n            StackLayout(height = Some("100%"))\n              .addElement(\n                StackItem(grow = Some(1)).addElement(\n                  FieldPicker(height = Some("100%"))\n                    .addField(\n                      TextArea("Description", 2, placeholder = "Dataset description..."),\n                      "description",\n                      true\n                    )\n                    .addField(Checkbox("Use user-defined schema"), "useSchema", true)\n                    .addField(Checkbox("Merge schema"), "mergeSchema")\n                    .addField(\n                      SelectBox("Datetime Rebase Mode")\n                        .addOption("EXCEPTION", "EXCEPTION")\n                        .addOption("CORRECTED", "CORRECTED")\n                        .addOption("LEGACY", "LEGACY"),\n                      "datetimeRebaseMode"\n                    )\n                    .addField(\n                      SelectBox("Int96 Rebase Mode")\n                        .addOption("EXCEPTION", "EXCEPTION")\n                        .addOption("CORRECTED", "CORRECTED")\n                        .addOption("LEGACY", "LEGACY"),\n                      "int96RebaseMode"\n                    )\n                    .addField(Checkbox("Recursive File Lookup"), "recursiveFileLookup")\n                    .addField(TextBox("Path Global Filter").bindPlaceholder(""), "pathGlobFilter")\n                    .addField(TextBox("Modified Before").bindPlaceholder(""), "modifiedBefore")\n                    .addField(TextBox("Modified After").bindPlaceholder(""), "modifiedAfter")\n                )\n              )\n          ),\n          "auto"\n        )\n        .addColumn(SchemaTable("").bindProperty("schema"), "5fr")\n    )\n    .addSection(\n      "PREVIEW",\n      PreviewTable("").bindProperty("schema")\n    )\n\n  def targetDialog: DatasetDialog = DatasetDialog("parquet")\n    .addSection("LOCATION", TargetLocation("path"))\n    .addSection(\n      "PROPERTIES",\n      ColumnsLayout(gap = Some("1rem"), height = Some("100%"))\n        .addColumn(\n          ScrollBox().addElement(\n            StackLayout(height = Some("100%")).addElement(\n              StackItem(grow = Some(1)).addElement(\n                FieldPicker(height = Some("100%"))\n                  .addField(\n                    TextArea("Description", 2, placeholder = "Dataset description..."),\n                    "description",\n                    true\n                  )\n                  .addField(\n                    SelectBox("Write Mode")\n                      .addOption("error", "error")\n                      .addOption("overwrite", "overwrite")\n                      .addOption("append", "append")\n                      .addOption("ignore", "ignore"),\n                    "writeMode"\n                  )\n                  .addField(\n                    SchemaColumnsDropdown("Partition Columns")\n                      .withMultipleSelection()\n                      .bindSchema("schema")\n                      .showErrorsFor("partitionColumns"),\n                    "partitionColumns"\n                  )\n                  .addField(\n                    SelectBox("Compression Codec")\n                      .addOption("none", "none")\n                      .addOption("uncompressed", "uncompressed")\n                      .addOption("gzip", "gzip")\n                      .addOption("lz4", "lz4")\n                      .addOption("snappy", "snappy")\n                      .addOption("lzo", "lzo")\n                      .addOption("brotli", "brotli")\n                      .addOption("zstd", "zstd"),\n                    "compression"\n                  )\n              )\n            )\n          ),\n          "auto"\n        )\n        .addColumn(SchemaTable("").isReadOnly().withoutInferSchema().bindProperty("schema"), "5fr")\n    )\n\n  override def validate(component: Component)(implicit context: WorkflowContext): List[Diagnostic] = {\n    import scala.collection.mutable.ListBuffer\n    val diagnostics = ListBuffer[Diagnostic]()\n    diagnostics ++= super.validate(component)\n\n    if (component.properties.path.isEmpty) {\n      diagnostics += Diagnostic("properties.path", "path variable cannot be empty [Location]", SeverityLevel.Error)\n    }\n    if (component.properties.schema.isEmpty) {\n      // diagnostics += Diagnostic("properties.schema", "Schema cannot be empty [Properties]", SeverityLevel.Error)\n    }\n\n    diagnostics.toList\n  }\n\n  def onChange(oldState: Component, newState: Component)(implicit context: WorkflowContext): Component = newState\n\n  class ParquetFormatCode(props: ParquetProperties) extends ComponentCode {\n\n    def sourceApply(spark: SparkSession): DataFrame = {\n      var reader = spark.read\n        .format("parquet")\n        .option("mergeSchema", props.mergeSchema)\n        .option("datetimeRebaseMode", props.datetimeRebaseMode)\n        .option("int96RebaseMode", props.int96RebaseMode)\n        .option("modifiedBefore", props.modifiedBefore)\n        .option("modifiedAfter", props.modifiedAfter)\n        .option("recursiveFileLookup", props.recursiveFileLookup)\n        .option("pathGlobFilter", props.pathGlobFilter)\n\n      if (props.useSchema.isDefined && props.useSchema.get)\n        props.schema.foreach(schema \u21d2 reader = reader.schema(schema))\n\n      reader.load(props.path)\n    }\n\n    def targetApply(spark: SparkSession, in: DataFrame): Unit = {\n      var writer = in.write\n        .format("parquet")\n        .option("compression", props.compression)\n\n      props.writeMode.foreach { mode \u21d2\n        writer = writer.mode(mode)\n      }\n      props.partitionColumns.foreach(pcols \u21d2\n        writer = pcols match {\n          case Nil \u21d2 writer\n          case _ \u21d2 writer.partitionBy(pcols: _*)\n        }\n      )\n      writer.save(props.path)\n    }\n\n  }\n\n}\n\n\n')))))}g.isMDXComponent=!0},15354:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/gem-builder-ui-b0bd1d841891aeb22ddef5398815b8f0.png"}}]);