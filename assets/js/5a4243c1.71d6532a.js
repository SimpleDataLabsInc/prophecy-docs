"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[1533],{3905:function(e,t,a){a.d(t,{Zo:function(){return d},kt:function(){return c}});var r=a(67294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},l=Object.keys(e);for(r=0;r<l.length;r++)a=l[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(r=0;r<l.length;r++)a=l[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var s=r.createContext({}),p=function(e){var t=r.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},d=function(e){var t=p(e.components);return r.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,l=e.originalType,s=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),m=p(a),c=n,k=m["".concat(s,".").concat(c)]||m[c]||u[c]||l;return a?r.createElement(k,i(i({ref:t},d),{},{components:a})):r.createElement(k,i({ref:t},d))}));function c(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var l=a.length,i=new Array(l);i[0]=m;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:n,i[1]=o;for(var p=2;p<l;p++)i[p]=a[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,a)}m.displayName="MDXCreateElement"},72360:function(e,t,a){a.d(t,{Z:function(){return i}});var r=a(67294),n=a(86010),l="tabItem_OmH5";function i(e){var t=e.children,a=e.hidden,i=e.className;return r.createElement("div",{role:"tabpanel",className:(0,n.Z)(l,i),hidden:a},t)}},9877:function(e,t,a){a.d(t,{Z:function(){return c}});var r=a(87462),n=a(67294),l=a(72389),i=a(67392),o=a(7094),s=a(12466),p=a(86010),d="tabList_uSqn",u="tabItem_LplD";function m(e){var t,a,l,m=e.lazy,c=e.block,k=e.defaultValue,f=e.values,g=e.groupId,h=e.className,b=n.Children.map(e.children,(function(e){if((0,n.isValidElement)(e)&&void 0!==e.props.value)return e;throw new Error("Docusaurus error: Bad <Tabs> child <"+("string"==typeof e.type?e.type:e.type.name)+'>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.')})),N=null!=f?f:b.map((function(e){var t=e.props;return{value:t.value,label:t.label,attributes:t.attributes}})),v=(0,i.l)(N,(function(e,t){return e.value===t.value}));if(v.length>0)throw new Error('Docusaurus error: Duplicate values "'+v.map((function(e){return e.value})).join(", ")+'" found in <Tabs>. Every value needs to be unique.');var y=null===k?k:null!=(t=null!=k?k:null==(a=b.find((function(e){return e.props.default})))?void 0:a.props.value)?t:null==(l=b[0])?void 0:l.props.value;if(null!==y&&!N.some((function(e){return e.value===y})))throw new Error('Docusaurus error: The <Tabs> has a defaultValue "'+y+'" but none of its children has the corresponding value. Available values are: '+N.map((function(e){return e.value})).join(", ")+". If you intend to show no default tab, use defaultValue={null} instead.");var w=(0,o.U)(),T=w.tabGroupChoices,C=w.setTabGroupChoices,q=(0,n.useState)(y),P=q[0],S=q[1],E=[],x=(0,s.o5)().blockElementScrollPositionUntilNextRender;if(null!=g){var O=T[g];null!=O&&O!==P&&N.some((function(e){return e.value===O}))&&S(O)}var D=function(e){var t=e.currentTarget,a=E.indexOf(t),r=N[a].value;r!==P&&(x(t),S(r),null!=g&&C(g,r))},I=function(e){var t,a=null;switch(e.key){case"ArrowRight":var r=E.indexOf(e.currentTarget)+1;a=E[r]||E[0];break;case"ArrowLeft":var n=E.indexOf(e.currentTarget)-1;a=E[n]||E[E.length-1]}null==(t=a)||t.focus()};return n.createElement("div",{className:(0,p.Z)("tabs-container",d)},n.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,p.Z)("tabs",{"tabs--block":c},h)},N.map((function(e){var t=e.value,a=e.label,l=e.attributes;return n.createElement("li",(0,r.Z)({role:"tab",tabIndex:P===t?0:-1,"aria-selected":P===t,key:t,ref:function(e){return E.push(e)},onKeyDown:I,onFocus:D,onClick:D},l,{className:(0,p.Z)("tabs__item",u,null==l?void 0:l.className,{"tabs__item--active":P===t})}),null!=a?a:t)}))),m?(0,n.cloneElement)(b.filter((function(e){return e.props.value===P}))[0],{className:"margin-top--md"}):n.createElement("div",{className:"margin-top--md"},b.map((function(e,t){return(0,n.cloneElement)(e,{key:t,hidden:e.props.value!==P})}))))}function c(e){var t=(0,l.Z)();return n.createElement(m,(0,r.Z)({key:String(t)},e))}},51335:function(e,t,a){a.r(t),a.d(t,{assets:function(){return m},contentTitle:function(){return d},default:function(){return f},frontMatter:function(){return p},metadata:function(){return u},toc:function(){return c}});var r=a(87462),n=a(63366),l=(a(67294),a(3905)),i=a(9877),o=a(72360),s=["components"],p={title:"Parquet",id:"parquet",description:"Parquet",sidebar_position:2,tags:["gems","file","parquet"]},d=void 0,u={unversionedId:"low-code-spark/gems/source-target/file/parquet",id:"low-code-spark/gems/source-target/file/parquet",title:"Parquet",description:"Parquet",source:"@site/docs/low-code-spark/gems/source-target/file/parquet.md",sourceDirName:"low-code-spark/gems/source-target/file",slug:"/low-code-spark/gems/source-target/file/parquet",permalink:"/low-code-spark/gems/source-target/file/parquet",draft:!1,tags:[{label:"gems",permalink:"/tags/gems"},{label:"file",permalink:"/tags/file"},{label:"parquet",permalink:"/tags/parquet"}],version:"current",sidebarPosition:2,frontMatter:{title:"Parquet",id:"parquet",description:"Parquet",sidebar_position:2,tags:["gems","file","parquet"]},sidebar:"defaultSidebar",previous:{title:"FTP",permalink:"/low-code-spark/gems/source-target/file/ftp"},next:{title:"Avro",permalink:"/low-code-spark/gems/source-target/file/avro"}},m={},c=[{value:"Source",id:"source",level:2},{value:"Source Parameters",id:"source-parameters",level:3},{value:"Example",id:"source-example",level:3},{value:"Generated Code",id:"source-code",level:3},{value:"Target",id:"target",level:2},{value:"Target Parameters",id:"target-parameters",level:3},{value:"Supported Write Modes",id:"supported-write-modes",level:3},{value:"Example",id:"target",level:3},{value:"Generated Code",id:"target-code",level:3}],k={toc:c};function f(e){var t=e.components,a=(0,n.Z)(e,s);return(0,l.kt)("wrapper",(0,r.Z)({},k,a,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("p",null,"Parquet is an open-source Columnar storage data format. It handles large volumes of data by supporting complex pushdown predicates, nested schemas and a wide variety of column encoding types."),(0,l.kt)("p",null,"This Gem allows you to read from or write to Parquet files."),(0,l.kt)("h2",{id:"source"},"Source"),(0,l.kt)("p",null,"Reads data from Parquet files at the given path."),(0,l.kt)("h3",{id:"source-parameters"},"Source Parameters"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Parameter"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"),(0,l.kt)("th",{parentName:"tr",align:null},"Required"),(0,l.kt)("th",{parentName:"tr",align:null},"Default"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Location"),(0,l.kt)("td",{parentName:"tr",align:null},"File path where parquet files are present"),(0,l.kt)("td",{parentName:"tr",align:null},"True"),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Schema"),(0,l.kt)("td",{parentName:"tr",align:null},"Schema to be applied on the loaded data. Can be defined/edited as json or inferred using ",(0,l.kt)("inlineCode",{parentName:"td"},"Infer Schema")," button"),(0,l.kt)("td",{parentName:"tr",align:null},"True"),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Recursive File Lookup"),(0,l.kt)("td",{parentName:"tr",align:null},"This is used to recursively load files from the given Location. Disables partition discovery. An exception will be thrown if this option and a ",(0,l.kt)("inlineCode",{parentName:"td"},"partitionSpec")," are specified."),(0,l.kt)("td",{parentName:"tr",align:null},"False"),(0,l.kt)("td",{parentName:"tr",align:null},"False")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Path Global Filter"),(0,l.kt)("td",{parentName:"tr",align:null},"An optional glob pattern to only include files with paths matching the pattern. The syntax follows ",(0,l.kt)("a",{parentName:"td",href:"https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/GlobFilter.html"},"GlobFilter"),". It does not change the behavior of partition discovery."),(0,l.kt)("td",{parentName:"tr",align:null},"False"),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Modified Before"),(0,l.kt)("td",{parentName:"tr",align:null},"An optional Timestamp to only include files with modification times occurring before the specified Time. The provided timestamp must be in ",(0,l.kt)("inlineCode",{parentName:"td"},"YYYY-MM-DDTHH:mm:ss")," form (e.g. ",(0,l.kt)("inlineCode",{parentName:"td"},"2020-06-01T13:00:00"),")"),(0,l.kt)("td",{parentName:"tr",align:null},"False"),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Modified After"),(0,l.kt)("td",{parentName:"tr",align:null},"An optional timestamp to only include files with modification times occurring after the specified Time. The provided timestamp must be in ",(0,l.kt)("inlineCode",{parentName:"td"},"YYYY-MM-DDTHH:mm:ss")," form (e.g. ",(0,l.kt)("inlineCode",{parentName:"td"},"2020-06-01T13:00:00"),")"),(0,l.kt)("td",{parentName:"tr",align:null},"False"),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Merge Schema"),(0,l.kt)("td",{parentName:"tr",align:null},"Sets whether schemas should be merged from all collected Parquet part-files. This will override ",(0,l.kt)("inlineCode",{parentName:"td"},"spark.sql.parquet.mergeSchema"),"."),(0,l.kt)("td",{parentName:"tr",align:null},"False"),(0,l.kt)("td",{parentName:"tr",align:null},"(value of ",(0,l.kt)("inlineCode",{parentName:"td"},"spark.sql.parquet."),(0,l.kt)("br",null),(0,l.kt)("inlineCode",{parentName:"td"},"mergeSchema"),")")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Int96 Rebase mode"),(0,l.kt)("td",{parentName:"tr",align:null},"The ",(0,l.kt)("inlineCode",{parentName:"td"},"int96RebaseMode")," option allows to specify the rebasing mode for INT96 timestamps from the Julian to Proleptic Gregorian calendar. ",(0,l.kt)("br",null),(0,l.kt)("br",null)," Currently supported modes are: ",(0,l.kt)("br",null),(0,l.kt)("br",null),(0,l.kt)("inlineCode",{parentName:"td"},"EXCEPTION"),": fails in reads of ancient INT96 timestamps that are ambiguous between the two calendars.",(0,l.kt)("br",null),(0,l.kt)("br",null),(0,l.kt)("inlineCode",{parentName:"td"},"CORRECTED"),": loads INT96 timestamps without rebasing.",(0,l.kt)("br",null),(0,l.kt)("br",null),(0,l.kt)("inlineCode",{parentName:"td"},"LEGACY"),": performs rebasing of ancient timestamps from the Julian to Proleptic Gregorian calendar."),(0,l.kt)("td",{parentName:"tr",align:null},"False"),(0,l.kt)("td",{parentName:"tr",align:null},"(value of ",(0,l.kt)("inlineCode",{parentName:"td"},"spark.sql.parquet"),(0,l.kt)("br",null),(0,l.kt)("inlineCode",{parentName:"td"},".int96RebaseModeInRead"),")")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Datetime Rebase mode"),(0,l.kt)("td",{parentName:"tr",align:null},"The ",(0,l.kt)("inlineCode",{parentName:"td"},"datetimeRebaseMode")," option allows to specify the rebasing mode for the values of the DATE, TIMESTAMP_MILLIS, TIMESTAMP_MICROS logical types from the Julian to Proleptic Gregorian calendar.",(0,l.kt)("br",null),"Currently supported modes are:",(0,l.kt)("br",null),(0,l.kt)("br",null),(0,l.kt)("inlineCode",{parentName:"td"},"EXCEPTION"),": fails in reads of ancient dates/timestamps that are ambiguous between the two calendars.",(0,l.kt)("br",null),(0,l.kt)("br",null),(0,l.kt)("inlineCode",{parentName:"td"},"CORRECTED"),": loads dates/timestamps without rebasing.",(0,l.kt)("br",null),(0,l.kt)("br",null),(0,l.kt)("inlineCode",{parentName:"td"},"LEGACY"),": performs rebasing of ancient dates/timestamps from the Julian to Proleptic Gregorian calendar."),(0,l.kt)("td",{parentName:"tr",align:null},"False"),(0,l.kt)("td",{parentName:"tr",align:null},"(value of ",(0,l.kt)("inlineCode",{parentName:"td"},"spark.sql.parquet"),(0,l.kt)("br",null),(0,l.kt)("inlineCode",{parentName:"td"},".datetimeRebaseModeInRead"),")")))),(0,l.kt)("h3",{id:"source-example"},"Example"),(0,l.kt)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,l.kt)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,l.kt)("iframe",{src:"https://user-images.githubusercontent.com/103921419/175030738-4c53b5c9-73e7-46c7-9fdc-c49048f78572.mp4",title:"Parquet Source",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,l.kt)("h3",{id:"source-code"},"Generated Code"),(0,l.kt)(i.Z,{mdxType:"Tabs"},(0,l.kt)(o.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},'def read_parquet(spark: SparkSession) -> DataFrame:\n    return spark.read\\\n        .format("parquet")\\\n        .option("mergeSchema", True)\\\n        .load("dbfs:/FileStore/Users/parquet/test.parquet")\n\n'))),(0,l.kt)(o.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'object read_parquet {\n\n  def apply(spark: SparkSession): DataFrame =\n    spark.read\n        .format("parquet")\n        .option("mergeSchema", true)\n        .load("dbfs:/FileStore/Users/parquet/test.parquet")\n\n}\n')))),(0,l.kt)("hr",null),(0,l.kt)("h2",{id:"target"},"Target"),(0,l.kt)("h3",{id:"target-parameters"},"Target Parameters"),(0,l.kt)("p",null,"Write data as Parquet files at the specified path."),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Parameter"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"),(0,l.kt)("th",{parentName:"tr",align:null},"Required"),(0,l.kt)("th",{parentName:"tr",align:null},"Default"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Location"),(0,l.kt)("td",{parentName:"tr",align:null},"File path where the Parquet files will be written"),(0,l.kt)("td",{parentName:"tr",align:null},"True"),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Compression"),(0,l.kt)("td",{parentName:"tr",align:null},"Compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (",(0,l.kt)("inlineCode",{parentName:"td"},"none"),", ",(0,l.kt)("inlineCode",{parentName:"td"},"uncompressed"),", ",(0,l.kt)("inlineCode",{parentName:"td"},"snappy"),", ",(0,l.kt)("inlineCode",{parentName:"td"},"gzip"),", ",(0,l.kt)("inlineCode",{parentName:"td"},"lzo"),", ",(0,l.kt)("inlineCode",{parentName:"td"},"brotli"),", ",(0,l.kt)("inlineCode",{parentName:"td"},"lz4"),", and ",(0,l.kt)("inlineCode",{parentName:"td"},"zstd"),"). This will override ",(0,l.kt)("inlineCode",{parentName:"td"},"spark.sql.parquet.compression.codec"),"."),(0,l.kt)("td",{parentName:"tr",align:null},"False"),(0,l.kt)("td",{parentName:"tr",align:null},"`snappy")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Write Mode"),(0,l.kt)("td",{parentName:"tr",align:null},"How to handle existing data. See ",(0,l.kt)("a",{parentName:"td",href:"#supported-write-modes"},"this table")," for a list of available options."),(0,l.kt)("td",{parentName:"tr",align:null},"True"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("inlineCode",{parentName:"td"},"error"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Partition Columns"),(0,l.kt)("td",{parentName:"tr",align:null},"List of columns to partition the Parquet files by"),(0,l.kt)("td",{parentName:"tr",align:null},"False"),(0,l.kt)("td",{parentName:"tr",align:null},"None")))),(0,l.kt)("h3",{id:"supported-write-modes"},"Supported Write Modes"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Write Mode"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"overwrite"),(0,l.kt)("td",{parentName:"tr",align:null},"If data already exists, overwrite with the contents of the Dataframe")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"append"),(0,l.kt)("td",{parentName:"tr",align:null},"If data already exists, append the contents of the Dataframe")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"ignore"),(0,l.kt)("td",{parentName:"tr",align:null},"If data already exists, do nothing with the contents of the Dataframe. This is similar to a ",(0,l.kt)("inlineCode",{parentName:"td"},"CREATE TABLE IF NOT EXISTS")," in SQL.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"error"),(0,l.kt)("td",{parentName:"tr",align:null},"If data already exists, throw an exception.")))),(0,l.kt)("h3",{id:"target"},"Example"),(0,l.kt)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,l.kt)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,l.kt)("iframe",{src:"https://user-images.githubusercontent.com/103921419/175030713-9de9d38a-c145-42e9-8411-baa44a70d0d0.mp4",title:"Parquet Target",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,l.kt)("h3",{id:"target-code"},"Generated Code"),(0,l.kt)(i.Z,{mdxType:"Tabs"},(0,l.kt)(o.Z,{value:"py",label:"Python",mdxType:"TabItem"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-py"},'def write_parquet(spark: SparkSession, in0: DataFrame):\n    in0.write\\\n        .format("parquet")\\\n        .mode("overwrite")\\\n        .save("dbfs:/data/test_output.parquet")\n'))),(0,l.kt)(o.Z,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},'object write_parquet {\n  def apply(spark: SparkSession, in: DataFrame): Unit =\n    in.write\n        .format("parquet")\n        .mode("overwrite")\n        .save("dbfs:/data/test_output.parquet")\n}\n')))),(0,l.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,l.kt)("div",{parentName:"div",className:"admonition-heading"},(0,l.kt)("h5",{parentName:"div"},(0,l.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,l.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,l.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"info")),(0,l.kt)("div",{parentName:"div",className:"admonition-content"},(0,l.kt)("p",{parentName:"div"},"To know more about tweaking Parquet related properties in Spark config ",(0,l.kt)("a",{parentName:"p",href:"https://spark.apache.org/docs/latest/sql-data-sources-parquet.html"},(0,l.kt)("strong",{parentName:"a"},"click here")),"."))))}f.isMDXComponent=!0}}]);