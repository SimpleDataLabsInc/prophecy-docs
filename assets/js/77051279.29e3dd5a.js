"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[75713],{28453:(e,r,t)=>{t.d(r,{R:()=>o,x:()=>a});var s=t(96540);const i={},n=s.createContext(i);function o(e){const r=s.useContext(n);return s.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function a(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(n.Provider,{value:r},e.children)}},49235:(e,r,t)=>{t.d(r,{A:()=>i});t(96540);var s=t(74848);function i(e){return(0,s.jsxs)("div",{children:[e.python_package_name&&e.python_package_version&&(0,s.jsx)("a",{href:"https://docs.prophecy.io/engineers/package-hub/",children:(0,s.jsxs)("span",{className:"badge-dependency",children:[(0,s.jsx)("span",{className:"left",children:e.python_package_name}),(0,s.jsx)("span",{className:"right",children:e.python_package_version})]})}),e.scala_package_name&&e.scala_package_version&&(0,s.jsx)("a",{href:"https://docs.prophecy.io/engineers/package-hub",children:(0,s.jsxs)("span",{className:"badge-dependency",children:[(0,s.jsx)("span",{className:"left",children:e.scala_package_name}),(0,s.jsx)("span",{className:"right",children:e.scala_package_version})]})}),e.python_lib&&(0,s.jsx)("a",{href:"https://docs.prophecy.io/extensibility/dependencies/prophecy-libraries",children:(0,s.jsxs)("span",{className:"badge-dependency",children:[(0,s.jsx)("span",{className:"left",children:"ProphecyLibsPython"}),(0,s.jsx)("span",{className:"right",children:e.python_lib})]})}),e.scala_lib&&(0,s.jsx)("a",{href:"https://docs.prophecy.io/extensibility/dependencies/prophecy-libraries",children:(0,s.jsxs)("span",{className:"badge-dependency",children:[(0,s.jsx)("span",{className:"left",children:"ProphecyLibsScala"}),(0,s.jsx)("span",{className:"right",children:e.scala_lib})]})}),e.uc_single&&(0,s.jsx)("a",{href:"https://docs.prophecy.io/administration/fabrics/Spark-fabrics/databricks/",children:(0,s.jsxs)("span",{className:"badge-spark",children:[(0,s.jsx)("span",{className:"left",children:"Databricks UC Single Cluster"}),(0,s.jsx)("span",{className:"right",children:e.uc_single})]})}),e.uc_shared&&(0,s.jsx)("a",{href:"https://docs.prophecy.io/administration/fabrics/Spark-fabrics/databricks/ucshared",children:(0,s.jsxs)("span",{className:"badge-spark",children:[(0,s.jsx)("span",{className:"left",children:"Databricks UC Standard"}),(0,s.jsx)("span",{className:"right",children:e.uc_shared})]})}),e.livy&&(0,s.jsx)("a",{href:"https://docs.prophecy.io/administration/fabrics/Spark-fabrics/livy",children:(0,s.jsxs)("span",{className:"badge-spark",children:[(0,s.jsx)("span",{className:"left",children:"Livy"}),(0,s.jsx)("span",{className:"right",children:e.livy})]})}),(0,s.jsx)("br",{}),(0,s.jsx)("br",{})]})}},65537:(e,r,t)=>{t.d(r,{A:()=>w});var s=t(96540),i=t(18215),n=t(65627),o=t(56347),a=t(50372),d=t(30604),c=t(11861),l=t(78749);function h(e){return s.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:r}=e;return!!r&&"object"==typeof r&&"value"in r}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function x(e){const{values:r,children:t}=e;return(0,s.useMemo)((()=>{const e=r??function(e){return h(e).map((e=>{let{props:{value:r,label:t,attributes:s,default:i}}=e;return{value:r,label:t,attributes:s,default:i}}))}(t);return function(e){const r=(0,c.XI)(e,((e,r)=>e.value===r.value));if(r.length>0)throw new Error(`Docusaurus error: Duplicate values "${r.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[r,t])}function j(e){let{value:r,tabValues:t}=e;return t.some((e=>e.value===r))}function u(e){let{queryString:r=!1,groupId:t}=e;const i=(0,o.W6)(),n=function(e){let{queryString:r=!1,groupId:t}=e;if("string"==typeof r)return r;if(!1===r)return null;if(!0===r&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:r,groupId:t});return[(0,d.aZ)(n),(0,s.useCallback)((e=>{if(!n)return;const r=new URLSearchParams(i.location.search);r.set(n,e),i.replace({...i.location,search:r.toString()})}),[n,i])]}function p(e){const{defaultValue:r,queryString:t=!1,groupId:i}=e,n=x(e),[o,d]=(0,s.useState)((()=>function(e){let{defaultValue:r,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(r){if(!j({value:r,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${r}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return r}const s=t.find((e=>e.default))??t[0];if(!s)throw new Error("Unexpected error: 0 tabValues");return s.value}({defaultValue:r,tabValues:n}))),[c,h]=u({queryString:t,groupId:i}),[p,b]=function(e){let{groupId:r}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(r),[i,n]=(0,l.Dv)(t);return[i,(0,s.useCallback)((e=>{t&&n.set(e)}),[t,n])]}({groupId:i}),m=(()=>{const e=c??p;return j({value:e,tabValues:n})?e:null})();(0,a.A)((()=>{m&&d(m)}),[m]);return{selectedValue:o,selectValue:(0,s.useCallback)((e=>{if(!j({value:e,tabValues:n}))throw new Error(`Can't select invalid tab value=${e}`);d(e),h(e),b(e)}),[h,b,n]),tabValues:n}}var b=t(9136);const m={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var g=t(74848);function f(e){let{className:r,block:t,selectedValue:s,selectValue:o,tabValues:a}=e;const d=[],{blockElementScrollPositionUntilNextRender:c}=(0,n.a_)(),l=e=>{const r=e.currentTarget,t=d.indexOf(r),i=a[t].value;i!==s&&(c(r),o(i))},h=e=>{let r=null;switch(e.key){case"Enter":l(e);break;case"ArrowRight":{const t=d.indexOf(e.currentTarget)+1;r=d[t]??d[0];break}case"ArrowLeft":{const t=d.indexOf(e.currentTarget)-1;r=d[t]??d[d.length-1];break}}r?.focus()};return(0,g.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":t},r),children:a.map((e=>{let{value:r,label:t,attributes:n}=e;return(0,g.jsx)("li",{role:"tab",tabIndex:s===r?0:-1,"aria-selected":s===r,ref:e=>{d.push(e)},onKeyDown:h,onClick:l,...n,className:(0,i.A)("tabs__item",m.tabItem,n?.className,{"tabs__item--active":s===r}),children:t??r},r)}))})}function y(e){let{lazy:r,children:t,selectedValue:n}=e;const o=(Array.isArray(t)?t:[t]).filter(Boolean);if(r){const e=o.find((e=>e.props.value===n));return e?(0,s.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,g.jsx)("div",{className:"margin-top--md",children:o.map(((e,r)=>(0,s.cloneElement)(e,{key:r,hidden:e.props.value!==n})))})}function v(e){const r=p(e);return(0,g.jsxs)("div",{className:(0,i.A)("tabs-container",m.tabList),children:[(0,g.jsx)(f,{...r,...e}),(0,g.jsx)(y,{...r,...e})]})}function w(e){const r=(0,b.A)();return(0,g.jsx)(v,{...e,children:h(e.children)},String(r))}},79329:(e,r,t)=>{t.d(r,{A:()=>o});t(96540);var s=t(18215);const i={tabItem:"tabItem_Ymn6"};var n=t(74848);function o(e){let{children:r,hidden:t,className:o}=e;return(0,n.jsx)("div",{role:"tabpanel",className:(0,s.A)(i.tabItem,o),hidden:t,children:r})}},81455:(e,r,t)=>{t.r(r),t.d(r,{assets:()=>h,contentTitle:()=>l,default:()=>u,frontMatter:()=>c,metadata:()=>s,toc:()=>x});const s=JSON.parse('{"id":"Spark/gems/source-target/warehouse/bigquery","title":"BigQuery","description":"Parameters and properties to read from and write to the BigQuery warehouse","source":"@site/docs/Spark/gems/source-target/warehouse/bigquery.md","sourceDirName":"Spark/gems/source-target/warehouse","slug":"/engineers/bigquery","permalink":"/engineers/bigquery","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"gems","permalink":"/tags/gems"},{"inline":true,"label":"warehouse","permalink":"/tags/warehouse"},{"inline":true,"label":"bigquery","permalink":"/tags/bigquery"}],"version":"current","frontMatter":{"title":"BigQuery","id":"bigquery","slug":"/engineers/bigquery","description":"Parameters and properties to read from and write to the BigQuery warehouse","tags":["gems","warehouse","bigquery"]},"sidebar":"mySidebar","previous":{"title":"XML","permalink":"/engineers/xml"},"next":{"title":"CosmosDB","permalink":"/engineers/cosmosdb"}}');var i=t(74848),n=t(28453),o=t(49235),a=t(65537),d=t(79329);const c={title:"BigQuery",id:"bigquery",slug:"/engineers/bigquery",description:"Parameters and properties to read from and write to the BigQuery warehouse",tags:["gems","warehouse","bigquery"]},l=void 0,h={},x=[{value:"Parameters",id:"parameters",level:2},{value:"Credentials",id:"credentials",level:3},{value:"Retrieve JSON credentials",id:"retrieve-json-credentials",level:3},{value:"Source",id:"source",level:2},{value:"Source properties",id:"source-properties",level:3},{value:"Example",id:"example",level:2},{value:"Compiled code",id:"source-code",level:3},{value:"Target",id:"target",level:2},{value:"Target properties",id:"target-properties",level:3},{value:"Supported write modes",id:"supported-write-modes",level:3},{value:"Compiled code",id:"target-code",level:3}];function j(e){const r={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,n.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(o.A,{python_package_name:"ProphecyWarehousePython",python_package_version:"0.0.1+",scala_package_name:"",scala_package_version:"",scala_lib:"",python_lib:"",uc_single:"14.3+",uc_shared:"14.3+",livy:"Not Supported"}),"\n",(0,i.jsx)(r.admonition,{title:"Built on",type:"info",children:(0,i.jsxs)(r.p,{children:["This connector is built on top of the already available ",(0,i.jsx)(r.a,{href:"https://github.com/GoogleCloudDataproc/spark-bigquery-connector/",children:"Apache Spark SQL connector for Google BigQuery"}),".",(0,i.jsx)("br",{}),(0,i.jsx)("br",{}),"\nFor non-Databricks clusters, install the corresponding library, and see ",(0,i.jsx)(r.a,{href:"https://github.com/GoogleCloudDataproc/spark-bigquery-connector#connector-to-spark-compatibility-matrix",children:"Spark BigQuery library compatibility matrix"})," documentation. ",(0,i.jsx)("br",{})]})}),"\n",(0,i.jsxs)(r.p,{children:["You can read from and write to ",(0,i.jsx)(r.code,{children:"BigQuery"}),"."]}),"\n",(0,i.jsx)(r.h2,{id:"parameters",children:"Parameters"}),"\n",(0,i.jsxs)(r.table,{children:[(0,i.jsx)(r.thead,{children:(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.th,{children:"Parameter"}),(0,i.jsx)(r.th,{children:"Tab"}),(0,i.jsx)(r.th,{children:"Description"})]})}),(0,i.jsxs)(r.tbody,{children:[(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Parent Project Name"}),(0,i.jsx)(r.td,{children:"Location"}),(0,i.jsx)(r.td,{children:"Google Cloud Project ID of the table to bill for the export."})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Table Name"}),(0,i.jsx)(r.td,{children:"Location"}),(0,i.jsx)(r.td,{children:"Name of the table."})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Credentials"}),(0,i.jsx)(r.td,{children:"Location"}),(0,i.jsxs)(r.td,{children:["How you want to connect to BigQuery. ",(0,i.jsx)("br",{}),"Possible values are: ",(0,i.jsx)(r.code,{children:"None"}),", ",(0,i.jsx)(r.code,{children:"JSON Credentials Filepath"}),", or ",(0,i.jsx)(r.code,{children:"Databricks secrets"}),". ",(0,i.jsx)("br",{}),"To learn more, see ",(0,i.jsx)(r.a,{href:"#credentials",children:"Credentials"}),"."]})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Schema"}),(0,i.jsx)(r.td,{children:"Properties"}),(0,i.jsxs)(r.td,{children:["Schema to apply on the loaded data.",(0,i.jsx)("br",{}),"In the Source gem, you can define or edit the schema visually or in JSON code.",(0,i.jsx)("br",{}),"In the Target gem, you can view the schema visually or as JSON code."]})]})]})]}),"\n",(0,i.jsx)(r.h3,{id:"credentials",children:"Credentials"}),"\n",(0,i.jsxs)(r.table,{children:[(0,i.jsx)(r.thead,{children:(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.th,{children:"Credential type"}),(0,i.jsx)(r.th,{children:"Description"})]})}),(0,i.jsxs)(r.tbody,{children:[(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"None"}),(0,i.jsx)(r.td,{children:"You don't have to set credentials if the BigQuery configurations are set at the cluster level."})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"JSON Credentials Filepath"}),(0,i.jsxs)(r.td,{children:["BigQuery JSON key configuration you can pass to BigQuery. ",(0,i.jsx)("br",{}),"To learn how, see ",(0,i.jsx)(r.a,{href:"#retrieve-json-credentials",children:"Retrieve JSON Credentials"}),"."]})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Databricks secrets"}),(0,i.jsxs)(r.td,{children:["If the JSON configuration is directly stored on pipeline configuration as Databricks secrets, then refer to the config variable as ",(0,i.jsx)(r.code,{children:"${config_variable}"}),". If the configuration variable above is Base64 encoded, enable ",(0,i.jsx)(r.code,{children:"Is secret base64 encoded"}),"."]})]})]})]}),"\n",(0,i.jsx)(r.h3,{id:"retrieve-json-credentials",children:"Retrieve JSON credentials"}),"\n",(0,i.jsx)(r.p,{children:"To get your JSON Credentials from BigQuery:"}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:["Navigate to your ",(0,i.jsx)(r.a,{href:"https://console.cloud.google.com/apis/credentials",children:"Google Cloud Credentials page"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:["In the top navigation bar, click ",(0,i.jsx)(r.code,{children:"+ CREATE CREDENTIALS"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:["Select ",(0,i.jsx)(r.code,{children:"Service account"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:"If you don't have a Service account, create a service account."}),"\n",(0,i.jsxs)(r.p,{children:["a. If you don't have a Service account, enter your ",(0,i.jsx)(r.strong,{children:"Service account name"}),", ",(0,i.jsx)(r.strong,{children:"Service account ID"}),", and ",(0,i.jsx)(r.strong,{children:"Service account description"}),".\nThen click ",(0,i.jsx)(r.code,{children:"Create and continue"}),". ",(0,i.jsx)("br",{}),"\nb. Click ",(0,i.jsx)(r.code,{children:"Done"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:["Under the ",(0,i.jsx)(r.code,{children:"Service Accounts"})," section, click on your service account email."]}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:["Navigate to the ",(0,i.jsx)(r.code,{children:"Keys"})," tab."]}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:"Create a new key."}),"\n",(0,i.jsxs)(r.p,{children:["a. Click ",(0,i.jsx)(r.code,{children:"Add key"}),".",(0,i.jsx)("br",{}),"\nb. Click ",(0,i.jsx)(r.code,{children:"Create new key"})," ",(0,i.jsx)("br",{}),"\nc. Select the ",(0,i.jsx)(r.code,{children:"JSON key type"}),". ",(0,i.jsx)("br",{}),"\nd. Click ",(0,i.jsx)(r.code,{children:"Create"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(r.h2,{id:"source",children:"Source"}),"\n",(0,i.jsx)(r.p,{children:"The Source gem reads data from BigQuery and allows you to optionally specify the following additional properties."}),"\n",(0,i.jsx)(r.h3,{id:"source-properties",children:"Source properties"}),"\n",(0,i.jsxs)(r.table,{children:[(0,i.jsx)(r.thead,{children:(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.th,{children:"Properties"}),(0,i.jsx)(r.th,{children:"Description"}),(0,i.jsx)(r.th,{children:"Default"})]})}),(0,i.jsxs)(r.tbody,{children:[(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Description"}),(0,i.jsx)(r.td,{children:"Description of your dataset."}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Project Name"}),(0,i.jsx)(r.td,{children:"Google Cloud Project ID of the table."}),(0,i.jsx)(r.td,{children:"Project of the Service Account"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Dataset Name"}),(0,i.jsxs)(r.td,{children:["Dataset containing the table. ",(0,i.jsx)("br",{}),"This is required unless you mention it in the ",(0,i.jsx)(r.strong,{children:"Table Name"}),"."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Maximum partitions"}),(0,i.jsxs)(r.td,{children:["Maximum number of partitions to split the data into. ",(0,i.jsx)("br",{}),"The actual number may be less if BigQuery deems the data small enough."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Minimum partitions"}),(0,i.jsxs)(r.td,{children:["Minimum number of partitions to split the data into. ",(0,i.jsx)("br",{}),"The actual number may be less if BigQuery deems the data small enough."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Enables read views"}),(0,i.jsx)(r.td,{children:"Whether to enable the connector to read from views and not only tables."}),(0,i.jsx)(r.td,{children:"false"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"MaterializedView projectID"}),(0,i.jsx)(r.td,{children:"Project ID where you create the materialized view."}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"MaterializedView dataset"}),(0,i.jsxs)(r.td,{children:["Dataset where you create the materialized view. ",(0,i.jsx)("br",{}),"This dataset should be in the same location as the view or the queried tables."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Materialized expiration time in min's"}),(0,i.jsxs)(r.td,{children:["Expiration time in minutes of the temporary table holding the materialized data of a view or a query. ",(0,i.jsx)("br",{}),"The connector may re-use the temporary table due to the use of local cache and to reduce BigQuery computation, so very low values may cause errors."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Read dataformat"}),(0,i.jsxs)(r.td,{children:["Data format for reading from BigQuery. ",(0,i.jsx)("br",{}),"Possible values are: ",(0,i.jsx)(r.code,{children:"ARROW"}),", or ",(0,i.jsx)(r.code,{children:"AVRO"})," ",(0,i.jsx)("br",{})," ",(0,i.jsx)(r.strong,{children:"Note:"})," Unsupported Arrow filters are not pushed down and results are filtered later by Spark. Currently, Arrow does not support disjunction across columns."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Enable optimize-empty-projection"}),(0,i.jsxs)(r.td,{children:["Whether the connector uses an optimized empty projection (a ",(0,i.jsx)(r.code,{children:"SELECT"})," without any columns) logic for a ",(0,i.jsx)(r.code,{children:"count()"})," execution."]}),(0,i.jsx)(r.td,{children:"false"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Enable push-all-filters"}),(0,i.jsxs)(r.td,{children:["Whether to push all the filters Spark can delegate to BigQuery Storage API. ",(0,i.jsx)("br",{}),"This reduces the amount of data that BigQuery Storage API servers need to send to Spark clients."]}),(0,i.jsx)(r.td,{children:"true"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Additional Job Labels"}),(0,i.jsx)(r.td,{children:"Labels to add to the connector-initiated query and load BigQuery jobs."}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Traceability Application Name"}),(0,i.jsxs)(r.td,{children:["Application name to trace BigQuery Storage read and write sessions. ",(0,i.jsx)("br",{}),"You must set this property to set the trace ID on the sessions."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Traceability Job ID"}),(0,i.jsx)(r.td,{children:"Job ID to trace BigQuery Storage read and write sessions."}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Proxy URL"}),(0,i.jsxs)(r.td,{children:["HTTP proxy and address in the ",(0,i.jsx)(r.code,{children:"host:port"})," format. ",(0,i.jsx)("br",{}),"You can alternatively set this in the Spark configuration ",(0,i.jsx)(r.code,{children:"spark.conf.set(...)"}),", or Hadoop Configuration ",(0,i.jsx)(r.code,{children:"fs.gs.proxy.address"}),"."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Proxy username"}),(0,i.jsxs)(r.td,{children:["Username to connect to the proxy. ",(0,i.jsx)("br",{}),"You can alternatively set this in the Spark configuration ",(0,i.jsx)(r.code,{children:"spark.conf.set(...)"}),", or Hadoop Configuration ",(0,i.jsx)(r.code,{children:"fs.gs.proxy.username"}),"."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Proxy password"}),(0,i.jsxs)(r.td,{children:["Password to connect to the proxy. ",(0,i.jsx)("br",{}),"You can alternatively set this in the Spark configuration ",(0,i.jsx)(r.code,{children:"spark.conf.set(...)"}),", or Hadoop Configuration ",(0,i.jsx)(r.code,{children:"fs.gs.proxy.password"}),"."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Maximum HTTP retries"}),(0,i.jsxs)(r.td,{children:["Maximum number of retries for the low-level HTTP requests to BigQuery. ",(0,i.jsx)("br",{})," You can alternatively set in the Spark configuration ",(0,i.jsx)(r.code,{children:'spark.conf.set("httpMaxRetry", ...)'}),", or Hadoop Configuration ",(0,i.jsx)(r.code,{children:"fs.gs.http.max.retry"}),"."]}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"10"})})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"HTTP Connection timeout in MSec's"}),(0,i.jsxs)(r.td,{children:["Timeout in milliseconds to establish a connection with BigQuery. ",(0,i.jsx)("br",{})," You can alternatively set in the Spark configuration ",(0,i.jsx)(r.code,{children:'spark.conf.set("httpConnectTimeout", ...)'}),", or Hadoop Configuration ",(0,i.jsx)(r.code,{children:"fs.gs.http.connect-timeout"}),"."]}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"60000"})})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"HTTP Read timeout in MSec's"}),(0,i.jsxs)(r.td,{children:["Timeout in milliseconds to read data from an established connection. ",(0,i.jsx)("br",{})," You can alternatively set in the Spark configuration ",(0,i.jsx)(r.code,{children:'spark.conf.set("httpReadTimeout", ...)'}),", or Hadoop Configuration ",(0,i.jsx)(r.code,{children:"fs.gs.http.read-timeout"}),"."]}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"60000"})})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Arrow Compression Codec"}),(0,i.jsxs)(r.td,{children:["Compression codec to use while reading from a BigQuery table when using Arrow format. ",(0,i.jsx)("br",{}),"Possible values are: ",(0,i.jsx)(r.code,{children:"ZSTD"}),", ",(0,i.jsx)(r.code,{children:"LZ4_FRAME"}),", or ",(0,i.jsx)(r.code,{children:"COMPRESSION_UNSPECIFIED"}),"."]}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"COMPRESSION_UNSPECIFIED"})})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Cache expiration time in min's"}),(0,i.jsxs)(r.td,{children:["Expiration time of the in-memory cache storing query information. ",(0,i.jsx)("br",{}),"To disable caching, set the value to ",(0,i.jsx)(r.code,{children:"0"}),"."]}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"15"})})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Cache read session timeout in sec's"}),(0,i.jsxs)(r.td,{children:["Timeout in seconds to create a read session when reading a table. ",(0,i.jsx)("br",{}),"For extremely large tables, this value should be increased."]}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"600"})})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"GCP Access Token"}),(0,i.jsx)(r.td,{children:"GCP token that allows you to use Google API's."}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Conversation datetime zone ID"}),(0,i.jsxs)(r.td,{children:["Time zone ID to use when converting BigQuery's DATETIME into Spark's Timestamp, and vice versa. ",(0,i.jsx)("br",{}),"To see a full list, run ",(0,i.jsx)(r.code,{children:"java.time.ZoneId.getAvailableZoneIds()"})," in Java/Scala, or ",(0,i.jsx)(r.code,{children:"sc.\\_jvm.java.time.ZoneId.getAvailableZoneIds()"})," in PySpark."]}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"UTC"})})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Job query priority"}),(0,i.jsxs)(r.td,{children:["Priority levels set for the job while reading data from a BigQuery query. ",(0,i.jsx)("br",{}),"Possible values are: ",(0,i.jsx)("br",{}),"- ",(0,i.jsx)(r.code,{children:"BATCH"}),", which means to queue and start a query as soon as idle resources are available. ",(0,i.jsx)("br",{}),"- ",(0,i.jsx)(r.code,{children:"INTERACTIVE"}),", which is automatically selected if the query hasn't started within 3 hours."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Filter Condition"}),(0,i.jsxs)(r.td,{children:["One or more boolean expressions to further filter results for the output. ",(0,i.jsx)("br",{}),"Supports SQL, Python, and Scala expressions."]}),(0,i.jsx)(r.td,{children:"None"})]})]})]}),"\n",(0,i.jsx)(r.h2,{id:"example",children:"Example"}),"\n",(0,i.jsx)(r.p,{children:"The following example fetches all customer data from BigQuery table using Prophecy."}),"\n",(0,i.jsx)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"},children:(0,i.jsx)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"},children:(0,i.jsx)("iframe",{src:"https://user-images.githubusercontent.com/103921419/233473742-49cb6104-1f4b-4380-b34d-b89dc81d7921.mp4",title:"BigQuery Source",allow:"autoplay;fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"})})}),"\n",(0,i.jsx)(r.h3,{id:"source-code",children:"Compiled code"}),"\n",(0,i.jsx)(r.admonition,{type:"tip",children:(0,i.jsxs)(r.p,{children:["To see the compiled code of your project, ",(0,i.jsx)(r.a,{href:"/engineers/pipelines#project-editor",children:"switch to the Code view"})," in the project header."]})}),"\n","\n",(0,i.jsx)(a.A,{children:(0,i.jsx)(d.A,{value:"py",label:"Python",children:(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:'def read_bigquery(spark: SparkSession) -> DataFrame:\n    return spark.read\\\n        .format("bigquery")\\\n        .option("credentials", "dbfs:/bucket/prefix/file.json")\\\n        .option("table", "tablename")\\\n        .load()\n'})})})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"target",children:"Target"}),"\n",(0,i.jsx)(r.p,{children:"The Target gem writes data to BigQuery and allows you to optionally specify the following additional properties."}),"\n",(0,i.jsx)(r.h3,{id:"target-properties",children:"Target properties"}),"\n",(0,i.jsxs)(r.table,{children:[(0,i.jsx)(r.thead,{children:(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.th,{children:"Property"}),(0,i.jsx)(r.th,{children:"Description"}),(0,i.jsx)(r.th,{children:"Default"})]})}),(0,i.jsxs)(r.tbody,{children:[(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Description"}),(0,i.jsx)(r.td,{children:"Description of your dataset."}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Job query priority"}),(0,i.jsxs)(r.td,{children:["Priority levels set for the job while reading data from a BigQuery query. ",(0,i.jsx)("br",{}),"This property is available when DIRECT write is used with OVERWRITE mode, where the connector overwrites the destination table using MERGE statement. ",(0,i.jsx)("br",{}),"Possible values are: ",(0,i.jsx)("br",{}),"- ",(0,i.jsx)(r.code,{children:"BATCH"}),", which means to queue and start a query as soon as idle resources are available. ",(0,i.jsx)("br",{}),"- ",(0,i.jsx)(r.code,{children:"INTERACTIVE"}),", which is automatically selected if the query hasn't started within 3 hours."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Project Name"}),(0,i.jsx)(r.td,{children:"Google Cloud Project ID of the table."}),(0,i.jsx)(r.td,{children:"Project of the Service Account"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Dataset Name"}),(0,i.jsxs)(r.td,{children:["Dataset containing the table. ",(0,i.jsx)("br",{}),"This is required unless you mention it in the ",(0,i.jsx)(r.strong,{children:"Table Name"}),"."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Table labels"}),(0,i.jsx)(r.td,{children:"One or more labels to add to the table while writing."}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Disposition creation"}),(0,i.jsxs)(r.td,{children:["Specifies whether the job can create new tables. ",(0,i.jsx)("br",{}),"Possible values are: ",(0,i.jsx)("br",{}),"- ",(0,i.jsx)(r.code,{children:"Create table if not exists"}),", which configures the job to create the table if it does not exist. ",(0,i.jsx)("br",{}),"- ",(0,i.jsx)(r.code,{children:"Don't create table"}),", which configures the job to fail if the table does not exist."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Write Mode"}),(0,i.jsxs)(r.td,{children:["How to handle existing data. For a list of the possible values, see ",(0,i.jsx)(r.a,{href:"#supported-write-modes",children:"Supported write modes"}),"."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Write Method"}),(0,i.jsxs)(r.td,{children:["Method to write data to BigQuery. ",(0,i.jsx)("br",{}),"Possible values are: ",(0,i.jsx)("br",{}),"- ",(0,i.jsx)(r.code,{children:"Use storage write API"}),", which directly uses the BigQuery Storage Write API ",(0,i.jsx)("br",{}),"- ",(0,i.jsx)(r.code,{children:"Write first to GCS and Load"}),", which writes the data first to GCS and then triggers a BigQuery load operation"]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Temporary GCS Bucket"}),(0,i.jsxs)(r.td,{children:["GCS bucket that temporarily holds the data before it loads to BigQuery. ",(0,i.jsx)("br",{}),"This is required unless set in the Spark configuration ",(0,i.jsx)(r.code,{children:"spark.conf.set(...)"}),". ",(0,i.jsx)("br",{}),(0,i.jsx)(r.strong,{children:"Note:"})," This is not supported by the ",(0,i.jsx)(r.code,{children:"DIRECT"})," write method."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Persistent GCS Bucket"}),(0,i.jsxs)(r.td,{children:["GCS bucket that holds the data before it loads to BigQuery. ",(0,i.jsx)("br",{}),"If informed, the data won't be deleted after writing data into BigQuery. ",(0,i.jsx)("br",{}),(0,i.jsx)(r.strong,{children:"Note:"})," This is not supported by the ",(0,i.jsx)(r.code,{children:"DIRECT"})," write method."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Persistent GCS Path"}),(0,i.jsxs)(r.td,{children:["GCS path that holds the data before it loads to BigQuery. ",(0,i.jsx)("br",{})," This is only used with ",(0,i.jsx)(r.code,{children:"Persistent GcCS Bucket"}),". ",(0,i.jsx)("br",{}),(0,i.jsx)(r.strong,{children:"Note:"})," This is not supported by the ",(0,i.jsx)(r.code,{children:"DIRECT"})," write method."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Intermediate dataformat"}),(0,i.jsxs)(r.td,{children:["Format of the data before it loads to BigQuery. ",(0,i.jsx)("br",{})," Possible values are: ",(0,i.jsx)(r.code,{children:"Parquet"}),",",(0,i.jsx)(r.code,{children:"ORC"}),", or ",(0,i.jsx)(r.code,{children:"Avro"})," ",(0,i.jsx)("br",{}),"In order to use the Avro format, you must add the ",(0,i.jsx)(r.code,{children:"spark-avro"})," package in runtime."]}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"Parquet"})})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Date partition"}),(0,i.jsxs)(r.td,{children:["Date partition in the format ",(0,i.jsx)(r.code,{children:"YYYYMMDD"})," that the data writes to. ",(0,i.jsx)("br",{})," You can use this to overwrite the data of a single partition: ",(0,i.jsx)("br",{}),(0,i.jsx)(r.code,{children:'df.write.format("bigquery").option("datePartition", "20220331")'}),(0,i.jsx)("br",{}),(0,i.jsx)(r.code,{children:'.mode("overwrite").save("table")'})," ",(0,i.jsx)("br",{})," You can also use this with different partition types: ",(0,i.jsx)(r.code,{children:"HOUR: YYYYMMDDHH, MONTH: YYYYMM, YEAR: YYYY"})]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Partition expiration MSec's"}),(0,i.jsxs)(r.td,{children:["Number of milliseconds to keep the storage for partitions in the table. ",(0,i.jsx)("br",{}),"The storage in a partition has an expiration time of its partition time plus this value. ",(0,i.jsx)("br",{}),(0,i.jsx)(r.strong,{children:"NOTE:"})," This is not supported by the ",(0,i.jsx)(r.code,{children:"DIRECT"})," write method."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Partition type of the field"}),(0,i.jsxs)(r.td,{children:["Type of partition field. Possible values are: ",(0,i.jsx)(r.code,{children:"Hour"}),", ",(0,i.jsx)(r.code,{children:"Day"}),", ",(0,i.jsx)(r.code,{children:"Month"}),", or ",(0,i.jsx)(r.code,{children:"Year"}),".",(0,i.jsx)("br",{})," ",(0,i.jsx)(r.strong,{children:"Note:"})," This property is mandatory for a target table to be partitioned, and is not supported by the ",(0,i.jsx)(r.code,{children:"DIRECT"})," write method."]}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"Day"})})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Partition field"}),(0,i.jsxs)(r.td,{children:["Field to partition the table by. ",(0,i.jsx)("br",{}),"The field must be a top-level ",(0,i.jsx)(r.code,{children:"TIMESTAMP"})," or ",(0,i.jsx)(r.code,{children:"DATE"})," field. Its mode must be ",(0,i.jsx)(r.code,{children:"NULLABLE"})," or ",(0,i.jsx)(r.code,{children:"REQUIRED"}),". If the property is not set for a partitioned table, then the table will be partitioned by a pseudo column, referenced through either '_PARTITIONTIME' as ",(0,i.jsx)(r.code,{children:"TIMESTAMP"})," type, or '_PARTITIONDATE' as ",(0,i.jsx)(r.code,{children:"DATE"})," type. ",(0,i.jsx)("br",{}),(0,i.jsx)(r.strong,{children:"NOTE:"})," You must specify this field with ",(0,i.jsx)(r.code,{children:"partitionType"}),". This field is also not supported by the ",(0,i.jsx)(r.code,{children:"DIRECT"})," write method."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Enable allow-field-addition"}),(0,i.jsxs)(r.td,{children:["Whether to add the ",(0,i.jsx)(r.code,{children:"ALLOW_FIELD_ADDITION"})," SchemaUpdateOption to the BigQuery LoadJob."]}),(0,i.jsx)(r.td,{children:"false"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Enable allow-field-relaxation"}),(0,i.jsxs)(r.td,{children:["Whether to add the ",(0,i.jsx)(r.code,{children:"ALLOW_FIELD_RELAXATION"})," SchemaUpdateOption to the BigQuery LoadJob."]}),(0,i.jsx)(r.td,{children:"false"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Proxy URL"}),(0,i.jsxs)(r.td,{children:["HTTP proxy and address in the ",(0,i.jsx)(r.code,{children:"host:port"})," format. ",(0,i.jsx)("br",{}),"You can alternatively set this in the Spark configuration ",(0,i.jsx)(r.code,{children:"spark.conf.set(...)"}),", or Hadoop Configuration ",(0,i.jsx)(r.code,{children:"fs.gs.proxy.address"}),"."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Proxy username"}),(0,i.jsxs)(r.td,{children:["Username to connect to the proxy. ",(0,i.jsx)("br",{}),"You can alternatively set this in the Spark configuration ",(0,i.jsx)(r.code,{children:"spark.conf.set(...)"}),", or Hadoop Configuration ",(0,i.jsx)(r.code,{children:"fs.gs.proxy.username"}),"."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Proxy password"}),(0,i.jsxs)(r.td,{children:["Password to connect to the proxy. ",(0,i.jsx)("br",{}),"You can alternatively set this in the Spark configuration ",(0,i.jsx)(r.code,{children:"spark.conf.set(...)"}),", or Hadoop Configuration ",(0,i.jsx)(r.code,{children:"fs.gs.proxy.password"}),"."]}),(0,i.jsx)(r.td,{children:"None"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Maximum HTTP retries"}),(0,i.jsxs)(r.td,{children:["Maximum number of retries for the low-level HTTP requests to BigQuery. ",(0,i.jsx)("br",{})," You can alternatively set in the Spark configuration ",(0,i.jsx)(r.code,{children:'spark.conf.set("httpMaxRetry", ...)'}),", or Hadoop Configuration ",(0,i.jsx)(r.code,{children:"fs.gs.http.max.retry"}),"."]}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"10"})})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"HTTP Connection timeout in MSec's"}),(0,i.jsxs)(r.td,{children:["Timeout in milliseconds to establish a connection with BigQuery. ",(0,i.jsx)("br",{})," You can alternatively set in the Spark configuration ",(0,i.jsx)(r.code,{children:'spark.conf.set("httpConnectTimeout", ...)'}),", or Hadoop Configuration ",(0,i.jsx)(r.code,{children:"fs.gs.http.connect-timeout"}),"."]}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"60000"})})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Enable mode-check-for-schema-fields"}),(0,i.jsx)(r.td,{children:"Whether to check the mode of every field in the destination schema to be equal to the mode in the corresponding source field schema, during DIRECT write."}),(0,i.jsx)(r.td,{children:"true"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Enable list-interface"}),(0,i.jsxs)(r.td,{children:["Whether to use schema inference when the ",(0,i.jsx)(r.a,{href:"https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#parquetoptions",children:"mode is Parquet"}),"."]}),(0,i.jsx)(r.td,{children:"true"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Conversation datetime zone ID"}),(0,i.jsxs)(r.td,{children:["Time zone ID to use when converting BigQuery's DATETIME into Spark's Timestamp, and vice versa. ",(0,i.jsx)("br",{}),"To see a full list, run ",(0,i.jsx)(r.code,{children:"java.time.ZoneId.getAvailableZoneIds()"})," in Java/Scala, or ",(0,i.jsx)(r.code,{children:"sc.\\_jvm.java.time.ZoneId.getAvailableZoneIds()"})," in PySpark."]}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"UTC"})})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"GCP Access Token"}),(0,i.jsx)(r.td,{children:"GCP token that allows you to use Google API's."}),(0,i.jsx)(r.td,{children:"None"})]})]})]}),"\n",(0,i.jsx)(r.h3,{id:"supported-write-modes",children:"Supported write modes"}),"\n",(0,i.jsxs)(r.table,{children:[(0,i.jsx)(r.thead,{children:(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.th,{children:"Write mode"}),(0,i.jsx)(r.th,{children:"Description"})]})}),(0,i.jsxs)(r.tbody,{children:[(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"overwrite"}),(0,i.jsxs)(r.td,{children:["If the data already exists, overwrite the data with the contents of the ",(0,i.jsx)(r.code,{children:"DataFrame"}),"."]})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"append"}),(0,i.jsxs)(r.td,{children:["If the data already exists, append the contents of the ",(0,i.jsx)(r.code,{children:"DataFrame"}),"."]})]})]})]}),"\n",(0,i.jsx)(r.h3,{id:"target-code",children:"Compiled code"}),"\n",(0,i.jsx)(r.admonition,{type:"tip",children:(0,i.jsxs)(r.p,{children:["To see the compiled code of your project, ",(0,i.jsx)(r.a,{href:"/engineers/pipelines#project-editor",children:"switch to the Code view"})," in the project header."]})}),"\n",(0,i.jsx)(a.A,{children:(0,i.jsxs)(d.A,{value:"py",label:"Python",children:[(0,i.jsx)(r.p,{children:"Direct write using the BigQuery Storage Write API:"}),(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:'def write_bigquery(spark: SparkSession, in0: DataFrame):\n    in0.write \\\n        .format("bigquery") \\\n        .option("writeMethod", "direct") \\\n        .save("dataset.table")\n'})}),(0,i.jsx)(r.p,{children:"Indirect write using the BigQuery Storage Write API:"}),(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:'def write_bigquery(spark: SparkSession, in0: DataFrame):\n    in0.write \\\n        .format("bigquery") \\\n        .option("temporaryGcsBucket","some-bucket") \\\n        .save("dataset.table")\n'})})]})})]})}function u(e={}){const{wrapper:r}={...(0,n.R)(),...e.components};return r?(0,i.jsx)(r,{...e,children:(0,i.jsx)(j,{...e})}):j(e)}}}]);