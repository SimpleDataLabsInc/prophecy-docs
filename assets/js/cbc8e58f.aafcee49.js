"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[8441],{15680:(e,t,a)=>{a.d(t,{xA:()=>g,yg:()=>d});var r=a(96540);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},l=Object.keys(e);for(r=0;r<l.length;r++)a=l[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(r=0;r<l.length;r++)a=l[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var s=r.createContext({}),p=function(e){var t=r.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},g=function(e){var t=p(e.components);return r.createElement(s.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},c=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,l=e.originalType,s=e.parentName,g=o(e,["components","mdxType","originalType","parentName"]),u=p(a),c=n,d=u["".concat(s,".").concat(c)]||u[c]||m[c]||l;return a?r.createElement(d,i(i({ref:t},g),{},{components:a})):r.createElement(d,i({ref:t},g))}));function d(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var l=a.length,i=new Array(l);i[0]=c;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[u]="string"==typeof e?e:n,i[1]=o;for(var p=2;p<l;p++)i[p]=a[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,a)}c.displayName="MDXCreateElement"},19365:(e,t,a)=>{a.d(t,{A:()=>i});var r=a(96540),n=a(20053);const l={tabItem:"tabItem_Ymn6"};function i(e){let{children:t,hidden:a,className:i}=e;return r.createElement("div",{role:"tabpanel",className:(0,n.A)(l.tabItem,i),hidden:a},t)}},11470:(e,t,a)=>{a.d(t,{A:()=>w});var r=a(58168),n=a(96540),l=a(20053),i=a(23104),o=a(56347),s=a(57485),p=a(31682),g=a(89466);function u(e){return function(e){return n.Children.map(e,(e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:a,attributes:r,default:n}}=e;return{value:t,label:a,attributes:r,default:n}}))}function m(e){const{values:t,children:a}=e;return(0,n.useMemo)((()=>{const e=t??u(a);return function(e){const t=(0,p.X)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function c(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function d(e){let{queryString:t=!1,groupId:a}=e;const r=(0,o.W6)(),l=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,s.aZ)(l),(0,n.useCallback)((e=>{if(!l)return;const t=new URLSearchParams(r.location.search);t.set(l,e),r.replace({...r.location,search:t.toString()})}),[l,r])]}function y(e){const{defaultValue:t,queryString:a=!1,groupId:r}=e,l=m(e),[i,o]=(0,n.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!c({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const r=a.find((e=>e.default))??a[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:t,tabValues:l}))),[s,p]=d({queryString:a,groupId:r}),[u,y]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[r,l]=(0,g.Dv)(a);return[r,(0,n.useCallback)((e=>{a&&l.set(e)}),[a,l])]}({groupId:r}),b=(()=>{const e=s??u;return c({value:e,tabValues:l})?e:null})();(0,n.useLayoutEffect)((()=>{b&&o(b)}),[b]);return{selectedValue:i,selectValue:(0,n.useCallback)((e=>{if(!c({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);o(e),p(e),y(e)}),[p,y,l]),tabValues:l}}var b=a(92303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function h(e){let{className:t,block:a,selectedValue:o,selectValue:s,tabValues:p}=e;const g=[],{blockElementScrollPositionUntilNextRender:u}=(0,i.a_)(),m=e=>{const t=e.currentTarget,a=g.indexOf(t),r=p[a].value;r!==o&&(u(t),s(r))},c=e=>{let t=null;switch(e.key){case"Enter":m(e);break;case"ArrowRight":{const a=g.indexOf(e.currentTarget)+1;t=g[a]??g[0];break}case"ArrowLeft":{const a=g.indexOf(e.currentTarget)-1;t=g[a]??g[g.length-1];break}}t?.focus()};return n.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,l.A)("tabs",{"tabs--block":a},t)},p.map((e=>{let{value:t,label:a,attributes:i}=e;return n.createElement("li",(0,r.A)({role:"tab",tabIndex:o===t?0:-1,"aria-selected":o===t,key:t,ref:e=>g.push(e),onKeyDown:c,onClick:m},i,{className:(0,l.A)("tabs__item",f.tabItem,i?.className,{"tabs__item--active":o===t})}),a??t)})))}function N(e){let{lazy:t,children:a,selectedValue:r}=e;const l=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=l.find((e=>e.props.value===r));return e?(0,n.cloneElement)(e,{className:"margin-top--md"}):null}return n.createElement("div",{className:"margin-top--md"},l.map(((e,t)=>(0,n.cloneElement)(e,{key:t,hidden:e.props.value!==r}))))}function v(e){const t=y(e);return n.createElement("div",{className:(0,l.A)("tabs-container",f.tabList)},n.createElement(h,(0,r.A)({},e,t)),n.createElement(N,(0,r.A)({},e,t)))}function w(e){const t=(0,b.A)();return n.createElement(v,(0,r.A)({key:String(t)},e))}},30547:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>g,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>p,toc:()=>u});var r=a(58168),n=(a(96540),a(15680)),l=a(11470),i=a(19365);const o={title:"Iceberg",id:"iceberg",description:"Iceberg",tags:["gems","file","iceberg"]},s=void 0,p={unversionedId:"Spark/gems/source-target/file/iceberg",id:"Spark/gems/source-target/file/iceberg",title:"Iceberg",description:"Iceberg",source:"@site/docs/Spark/gems/source-target/file/iceberg.md",sourceDirName:"Spark/gems/source-target/file",slug:"/Spark/gems/source-target/file/iceberg",permalink:"/Spark/gems/source-target/file/iceberg",draft:!1,tags:[{label:"gems",permalink:"/tags/gems"},{label:"file",permalink:"/tags/file"},{label:"iceberg",permalink:"/tags/iceberg"}],version:"current",frontMatter:{title:"Iceberg",id:"iceberg",description:"Iceberg",tags:["gems","file","iceberg"]},sidebar:"mySidebar",previous:{title:"Fixed Format",permalink:"/Spark/gems/source-target/file/fixed-format"},next:{title:"JSON",permalink:"/Spark/gems/source-target/file/json"}},g={},u=[{value:"Required Settings",id:"required-settings",level:2},{value:"Environment Setting",id:"environment-setting",level:3},{value:"Initialization Settings",id:"initialization-settings",level:3},{value:"Runtime Settings",id:"runtime-settings",level:3},{value:"Source",id:"source",level:2},{value:"Source Parameters",id:"source-parameters",level:3},{value:"Example",id:"source-example",level:3},{value:"Generated Code",id:"source-code",level:3},{value:"Target",id:"target",level:2},{value:"Target Parameters",id:"target-parameters",level:3},{value:"Supported Write Modes",id:"supported-write-modes",level:4},{value:"Target Example",id:"target-example",level:3},{value:"Generated Code",id:"target-code",level:3}],m={toc:u},c="wrapper";function d(e){let{components:t,...o}=e;return(0,n.yg)(c,(0,r.A)({},m,o,{components:t,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"Reads and writes Iceberg tables, including Iceberg Merge operations and Time travel."),(0,n.yg)("h2",{id:"required-settings"},"Required Settings"),(0,n.yg)("p",null,"Before you can use Iceberg source Gems, you must configure some required settings at the environment, initialization, and runtime stages."),(0,n.yg)("h3",{id:"environment-setting"},"Environment Setting"),(0,n.yg)("p",null,"You must configure a required Spark JAR dependency in your Fabric environment."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"JAR dependency"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Package: ",(0,n.yg)("inlineCode",{parentName:"li"},"https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.5.0/iceberg-spark-runtime-3.3_2.12-1.5.0.jar"))),(0,n.yg)("admonition",{parentName:"li",type:"note"},(0,n.yg)("p",{parentName:"admonition"},"The JAR dependency is available on your compute platform wherever Spark is installed, such as on your Databricks cluster, EMR, or Dataproc.")))),(0,n.yg)("h3",{id:"initialization-settings"},"Initialization Settings"),(0,n.yg)("p",null,"You must configure the following Spark session property during the Spark session initialization."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Spark session property:"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Key - ",(0,n.yg)("inlineCode",{parentName:"li"},"spark.sql.extensions")),(0,n.yg)("li",{parentName:"ul"},"Value - ",(0,n.yg)("inlineCode",{parentName:"li"},"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions"))),(0,n.yg)("admonition",{parentName:"li",type:"note"},(0,n.yg)("p",{parentName:"admonition"},"This can be done during cluster bootstrap. For example, you can set ",(0,n.yg)("inlineCode",{parentName:"p"},'--properties "spark:spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtentions" \\')," with your create clusters command.")))),(0,n.yg)("h3",{id:"runtime-settings"},"Runtime Settings"),(0,n.yg)("p",null,"You must configure the following Spark conf properties, which can be done during the Spark session runtime."),(0,n.yg)("p",null,"These properties allow you to configure multiple catalogs and your respective metastores for Iceberg tables and data management. You can configure Hadoop and Hive as catalogs."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Spark conf properties"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Configure Hadoop as catalog"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"spark.sql.catalog.<catalog_name>=org.apache.iceberg.spark.SparkCatalog")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"spark.sql.catalog.<catalog_name>.type=hadoop")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"spark.sql.catalog.<catalog_name>.warehouse=gs://<bucket>/<folder_1>/<folder_1>/")))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},"Configure Hive as catalog"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"spark.sql.catalog.<catalog_name>=org.apache.iceberg.spark.SparkCatalog")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"spark.sql.catalog.<catalog_name>.type=hive")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"spark.sql.catalog.<catalog_name>.warehouse=gs://<bucket>/<folder_1>/<folder_1>/")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"spark.sql.catalog.<catalog_name>.uri=thrift://10.91.64.30:9083"))))),(0,n.yg)("admonition",{parentName:"li",type:"tip"},(0,n.yg)("p",{parentName:"admonition"},"You can set the default catalog by using ",(0,n.yg)("inlineCode",{parentName:"p"},"spark.default.catalog=<catalog_name>"),".")))),(0,n.yg)("p",null,"To configure the Spark conf properties, follow these steps:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"Click ",(0,n.yg)("strong",{parentName:"p"},"...")," at the top of the Prophecy canvas, and then click ",(0,n.yg)("strong",{parentName:"p"},"Pipeline Settings")," under Manage."),(0,n.yg)("p",{parentName:"li"},(0,n.yg)("img",{alt:"Open Pipeline Settings",src:a(83289).A,width:"2620",height:"1509"}))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},"On the Spark dialog, under Spark Configuration, add the Spark conf properties."),(0,n.yg)("p",{parentName:"li"},(0,n.yg)("img",{alt:"Spark Pipeline Settings",src:a(80084).A,width:"2620",height:"1509"})))),(0,n.yg)("h2",{id:"source"},"Source"),(0,n.yg)("h3",{id:"source-parameters"},"Source Parameters"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Parameter"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"),(0,n.yg)("th",{parentName:"tr",align:null},"Required"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Catalog Name"),(0,n.yg)("td",{parentName:"tr",align:null},"Any configured Hadoop/Hive catalog name"),(0,n.yg)("td",{parentName:"tr",align:null},"True (If any default catalog is not configured in Spark runtime properties.)")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Schema Name (Database Name)"),(0,n.yg)("td",{parentName:"tr",align:null},"Name of the database"),(0,n.yg)("td",{parentName:"tr",align:null},"True")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Table Name"),(0,n.yg)("td",{parentName:"tr",align:null},"Name of the table"),(0,n.yg)("td",{parentName:"tr",align:null},"True")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Read Timestamp"),(0,n.yg)("td",{parentName:"tr",align:null},"Time travel to a specific timestamp (value should be in milliseconds)"),(0,n.yg)("td",{parentName:"tr",align:null},"False")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Read Snapshot"),(0,n.yg)("td",{parentName:"tr",align:null},"Time travel to a specific version of the table (value should be a snapshot ID)"),(0,n.yg)("td",{parentName:"tr",align:null},"False")))),(0,n.yg)("admonition",{type:"note"},(0,n.yg)("p",{parentName:"admonition"},"For time travel on Iceberg tables:"),(0,n.yg)("ol",{parentName:"admonition"},(0,n.yg)("li",{parentName:"ol"},"Only ",(0,n.yg)("inlineCode",{parentName:"li"},"Read Timestamp")," ",(0,n.yg)("strong",{parentName:"li"},(0,n.yg)("em",{parentName:"strong"},"OR"))," ",(0,n.yg)("inlineCode",{parentName:"li"},"Read Snapshot")," can be selected, not both."),(0,n.yg)("li",{parentName:"ol"},"Timestamp should be between the first commit timestamp and the latest commit timestamp in the table."),(0,n.yg)("li",{parentName:"ol"},"Snapshot needs to be a snapshot ID.")),(0,n.yg)("p",{parentName:"admonition"},"By default most recent version of each row is fetched if no time travel option is used.")),(0,n.yg)("admonition",{type:"info"},(0,n.yg)("p",{parentName:"admonition"},"To read more about Iceberg time travel and its use cases, see the ",(0,n.yg)("a",{parentName:"p",href:"https://iceberg.apache.org/docs/nightly/hive/?h=time#timetravel"},"Apache Iceberg docs"),".")),(0,n.yg)("h3",{id:"source-example"},"Example"),(0,n.yg)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,n.yg)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,n.yg)("iframe",{src:"https://fast.wistia.net/embed/iframe/b1vt6gv6bl?seo=false?videoFoam=true",title:"Iceberg source example",allow:"autoplay; fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,n.yg)("script",{src:"https://fast.wistia.net/assets/external/E-v1.js",async:!0}),(0,n.yg)("h3",{id:"source-code"},"Generated Code"),(0,n.yg)(l.A,{mdxType:"Tabs"},(0,n.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-py"},'def iceberg_read(spark: SparkSession) -> DataFrame:\n    return spark.read.format("iceberg").load("`hadoop_catalog_1`.`prophecy_doc_demo`.`employees_test`")\n')))),(0,n.yg)("hr",null),(0,n.yg)("h2",{id:"target"},"Target"),(0,n.yg)("h3",{id:"target-parameters"},"Target Parameters"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Parameter"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"),(0,n.yg)("th",{parentName:"tr",align:null},"Required"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Catalog Type"),(0,n.yg)("td",{parentName:"tr",align:null},"File path to write the Iceberg table to"),(0,n.yg)("td",{parentName:"tr",align:null},"True")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Catalog Name"),(0,n.yg)("td",{parentName:"tr",align:null},"Any configured Hadoop/Hive catalog name"),(0,n.yg)("td",{parentName:"tr",align:null},"True (If any default catalog is not configured in Spark runtime properties.)")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Schema Name (Database Name)"),(0,n.yg)("td",{parentName:"tr",align:null},"Name of the database"),(0,n.yg)("td",{parentName:"tr",align:null},"True")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Table Name"),(0,n.yg)("td",{parentName:"tr",align:null},"Name of the table"),(0,n.yg)("td",{parentName:"tr",align:null},"True")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"File Location"),(0,n.yg)("td",{parentName:"tr",align:null},"External file path to store data (Only applicable if Catalog type is Hive.)"),(0,n.yg)("td",{parentName:"tr",align:null},"False")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Partition Columns"),(0,n.yg)("td",{parentName:"tr",align:null},"List of columns to partition the Iceberg table by (Provide it during createOrReplace write mode to leverage overwritePartitions write mode in future.)"),(0,n.yg)("td",{parentName:"tr",align:null},"False")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Merge schema"),(0,n.yg)("td",{parentName:"tr",align:null},"If true, then any columns that are present in the DataFrame but not in the target table are automatically added on to the end of the schema as part of a write transaction."),(0,n.yg)("td",{parentName:"tr",align:null},"False")))),(0,n.yg)("h4",{id:"supported-write-modes"},"Supported Write Modes"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"Write Mode"),(0,n.yg)("th",{parentName:"tr",align:null},"Description"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"overwrite"),(0,n.yg)("td",{parentName:"tr",align:null},"If data already exists, overwrite with the contents of the DataFrame")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"append"),(0,n.yg)("td",{parentName:"tr",align:null},"If data already exists, append the contents of the DataFrame")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"ignore"),(0,n.yg)("td",{parentName:"tr",align:null},"If data already exists, do nothing with the contents of the DataFrame. This is similar to a ",(0,n.yg)("inlineCode",{parentName:"td"},"CREATE TABLE IF NOT EXISTS")," in SQL.")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"error"),(0,n.yg)("td",{parentName:"tr",align:null},"If data already exists, throw an exception.")))),(0,n.yg)("p",null,"Among these write modes overwrite and append works the same way as in case of parquet file writes."),(0,n.yg)("h3",{id:"target-example"},"Target Example"),(0,n.yg)("div",{class:"wistia_responsive_padding",style:{padding:"56.25% 0 0 0",position:"relative"}},(0,n.yg)("div",{class:"wistia_responsive_wrapper",style:{height:"100%",left:0,position:"absolute",top:0,width:"100%"}},(0,n.yg)("iframe",{src:"https://fast.wistia.net/embed/iframe/8j80hc4k1e?seo=false?videoFoam=true",title:"Iceberg-target-example",allow:"autoplay; fullscreen",allowtransparency:"true",frameborder:"0",scrolling:"no",class:"wistia_embed",name:"wistia_embed",msallowfullscreen:!0,width:"100%",height:"100%"}))),(0,n.yg)("script",{src:"https://fast.wistia.net/assets/external/E-v1.js",async:!0}),(0,n.yg)("h3",{id:"target-code"},"Generated Code"),(0,n.yg)(l.A,{mdxType:"Tabs"},(0,n.yg)(i.A,{value:"py",label:"Python",mdxType:"TabItem"},(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-py"},'def iceberg_write(spark: SparkSession, in0: DataFrame):\n    df1 = in0.writeTo("`hadoop_catalog_1`.`prophecy_doc_demo`.`employees_test`")\n    df2 = df1.using("iceberg")\n    df3 = df2.partitionedBy("Department")\n    df4 = df3.tableProperty("write.spark.accept-any-schema", "true")\n    df4.createOrReplace()\n')))))}d.isMDXComponent=!0},83289:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/open-pipeline-settings-7f4727f13dde8416efbd6a7ab37186c5.png"},80084:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/spark-pipeline-settings-57f8518c7df28863b9d468c3f6d9c13d.png"}}]);