"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[52691],{15680:(e,t,o)=>{o.d(t,{xA:()=>l,yg:()=>d});var r=o(96540);function n(e,t,o){return t in e?Object.defineProperty(e,t,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[t]=o,e}function a(e,t){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),o.push.apply(o,r)}return o}function i(e){for(var t=1;t<arguments.length;t++){var o=null!=arguments[t]?arguments[t]:{};t%2?a(Object(o),!0).forEach((function(t){n(e,t,o[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):a(Object(o)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(o,t))}))}return e}function c(e,t){if(null==e)return{};var o,r,n=function(e,t){if(null==e)return{};var o,r,n={},a=Object.keys(e);for(r=0;r<a.length;r++)o=a[r],t.indexOf(o)>=0||(n[o]=e[o]);return n}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)o=a[r],t.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(n[o]=e[o])}return n}var s=r.createContext({}),p=function(e){var t=r.useContext(s),o=t;return e&&(o="function"==typeof e?e(t):i(i({},t),e)),o},l=function(e){var t=p(e.components);return r.createElement(s.Provider,{value:t},e.children)},g="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},f=r.forwardRef((function(e,t){var o=e.components,n=e.mdxType,a=e.originalType,s=e.parentName,l=c(e,["components","mdxType","originalType","parentName"]),g=p(o),f=n,d=g["".concat(s,".").concat(f)]||g[f]||u[f]||a;return o?r.createElement(d,i(i({ref:t},l),{},{components:o})):r.createElement(d,i({ref:t},l))}));function d(e,t){var o=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var a=o.length,i=new Array(a);i[0]=f;var c={};for(var s in t)hasOwnProperty.call(t,s)&&(c[s]=t[s]);c.originalType=e,c[g]="string"==typeof e?e:n,i[1]=c;for(var p=2;p<a;p++)i[p]=o[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,o)}f.displayName="MDXCreateElement"},34663:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>u,frontMatter:()=>a,metadata:()=>c,toc:()=>p});var r=o(58168),n=(o(96540),o(15680));const a={sidebar_position:2,title:"Composer",id:"composer_fabric",description:"How Prophecy creates a Composer Airflow Fabric",tags:["scheduling","airflow","jobs","composer","fabric"]},i=void 0,c={unversionedId:"Orchestration/airflow/setup/composer_fabric",id:"Orchestration/airflow/setup/composer_fabric",title:"Composer",description:"How Prophecy creates a Composer Airflow Fabric",source:"@site/docs/Orchestration/airflow/setup/composer.md",sourceDirName:"Orchestration/airflow/setup",slug:"/Orchestration/airflow/setup/composer_fabric",permalink:"/Orchestration/airflow/setup/composer_fabric",draft:!1,tags:[{label:"scheduling",permalink:"/tags/scheduling"},{label:"airflow",permalink:"/tags/airflow"},{label:"jobs",permalink:"/tags/jobs"},{label:"composer",permalink:"/tags/composer"},{label:"fabric",permalink:"/tags/fabric"}],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"Composer",id:"composer_fabric",description:"How Prophecy creates a Composer Airflow Fabric",tags:["scheduling","airflow","jobs","composer","fabric"]},sidebar:"defaultSidebar",previous:{title:"Limits and Restrictions",permalink:"/Orchestration/airflow/setup/prophecy-managed/prophecy_managed_airflow_fabric_limits"},next:{title:"MWAA",permalink:"/Orchestration/airflow/setup/MWAA_fabric"}},s={},p=[{value:"How to create a Composer Airflow Fabric",id:"how-to-create-a-composer-airflow-fabric",level:2},{value:"Setting up Connections",id:"setting-up-connections",level:2},{value:"Setting up Databricks Spark connection",id:"setting-up-databricks-spark-connection",level:3},{value:"Setting up Snowflake SQL Connection",id:"setting-up-snowflake-sql-connection",level:3},{value:"Create an Airflow Job",id:"create-an-airflow-job",level:2}],l={toc:p},g="wrapper";function u(e){let{components:t,...a}=e;return(0,n.yg)(g,(0,r.A)({},l,a,{components:t,mdxType:"MDXLayout"}),(0,n.yg)("p",null,"You can use Prophecy to connect to your Cloud Composer-based Airflow, to create, run, and monitor your Airflow DAGs.\nFor this, you would need to create a Composer Airflow Fabric."),(0,n.yg)("h2",{id:"how-to-create-a-composer-airflow-fabric"},"How to create a Composer Airflow Fabric"),(0,n.yg)("p",null,"Setting up a Fabric is very straightforward. Click the ",(0,n.yg)("strong",{parentName:"p"},"(1) Create Entity")," button, and choose the ",(0,n.yg)("strong",{parentName:"p"},"(2) Create Fabric")," option. The Fabric creation is composed of two steps: Basic Info and Providers setup.\nOn the Basic Info screen, enter a ",(0,n.yg)("strong",{parentName:"p"},"(3) Fabric Name"),", ",(0,n.yg)("strong",{parentName:"p"},"(4) Fabric Description"),", and choose the ",(0,n.yg)("strong",{parentName:"p"},"(5) Team")," that\u2019s going to own the Fabric."),(0,n.yg)("p",null,"Once ready, click ",(0,n.yg)("strong",{parentName:"p"},"(6) Continue"),"."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"CreateFabric",src:o(68673).A,width:"2880",height:"1084"})),(0,n.yg)("p",null,"Since we\u2019re setting up a Fabric connected to Cloud Composer Airflow, choose ",(0,n.yg)("strong",{parentName:"p"},"Airflow")," as the ",(0,n.yg)("strong",{parentName:"p"},"(1) Provider Type")," and ",(0,n.yg)("strong",{parentName:"p"},"Cloud Composer")," as the ",(0,n.yg)("strong",{parentName:"p"},"(2) Provider"),"."),(0,n.yg)("p",null,"Once you select Cloud Composer, you will start seeing fields for credentials as shown below."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"ComposerFabric",src:o(33993).A,width:"2880",height:"1726"})),(0,n.yg)("p",null,"Type in your GCP project in the ",(0,n.yg)("strong",{parentName:"p"},"(3)Project ID")," field, and select the ",(0,n.yg)("strong",{parentName:"p"},"(4)Location")," from dropdown. And Upload your ",(0,n.yg)("strong",{parentName:"p"},"(5)Private key")," file to authenticate.\nClick on ",(0,n.yg)("strong",{parentName:"p"},"(6) Fetch Environments")," to fetch the Airflow Instances running in the mentioned GCP Project."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"ComposerFabric2",src:o(49343).A,width:"2880",height:"1726"}),"\nSelect the desired instance of Airflow, in the ",(0,n.yg)("strong",{parentName:"p"},"(1) Airflow Environment")," dropdown. This will autofill the ",(0,n.yg)("strong",{parentName:"p"},"(2) Airflow URL")," and ",(0,n.yg)("strong",{parentName:"p"},"(3)Dag location")," field which are uneditable.\nClick on ",(0,n.yg)("strong",{parentName:"p"},"(4) Continue"),"."),(0,n.yg)("p",null,"This completes the Fabric Creation For Composer, and you can now start adding Connections."),(0,n.yg)("h2",{id:"setting-up-connections"},"Setting up Connections"),(0,n.yg)("p",null,"You need Airflow to talk to various other systems in your Data Platform to be able to do certain tasks like sending Email, triggering Spark pipelines, and SQL models.\nFor these, we create ",(0,n.yg)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/connections.html"},"connections")," in Airflow."),(0,n.yg)("p",null,"You can map connections already created in your Composer, in the Connections Tab of the Fabric.\nProphecy will use these connections to fetch the connection-id to generate the correct Airflow Code when you use these in your Airflow Gems."),(0,n.yg)("p",null,"For adding a connection, Click on ",(0,n.yg)("strong",{parentName:"p"},"(1) Add Connection")," button. This Opens up the Connection form as shown."),(0,n.yg)("h3",{id:"setting-up-databricks-spark-connection"},"Setting up Databricks Spark connection"),(0,n.yg)("p",null,"To be able to schedule your Databricks Spark pipelines via Airflow, you need to have a Databricks Spark Connections from Airflow to your Databricks Workspace. You need to create the connection in Airflow and provide the mapping to Prophecy in this form."),(0,n.yg)("p",null,"Select ",(0,n.yg)("strong",{parentName:"p"},"(2) Connection Type")," as Databricks(Spark), and select the ",(0,n.yg)("strong",{parentName:"p"},"(3) Fabric")," you have in Prophecy for the desired Databricks Workspace. Select the ",(0,n.yg)("strong",{parentName:"p"},"(4) Connection id")," you have created for this Databricks workspace in your Airflow.\nMake sure you select the Fabric for the same Databricks workspace you have already created the connection for in your Airflow.\nOnce done, hit ",(0,n.yg)("strong",{parentName:"p"},"(4) Save"),"."),(0,n.yg)("p",null,(0,n.yg)("img",{alt:"Composer_connection",src:o(68959).A,width:"2880",height:"1084"})),(0,n.yg)("h3",{id:"setting-up-snowflake-sql-connection"},"Setting up Snowflake SQL Connection"),(0,n.yg)("p",null,"Similarly, setup a connection to a Snowflake Fabric following ",(0,n.yg)("a",{parentName:"p",href:"/Orchestration/airflow/setup/MWAA_fabric#setting-up-connections"},"these")," steps."),(0,n.yg)("h2",{id:"create-an-airflow-job"},"Create an Airflow Job"),(0,n.yg)("p",null,"Once the Airflow Composer Fabric is setup with the relevant connections, Airflow Job scheduling is done with an easy-to-use interface. Follow this guide to ",(0,n.yg)("a",{parentName:"p",href:"/getting-started/airflow#2-create-an-airflow-job"},"Create an Airflow Job"),"."))}u.isMDXComponent=!0},33993:(e,t,o)=>{o.d(t,{A:()=>r});const r=o.p+"assets/images/Composer_Fabric-2a84b8c0e2b4e0055110b0553f135853.png"},49343:(e,t,o)=>{o.d(t,{A:()=>r});const r=o.p+"assets/images/Composer_Fabric2-e4b60fe2f20d912211ff53aea4c281ec.png"},68959:(e,t,o)=>{o.d(t,{A:()=>r});const r=o.p+"assets/images/Composer_connections-43774fe979c973dde7c27b3a3fac5995.png"},68673:(e,t,o)=>{o.d(t,{A:()=>r});const r=o.p+"assets/images/Fabric_Create-d23d384a07e74abee7b04879025b6cf6.png"}}]);